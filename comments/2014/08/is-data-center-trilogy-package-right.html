<div class="comments post" id="comments">
  <h4>3 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="7902089725421610367">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Piotr</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7902089725421610367" href="#7902089725421610367">29 August 2014 12:08</a>
              </span>
            </div>
            <div class="comment-content">I had similar issue with vm/it team 3 years ago. Talking and frightening them, finally helped. Unfortunately we (net) have to give way fields and go for some compromises. We agreed for L2 on vm network but backed infrastructure is pure L3. vm/it know and understand the risk.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7455603177142587762">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7455603177142587762" href="#7455603177142587762">03 September 2014 10:26</a>
              </span>
            </div>
            <div class="comment-content">Same here, similar issue 3 years ago.... After reading a lot about it we managed to avoid setting up L2 DCI: we had so many arguments against L2 DCI that VM team finally agreed to setup L3 DCI. What helped us? Reading Trilogy webinar and even reading Oracle and IBM recommendations (Oracle recommended us to setup Data Guard to maximize availability and data protection, and IBM recommends that WAS cells do not span DCs)...  Now we have 2 x dark fiber which we use to replicate Storage Arrays and 2 x dark fibere which we use to route traffic between DC, and we are quite happy this way. Not sure about VM team, but network is definitely more stable and outages have not impacted that much in critical services deployed in both DCs.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7842057118910065793">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">James Hess</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7842057118910065793" href="#7842057118910065793">08 September 2014 07:27</a>
              </span>
            </div>
            <div class="comment-content">I view Long Distance vMotion as a  technology for facilitating migration of an individual non-redundant enterprise datacenter by minimizing downtime of each individual workload.<br /><br />Understanding that;  until the migration is completed, an outage at EITHER datacenter  or the connection between them is a potential risk and would cause serious problems.   Combine a simultaneous vMotion and storage vMotion,  and the workload moves.<br /><br />As soon as everything is moved,  shutdown the old datacenter.<br /><br /><br /></div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
