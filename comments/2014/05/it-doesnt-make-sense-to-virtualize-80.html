<div class="comments post" id="comments">
  <h4>13 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="4843603211666969745">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03978851447355661014" rel="nofollow">abraxxa</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4843603211666969745" href="#4843603211666969745">07 May 2014 11:05</a>
              </span>
            </div>
            <div class="comment-content">That&#39;s fine if you only have x86 servers but there are also Solaris and IBM Power servers that should reside in the same networks/security zones.<br />Also hardware appliances for network services like firewalls, load balancers, IPS and also Citrix access gateways are often cheaper than their virtual licenses.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="3844862249961955074">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3844862249961955074" href="#3844862249961955074">07 May 2014 15:08</a>
              </span>
            </div>
            <div class="comment-content">Of course you&#39;re right but:<br /><br />A) Those exceptions usually don&#39;t represent 20% of the servers (or ports)<br />B) It still doesn&#39;t make sense to mix then with the hypervisor hosts on the same ToR switches.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3552890574012708309">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3552890574012708309" href="#3552890574012708309">07 May 2014 17:01</a>
              </span>
            </div>
            <div class="comment-content">Great post Ivan!<br /><br />Can you expand on why it doesn&#39;t make sense to mix those appliances and legacy servers that absolutely can&#39;t be virtualized onto the same ToR switches as the hypervisor hosts?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8226380223486025790">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/15818883829738651247" rel="nofollow">shah</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8226380223486025790" href="#8226380223486025790">07 May 2014 19:08</a>
              </span>
            </div>
            <div class="comment-content">Hardware VXLAN VTEPs are still important for connecting external feed. e.g. MPLS VPN service from a service provider, private GE links. Cloud operator needs to support bridging physical and virtual environment for a customer. Thing is you cannot connect any single physical cable directly to virtual appliance. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8209608483467151888">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08914689992468372696" rel="nofollow">Simon Hamilton-Wilkes</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8209608483467151888" href="#8209608483467151888">07 May 2014 21:28</a>
              </span>
            </div>
            <div class="comment-content">But software VTEPs are wirespeed at this point, so except in some very niche situations where you might have tens of gigabits of hardware SSL in F5 or a large AIX system or something, but for Inet/WAN connections etc. in the vast majority of firms there is no need.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6644146531384135125">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://www.anastarsha.com" rel="nofollow">Anas</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6644146531384135125" href="#6644146531384135125">08 May 2014 19:04</a>
              </span>
            </div>
            <div class="comment-content">The number of physical servers could be quite a bit especially if the customer is big Oracle shop. Most of the applications that use clustering to achieve HA (Oracle RAC, MySQL Clusters, etc..) or applications with heavy duty IO requirements have to stay on dedicated physical boxes. How would you then connect them to the VXLAN network?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="39886787187552331">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/15818883829738651247" rel="nofollow">shah</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c39886787187552331" href="#39886787187552331">08 May 2014 22:34</a>
              </span>
            </div>
            <div class="comment-content">using switches that supports Hardware VXLAN VTEPs. You can bridge VXLAN to a VLAN or to a port. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7298860798217719073">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://anastarsha.com" rel="nofollow">Anas</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7298860798217719073" href="#7298860798217719073">09 May 2014 02:29</a>
              </span>
            </div>
            <div class="comment-content">yes i understand that. But Ivan was suggesting not putting physical servers on same ToR as hypervisor hosts as i was wondering why. Sorry my question in the previous reply was not clear</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5629431186426876927">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5629431186426876927" href="#5629431186426876927">09 May 2014 04:37</a>
              </span>
            </div>
            <div class="comment-content">Simply route to the physical subnet. Assuming these heavy iron DBs are in a separate network segment for security reasons, it would be quite simple to route the appropriate traffic. <br /><br />More than likely, your virtual farm and big iron are going to be separately racked and cabled anyway.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="952589680317280756">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08914689992468372696" rel="nofollow">Simon Hamilton-Wilkes</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c952589680317280756" href="#952589680317280756">10 May 2014 06:03</a>
              </span>
            </div>
            <div class="comment-content">Or you put an on-ramp pair of switches someplace in the DC which have some form of clustering so they can support multi-switch teaming - then they talk L3 to the fabric and L2 to WAN connections/appliances/non-x86 iron.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6462760241861482679">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03803244374816795623" rel="nofollow">Bhargav</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6462760241861482679" href="#6462760241861482679">10 May 2014 18:54</a>
              </span>
            </div>
            <div class="comment-content">Ivan,<br /><br />I am not fully convinced with basic assumption of &quot;We quickly agreed physical-to-virtual gateways are the primary use case&quot;. Would rather look at the problem from controller&#39;s scalabilty and performance point of view. That is where would one deploy VTEPs, is it on hypervisors or or on ToR&#39;s.<br /><br />Consider a different usecase with 50K VM&#39;s, at 60VM&#39;s per physical host ~825+ physical hosts (all virtualized). Assuming 5 VM&#39;s per VNI, about 10K&#39;s VNI&#39;s and each VM&#39;s of a given tenant reside in different physical host. <br /><br />If one were to have VTEPs at the hyper-visors for the usecase considered. The performance numbers are as follows<br /><br />1) 2 TCP connection with each hyper visors. One for OVSDB and another for OF (With NSX or with ODL). So the controller has to handle about 1500+ TCP connections just for managing the hypervisors.<br /><br />2) If OF-1.0 is used, #virtual ports created on a single physical host are 60 * 5 = 300/physical host. So the controller to handle 300 * 825 ~ 25K virtual ports. Agree this number is reduced when OF1.3 is used. At this don&#39;t have numbers to what extent.<br /><br />3) #flows programmed by the controller also increases as flows are programmed by the controller.<br /><br />4) Controller to manage 825+ physical hosts to distribute VM routes.<br /><br />On the other hand, if the VTEPs are deployed at ToR switch, with 30 10GE Ports<br /><br />A) #TCP connection to controller is 25+. We only need OVSDB connection and don&#39;t require OF, as solution like NSX leave the programming of flows to HW vendor instead of using OF.<br /><br />B) As there is no OF in the picture, controller need not bother about creating virtual-ports/handling flow entries etc. <br /><br />C) Controller to manage only 25+ HW VTEPs to distribute VM routes.<br /><br />So, to summarize scalability of the controller becomes important point for choosing hardware VxLAN GWs<br /></div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="974557807709007253">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c974557807709007253" href="#974557807709007253">11 May 2014 08:16</a>
              </span>
            </div>
            <div class="comment-content">1) Last time I checked, web servers happily worked with 10K concurrent TCP connections. No reason a cluster of controllers couldn&#39;t do the same.<br /><br />2) You don&#39;t need virtual ports like you think you do. Read <br /><br />http://blog.ipspace.net/2013/08/are-overlay-networking-tunnels.html<br /><br />and comments to it.<br /><br />3) Number of forwarding entries isn&#39;t that different from the VTEP case, and the forwarding entries cost you less than the hardware ones.<br /><br />4) So what? What&#39;s the number of changes-per-second?<br /><br />Finally, with all the questions you&#39;re asking, I think it&#39;s time for full disclosure: who are you working for?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7874272925047660832">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7874272925047660832" href="#7874272925047660832">13 May 2014 02:04</a>
              </span>
            </div>
            <div class="comment-content">VMware?</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
