comments:
- date: 19 March 2014 09:29
  html: 'See http://tools.ietf.org/agenda/89/slides/slides-89-tcpm-7.pdf for a recent
    discussion on the impact of reordering on TCP performance on recent stacks. '
  id: '7257247553615947533'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Olivier Bonaventure
  profile: https://www.blogger.com/profile/16010323799492079420
  pub: '2014-03-19T09:29:42.705+01:00'
  ref: '3418762271906253018'
  type: comment
- comments:
  - date: 19 March 2014 10:50
    html: That would be reordering across multiple paths (i.e. TCP sessions), not
      within a single TCP session, right?<br /><br />Thanks for the links, they&#39;re
      great!
    id: '2834143931053708209'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2014-03-19T10:50:43.034+01:00'
    ref: '4802685953559007697'
    type: comment
  - date: 19 March 2014 13:28
    html: Yes, MPTCP handles reordering accross different subflows/paths. The tcpm
      link above deals with reordering on a single tcp connection<br />
    id: '3860717170733321835'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Olivier Bonaventure
    profile: https://www.blogger.com/profile/16010323799492079420
    pub: '2014-03-19T13:28:20.761+01:00'
    ref: '4802685953559007697'
    type: comment
  date: 19 March 2014 09:31
  html: Multipath TCP can deal efficiently with the reordering caused by different
    paths through the network. See http://multipath-tcp.org/pmwiki.php?n=Main.50Gbps
    on how to reach 50 Gbps with a single Multipath TCP connection on 6 10 Gbps interfaces
  id: '4802685953559007697'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Olivier Bonaventure
  profile: https://www.blogger.com/profile/16010323799492079420
  pub: '2014-03-19T09:31:42.225+01:00'
  ref: '3418762271906253018'
  type: comment
- date: 19 March 2014 11:16
  html: Also when you have a problem (for example faultry transceiver/sfp/fiber) per
    packet load balancing may lead to situations that are very hard to troubleshoot.
    often enough I get inaccurate/wrong information on an issue by the customer/admin
    so adding another layer which probably complicates  the reproduction of the problem&#39;s
    effects is just bad
  id: '3328769487402545123'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/17119268104941260145
  pub: '2014-03-19T11:16:50.672+01:00'
  ref: '3418762271906253018'
  type: comment
- date: 19 March 2014 19:27
  html: 'Regarding TCP in the datacenter, though not specifically related to packet
    reordering, there is an specific variant denoted as DataCenter TCP (DCTCP) which
    leverages on ECN bits marked by switches when the queue occupancy is high. This
    DCTCP adjusts the congestion window of the transmitter based on the amount of
    packets received with the ECN bits marked per RTT, and obtains much better performance
    (in terms of latency, response to incast traffic and fairness between different
    flows sharing links) than traditional TCP implementations. DCTCP is active by
    default in Windows server 2012 (the server detects automatically which connections
    are from within the DC, to use DCTCP, and which ones are external to rely on traditional
    TCP), and there exist patches for linux. Additionally, some switches (see Nexus
    3548, for example) are advertised as &quot;DCTCP compatible&quot;, or simply said,
    they mark the ECN bits. I am not involved with it, I found the info in their website:
    http://simula.stanford.edu/~alizade/Site/DCTCP.html<br /><br />Knowing that TCP
    or DCTCP only adjust the transmission window once per RTT, it would be interesting
    to know how this mechanism (or a similar one) reacts to packet reordering. '
  id: '3684949643264226194'
  image: https://resources.blogblog.com/img/blank.gif
  name: Enrique Vallejo
  profile: http://personales.unican.es/vallejoe/
  pub: '2014-03-19T19:27:23.947+01:00'
  ref: '3418762271906253018'
  type: comment
- comments:
  - date: 20 March 2014 08:37
    html: Thanks a million for figuring this out!!! Windows Server 2012 R2 is also
      using flowlets, but I never found time to chase that down.<br /><br />See &quot;Dynamic
      NIC teaming&quot; section in this document:<br /><br />http://blogs.technet.com/b/networking/archive/2013/07/31/transforming-your-datacenter-networking.aspx
    id: '3175588625727589286'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2014-03-20T08:37:31.792+01:00'
    ref: '3269946433203538010'
    type: comment
  date: 19 March 2014 21:48
  html: "I&#39;ve seen a couple of articles mentioning that the Cisco 9k will do something\
    \ similar to Brocade regarding per-packet load-balancing:<br />http://keepingitclassless.net/2013/11/insieme-and-cisco-aci-part-2-aci-and-programmability/<br\
    \ />http://lamejournal.com/2013/11/21/ciscos-aci-insieme-launch/<br /><br />A\
    \ few searches later, it looks like Cisco will be using &quot;Flowlet Switching\
    \ (Kandula et al \u201904)&quot;, as can be seen on page 20:<br />http://www.cisco.com/web/strategy/docs/gov/application_centric_infrastructure.pdf<br\
    \ /><br />I had never heard of &quot;flowlet switching&quot; before, but it does\
    \ sound interesting:<br />http://nms.lcs.mit.edu/~kandula/data/FLARE_HotNets04_web.ppt"
  id: '3269946433203538010'
  image: https://resources.blogblog.com/img/blank.gif
  name: Alvaro Pereira
  profile: null
  pub: '2014-03-19T21:48:20.073+01:00'
  ref: '3418762271906253018'
  type: comment
- comments:
  - date: 04 April 2014 21:38
    html: 5-tuple load balancing should _not_ cause packet reordering. Per-packet
      load balancing will.<br /><br />Opinions on the usefulness of LRO/LSO are mixed,
      but if it works well, it can save significant amount of CPU cycles or improve
      throughput of high-bandwidth TCP sessions.
    id: '6286082881785395448'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2014-04-04T21:38:35.366+02:00'
    ref: '7401247133978842229'
    type: comment
  date: 03 April 2014 13:56
  html: Hi ivan, this raises an interesting point regarding the port channel load-balancing
    algorithm for PortChannels in vPCs on FlexPods. Essentially we have been advocating
    algorithms that use SRC port to spread the storage ethernet traffic across available
    paths to get the full use of 20, 30, 40Gbps channels (egress) from/to NetApp filers
    and Cisco UCS for NFS and iSCSI. So the jumbo storage frames would essentially
    not monopolise a single link when all NFS flows go from one SRC IP to one DST
    IP NFS mount. What we did see a lot in packet dumps for either VMware host storage
    or guest iSCSI traffic was out of order packets which I didn&#39;t think much
    of at the time... however in retrospect the default LRO/GRO, LSO/LRO behaviour
    on hosts and guests would be being slowed down and another reason to turn it off
    completely I guess? Thoughts?
  id: '7401247133978842229'
  image: https://4.bp.blogspot.com/-xQY4JttGiCw/W7XXHXiTEKI/AAAAAAAABpU/FbhLqq15knswnPBkwnPL2ig37sbxpLlhACK4BGAYYCw/s32/linked_in_donal_profile.png
  name: Donal
  profile: https://www.blogger.com/profile/13772533723547791668
  pub: '2014-04-03T13:56:16.985+02:00'
  ref: '3418762271906253018'
  type: comment
- comments:
  - date: 14 February 2015 03:47
    html: Please search my blog for recent posts on ECMP load balancing and flowlets
      - I covered both topics.<br /><br />Thank you!
    id: '3639752724404015491'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-02-14T03:47:27.594+01:00'
    ref: '3731540712565330364'
    type: comment
  date: 14 February 2015 03:36
  html: Last year Juniper introduced an interesting feature for their VC-Fabric called
    Adaptive Flowlet Splicing which inspects the metrics of flows and links to determine
    when packets within a flow may be &quot;safely sprayed&quot; without risk of reordering
    on the receiving end.<br /><br />I believe it measures link utilization, queue
    depth, latency, and several flow-specific metrics to determine a flow&#39;s &quot;burstiness&quot;
    in making this decision.<br /><br />This Juniper post explains it very well.<br
    />http://forums.juniper.net/t5/Data-Center-Technologists/Adaptive-Flowlet-Splicing-VCF-s-Fine-Grained-Adaptive-Load/ba-p/251674
  id: '3731540712565330364'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Adam
  profile: https://www.blogger.com/profile/18417457563490718355
  pub: '2015-02-14T03:36:27.737+01:00'
  ref: '3418762271906253018'
  type: comment
count: 12
id: '3418762271906253018'
type: post
url: 2014/03/per-packet-load-balancing-interferes.html
