{
  "comments": [
    {
      "comments": [
        {
          "date": "09 October 2014 15:02",
          "html": "Neither will I ... or a few other people:<br /><br />http://blog.ipspace.net/2012/03/stretched-layer-2-subnets-server.html<br /><br />It is, however, more lucrative from vendors&#39; or system integrators&#39; perspective to sell more boxes (or more complex boxe) to the customer than telling them that they should fix their apps.",
          "id": "8709335208035101532",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2014-10-09T15:02:51.174+02:00",
          "ref": "792463811799743973",
          "type": "comment"
        },
        {
          "date": "15 October 2014 17:03",
          "html": "While I agree that stretched layer 2 does not make sense from a networking perspective, your assertion that &quot;they should fix their apps&quot; lives in world of unicorns and fairies.  That will never happen.  There is not a chance that an Oracle ERP with 1000&#39;s of moving parts will ever work they way we wish it would (Ok maybe it is possible but how many millions would it take....). Just my prospective from higher ed.",
          "id": "7370380735839905949",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/13274292942859087623",
          "pub": "2014-10-15T17:03:37.207+02:00",
          "ref": "792463811799743973",
          "type": "comment"
        },
        {
          "date": "15 October 2014 17:46",
          "html": "Yep, and mission-critical apps will never move from mainframes to minicomputers (let alone PCs). Been there, heard that.<br /><br />It will just take a while longer in some environments than others.",
          "id": "4817950741070025542",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2014-10-15T17:46:59.144+02:00",
          "ref": "792463811799743973",
          "type": "comment"
        }
      ],
      "date": "09 October 2014 14:22",
      "html": "What I will never understand is the insistence to do l2 dci to begin with.  No matter what the solution is it will always be kludgy as its outside of ethernet&#39;s design scope.  Problems need to be solved at the proper place and app redundancy is not a network one in my opinion.  Am I off base?",
      "id": "792463811799743973",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2014-10-09T14:22:52.858+02:00",
      "ref": "212506992052150272",
      "type": "comment"
    },
    {
      "date": "09 October 2014 19:44",
      "html": "Nice responses to readers questions.I agree with the responses and additionally , state synchronization between the FWs in the both data centers and state sync between other stateful devices could be necessary if the RTO requirement is 0 or near 0 , in case one of the data center fail. <br /><br />But of course just you can do it does not mean you should do it. DCI link failures might cause more issue than what ot would solve.<br /><br />Also for the OTV edge devices ,what I would say to this commenter in addition to your excellent and meaningful response :) , although they are not single point of failure , they can not be used in load share scenario due to how it works to avoid forwarding loop. So the convergence time needs to be considered and maybe adjusted and also odd/even Vlan separation needs to be arranged carefully for multihoming ( Management complexity.! ).",
      "id": "5579131111211159677",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/14904477081635905810",
      "pub": "2014-10-09T19:44:12.096+02:00",
      "ref": "212506992052150272",
      "type": "comment"
    },
    {
      "date": "10 October 2014 02:04",
      "html": "&quot;but you&#39;re protected from bridging failures going over your DCI&quot;<br /><br />You are only protected whilst nothing has failed. It doesn&#39;t matter what protocol/technology/features you are using, or what protection mechanisms you have in place. A faulty supervisor module can quite easily identify your single failure domain, I speak from experience ;) ",
      "id": "513248422472285225",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2014-10-10T02:04:01.855+02:00",
      "ref": "212506992052150272",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "18 October 2014 12:46",
          "html": "Same set of problems. Some things (like squaring the circle) cannot be solve using rational means.",
          "id": "1035559682070520422",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2014-10-18T12:46:27.348+02:00",
          "ref": "5745377697702235509",
          "type": "comment"
        }
      ],
      "date": "17 October 2014 22:41",
      "html": "What about the rest? What do you think about H3C\u00b4s EVI or Huawei\u00b4s EVN?<br /><br />http://www.h3c.com/portal/download.do?id=1798081<br />http://enterprise.huawei.com/ilink/cnenterprise/download/HW_341904",
      "id": "5745377697702235509",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Michael67",
      "profile": "https://www.blogger.com/profile/03014282355010119539",
      "pub": "2014-10-17T22:41:30.670+02:00",
      "ref": "212506992052150272",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "20 October 2014 16:35",
          "html": "Hi Yves!<br /><br />Thanks for an extensive reply. We agree on quite a few topics, and will have to agree to disagree on a few others.<br /><br />First a nit: most overlay virtual networking implementations no longer require multicast, the only exception being vShield suite (which is a zombie anyway).<br /><br />More importantly, you haven&#39;t addressed my major concern: OTV requires a DC-wide bridging domain because it sits at the edge of a data center (and I will defer the LISP discussion for some other time ;).<br /><br />Finally, while I understand your excitement regarding the long-distance ASA cluster, do tell me what happens when the DCI link fails?<br /><br />Kind regards,<br />Ivan",
          "id": "5687211153730648363",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2014-10-20T16:35:42.613+02:00",
          "ref": "6907903599439189521",
          "type": "comment"
        },
        {
          "date": "20 October 2014 19:31",
          "html": "This comment has been removed by the author.",
          "id": "8804183978747364483",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/09348068006439724629",
          "pub": "2014-10-20T19:31:41.730+02:00",
          "ref": "6907903599439189521",
          "type": "comment"
        },
        {
          "date": "21 October 2014 08:03",
          "html": "/ <br />Let me split the answers in 3 different replies, otherwise it seems that the whole response is too long<br />/<br />Thank you Ivan for the fast reply. Let me give you here my thoughts. Please note that I\u2019m replying to the comments made previously only, but several other requirements to build a solid DCI are described in the post I mentioned above.<br />\u00a0<br />Ivan&gt;&gt; First a nit: most overlay virtual networking implementations no longer require multicast, the only exception being vShield suite (which is a zombie anyway).<br />\u00a0<br />yves &gt;&gt; yes and no :) - But this point is crucial.<br />\u00a0<br />From a software-based VXLAN point of view, there are indeed several vendors who already offer unicast-only mode.<br />From a ToR-based VXLAN point of view, as far as I\u2019m aware, most of vendors are still enabling VXLAN in Multicast mode (TBC).<br />We will certainly comeback in the near future to this last statement, but as I said things are moving quickly however I\u2019m talking about what is available today.<br />\u00a0<br />Back to the software-based VXLAN. Leaving aside discussions on scalability and performances, no one wants/can deploy a fully virtual environment, it&#39;s always an hybrid network with communication with traditional platforms, hence we need VTEP Gateway (L2) for the VLAN ID &lt;=&gt; VNI mapping. Unfortunately, currently only one single active L2 gateway per VXLAN domain is supported in software-based VTEP. Thus the reason I left aside the software-based solution although it offers unicast-only mode. If that assumption is correct, then the software-based VXLAN cannot be a DCI solution as I explained in these posts below.<br />\u00a0<br />In short, if we want to extend a set of VLAN across 2 or more locations using VXLAN as the DCI protocol transport with only one L2 gateway that exists for the whole VXLAN domain, how a remote VLAN to be extended can hit the active L2 gateway. The ToR-based VXLAN can support today distributed active L2 gateways but does not offer currently a CP for the distribution and learning process.<br />26 \u2013 Is VxLAN a DCI solution for LAN extension?<br />http://yves-louis.com/DCI/?p=648 and<br />26-bis \u2013 VxLAN VTEP GW: Software versus Hardware-based<br />http://yves-louis.com/DCI/?p=742<br />",
          "id": "683042383479102558",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/09348068006439724629",
          "pub": "2014-10-21T08:03:32.466+02:00",
          "ref": "6907903599439189521",
          "type": "comment"
        },
        {
          "date": "21 October 2014 08:05",
          "html": "Ivan&gt;&gt; More importantly, you haven\u2019t addressed my major concern: OTV requires a DC-wide bridging domain because it sits at the edge of a data center (and I will defer the LISP discussion for some other time ;).<br />\u00a0<br />yves &gt;&gt; OTV is very flexible and actually it can sit at the aggregation PoD&#39;s\u00a0 if we wish, where the L2/L3 boundary exists (maintaining the DC core a pure L3 network). If we look at the evolution of the network fabric, VXLAN intra-DC is one solution. We can contain the failure domain inside the ToR, extend the VLAN intra-DC using a VXLAN overlay from the ToR leaf and terminate the VXLAN tunnel at the DCI layer, and finally map the original VLAN to the OTV overlay for example, hence the bridging domain keeps limits to the access PoD and not widely spread across the whole DC infrastructure.<br />VLAN \u21d0ToR_VTEP\u21d2 VXLAN \u21d0Core_VTEP\u21d2 VLAN \u21d0DCI_OTV\u21d2",
          "id": "7368621365161724114",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/09348068006439724629",
          "pub": "2014-10-21T08:05:07.858+02:00",
          "ref": "6907903599439189521",
          "type": "comment"
        },
        {
          "date": "21 October 2014 08:05",
          "html": "Ivan&gt;&gt; Finally, while I understand your excitement regarding the long-distance ASA cluster, do tell me what happens when the DCI link fails?<br />\u00a0<br />yves&gt;&gt; This is a fair comment. Actually there are two levels of LAN extension when deploying Act/Act FW clustering stretched across multiple locations, one level for the ASA clustering (extending the cluster control link) and one level for the Apps and user data (traditional DCI), obviously all redundant network connections for a resilient solution must exist, but let\u2019s assume the DCI dies.<br />\u00a0<br />The key components are that 1st of all, all FW appliances are active and secondly there is a way to control dynamically the direction In &amp; Out to the right DC supporting the Apps of interest.<br />\u00a0<br />Hence the best is a combination of ASA clustering stretched across two sites (using a single cluster control plane) with the LAN extension (such OTV offering stateful \u201chot\u201d live migration), in conjunction with LISP IP mobility (ingress redirection) and FHRP localization (egress redirection).<br />\u00a0<br />If the DCI breaks, we will get a split brain like for any other clusters. But the main difference is that with the ASA clustering, all FW appliances (phy or virtual) are active, and thanks to LISP IP mobility and FHRP isolation, the traffic will follow the location of the apps per site and will stay stuck to the site where the apps resides as it will not be possible to address hot live migration anymore. However Disaster Recovery with cold migration should be supported. Both DC becomes two active independent DC\u2019s while the same security rules will remain intact on both sites, but now with two FW control planes.<br />\u00a0<br />More to read in this article<br />27 \u2013 Bis \u2013 Path Optimisation with ASA cluster stretched across long distances \u2013 Part 2<br />http://yves-louis.com/DCI/?p=856<br />\u00a0",
          "id": "8180185235780599069",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/09348068006439724629",
          "pub": "2014-10-21T08:05:55.758+02:00",
          "ref": "6907903599439189521",
          "type": "comment"
        }
      ],
      "date": "20 October 2014 15:45",
      "html": "VXLAN is an overlay network that aims to extend Layer 2 over an IP network; that doesn&#39;t mean that this is a DCI solution. Same statement could be made with L2TPv3 or EoMPLS, just to list few, they aim to extend L2 segments, but are not natively DCI solution per se. And that&#39;s the reason why OTV or EVPN exists to offer natively a solid DCI solution.\u00a0It\u2019s not just a feature-to-feature comparison. That requires above all an architectural comparison; As described in your drawing, VXLAN is an end-to-end encapsulation meaning that from a transport point of view, it is a flat architecture. OTV is a DC WAN edge encapsulation, this means that it is an hierarchical architecture, and according to this principal, it better isolates the failure domain inside the DC and it offers better scalability and convergences. <br /><br />With OTV and FHRP localization, you can definitively reduce the tromboning effect for E-W traffic (inter-tier Apps workflows) without the need to deploy LISP IP mobility or other southbound optimization services. The Layer 3 gateway of interest is active on each site thus the traffic between application tiers is contained inside each DC without hair-pining workflows across sites. This is currently challenging with VXLAN alone.<br /><br />With OTV, the UU are not flooded. And storm-control configuration is simplified, because of failure isolation functionality natively available with OTV (for broadcast and unknown unicast). Thus a VM with a 10GE throughput that becomes &quot;broadcast-crazy&quot; should not impact the secondary site if the right tools are implemented accordingly. This is not completely true with VXLAN unfortunately, and it might be very challenging to rate limit an IP Multicast group. As the result with the current implementation of VXLAN with no control plane, the failure domain is extended throughout the whole VXLAN domain with a huge risk to saturate the DCI links and impacting the remote sites.<br />Note that if VXLAN requires an IP multicast network intra-DC which may be accepted by the enterprise, it would also impose IP multicast enabled for inter-DC network, which might be more challenging for the enterprise and/or the SP managing the inter-site IP network.<br /><br />OTV in conjunction with LISP IP mobility and FHRP localization offers a real optimization for long distances between DC\u2019s. When LISP IP mobility is deployed with extended subnets, it is complementary to OTV but is not mandatory. It is a smart choice that the network mgr will take based on how the high latency induced by the long distances may impact the enterprise business. Anyhow, for most of enterprises deploying DCI, the WAN access network is an Intranet network where LISP is easy to deploy and rarely a global Internet core per se. <br /><br />What is said today may be a bit different in the future. Solutions for distributed virtual DC are evolving in order to improve sturdiness, efficiency and optimization, including FW clustering stretched across multiple locations. It is important for enterprises to pay attention to this fast evolution.<br /><br />Finally long story short, it is certainly worth reading the detailed posts I made that discuss about the current VXLAN release and DCI, including LISP mobility and other FW clustering articles.<br /><br />26 \u2013 Is VxLAN a DCI solution for LAN extension ?\u00a0<br /> http://yves-louis.com/DCI/?p=648<br />23 \u2013 LISP Mobility in a virtualized environment\u00a0<br /> http://yves-louis.com/DCI/?p=428<br />27 \u2013 Stateful Firewall devices and DCI challenges\u00a0<br /> http://yves-louis.com/DCI/?p=785<br />",
      "id": "6907903599439189521",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/09348068006439724629",
      "pub": "2014-10-20T15:45:58.326+02:00",
      "ref": "212506992052150272",
      "type": "comment"
    },
    {
      "date": "15 October 2015 19:16",
      "html": "Sadly Randall has set aside this time to embarrass himself publicly again. Sorry everyone. ",
      "id": "9090897749903609043",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2015-10-15T19:16:38.701+02:00",
      "ref": "212506992052150272",
      "type": "comment"
    }
  ],
  "count": 15,
  "id": "212506992052150272",
  "type": "post",
  "url": "2014/10/vxlan-and-otv-saga-continues.html"
}