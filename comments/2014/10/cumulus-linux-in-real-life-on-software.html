<div class="comments post" id="comments">
  <h4>11 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="6642876431116697441">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/10446907709223361732" rel="nofollow">Doronin Pavel </a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6642876431116697441" href="#6642876431116697441">28 October 2014 04:00</a>
              </span>
            </div>
            <div class="comment-content">http://puppetlabs.com/presentations/managing-cisco-devices-using-puppet</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7428312054303228436">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01335898315678692950" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7428312054303228436" href="#7428312054303228436">30 October 2014 18:47</a>
              </span>
            </div>
            <div class="comment-content">Would love to see more on this. Did management ok the deviation from big vendors up front, or did IT have to sell them on it? How were the comparisons made? Strictly capacity and costs?<br /><br />We are approaching a point where we need to replace a lot of very old infrastructure while at the same time our business is seeing the benefits of public cloud. The current roadmap is to build a private/hybrid converged infrastructure adjacent to the aging infrastructure and migrate.<br /><br />However... go with a large vendor product or build white box and maintain it ourselves (like Cumuls)? There are huge pros and cons with either model. I would love to hear how the various venders were selected, what was compared, and how the decision was arrived at. Also, since it&#39;s been in operation for a while, have there been any major challenges? Regrets? Would they do it again?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="992556116749314944">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c992556116749314944" href="#992556116749314944">31 October 2014 06:51</a>
              </span>
            </div>
            <div class="comment-content">If all you need are two switches (and you&#39;d have to be pretty big to need more, see also http://blog.ipspace.net/2014/10/all-you-need-are-two-top-of-rack.html) you shouldn&#39;t bother. The potential savings you might get by switching vendors will be more than offset by increased OpEx (not to mention longer time-to-production).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8714621695459359441">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01335898315678692950" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8714621695459359441" href="#8714621695459359441">05 November 2014 21:39</a>
              </span>
            </div>
            <div class="comment-content">Ivan, I read that article and we&#39;ve actually had these conversations internally. Some environments have adopted the full VMware suite of tools and the traditional networking is all now in the hypervisor. There are physical switches that terminate internet/WAN/other-edge, but that&#39;s about it. In those cases, we agree that next gen networks could be dramatically smaller. Good for the customers, but makes the network engineers nervous.<br /><br />On the flipside, some engineers have attended ACI roadshows and classes. Going with that model definitely seems to guarantee that network engineers will still have jobs. It is very hardware centric, I&#39;m told, with MP-BGP, route-reflectors and MPLS running underneath.<br /><br />And finally there is the do-it-all-yourself model with whitebox hardware, an overlay, an orchestrator and probably a lot of operational learning curve pain. (technical debt). I was curious if the people in this article provided any insight as to how much pain there was to going this route.<br /><br /></div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8965635310310894267">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8965635310310894267" href="#8965635310310894267">04 November 2014 02:16</a>
              </span>
            </div>
            <div class="comment-content">Disclaimer: I work for Arista<br />I&#39;d like to clarify one point, Arista EOS is based on a standard Linux distribution (Fedora), so you can rpm install (or pip install once you&#39;ve install python tools) standard Linux tools on an EOS box just as you would apt-get a Debian based switch OS.  Great podcast!</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="701805077954460750">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c701805077954460750" href="#701805077954460750">04 November 2014 08:08</a>
              </span>
            </div>
            <div class="comment-content">Perfectly valid point (and I like Arista&#39;s approach to &quot;let&#39;s use Linux as much as possible&quot;). <br /><br />However, you still configure EOS using session-based CLI (or REST API) and extract information with SHOW commands (although that&#39;s getting better now with eAPI), whereas you can do most of the things with command line (which is easier to automate) on Linux-based networking.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1156942111597043607">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09571175342652550804" rel="nofollow">DuaneO</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1156942111597043607" href="#1156942111597043607">04 November 2014 03:40</a>
              </span>
            </div>
            <div class="comment-content">no one ever talks about debugging issues on these things.  are we typing things like ip addr show, brctl show and sysctl -a (and more) to try to piece together that&#39;s going on in the box?<br /><br />  I don&#39;t mind doing this stuff on a test box or two I have in the lab, but on my prod network gear I&#39;ll take a good unified interface over fragmented utilities on my network hardware.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="242459799356850372">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c242459799356850372" href="#242459799356850372">04 November 2014 08:06</a>
              </span>
            </div>
            <div class="comment-content">Of course, what else would you use ;) Happy Linux troubleshooting :D</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2966756706291703061">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13848639114358999199" rel="nofollow">Simon Leinen</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2966756706291703061" href="#2966756706291703061">25 November 2014 14:31</a>
              </span>
            </div>
            <div class="comment-content">“you could switch the operating system if you don’t like the one you use today” (let’s ignore that there’s approximately one at the moment)<br /><br />Well, there&#39;s Cumulus Linux, and there&#39;s <a href="https://github.com/opennetworklinux/ONL" rel="nofollow">ONL</a> for some platforms (e.g. Accton/Edge-Core).  And most white-boxen have a &quot;non-white-box option&quot; where they bundle a proprietary/licensed OS, should you become so frustrated that you want to go back to that model (or for the day when SDN has become tired and someone invents &quot;HDN!!!111&quot;).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1288843583884357455">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01337635561488658974" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1288843583884357455" href="#1288843583884357455">03 March 2017 22:19</a>
              </span>
            </div>
            <div class="comment-content">We are discarding Cumulus OS from our network,; almost 50 switches from production core. It was too inconsistent with system interface and management of features, protocol is also way cluttered. But now, we have got into bigger trouble - What else can we install on this White-box switches? Is there something better than Cumulus OS for data center with most common L2/L3 switching features?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="3632388505054840900">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3632388505054840900" href="#3632388505054840900">04 March 2017 07:42</a>
              </span>
            </div>
            <div class="comment-content">If you want to have a traditional architecture, Cumulus OS seems to be by far the best alternative. You could try Big Cloud Fabric next for a totally different approach.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
