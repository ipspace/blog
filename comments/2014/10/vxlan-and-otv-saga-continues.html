<div class="comments post" id="comments">
  <h4>15 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="792463811799743973">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c792463811799743973" href="#792463811799743973">09 October 2014 14:22</a>
              </span>
            </div>
            <div class="comment-content">What I will never understand is the insistence to do l2 dci to begin with.  No matter what the solution is it will always be kludgy as its outside of ethernet&#39;s design scope.  Problems need to be solved at the proper place and app redundancy is not a network one in my opinion.  Am I off base?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="8709335208035101532">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8709335208035101532" href="#8709335208035101532">09 October 2014 15:02</a>
              </span>
            </div>
            <div class="comment-content">Neither will I ... or a few other people:<br /><br />http://blog.ipspace.net/2012/03/stretched-layer-2-subnets-server.html<br /><br />It is, however, more lucrative from vendors&#39; or system integrators&#39; perspective to sell more boxes (or more complex boxe) to the customer than telling them that they should fix their apps.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7370380735839905949">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13274292942859087623" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7370380735839905949" href="#7370380735839905949">15 October 2014 17:03</a>
              </span>
            </div>
            <div class="comment-content">While I agree that stretched layer 2 does not make sense from a networking perspective, your assertion that &quot;they should fix their apps&quot; lives in world of unicorns and fairies.  That will never happen.  There is not a chance that an Oracle ERP with 1000&#39;s of moving parts will ever work they way we wish it would (Ok maybe it is possible but how many millions would it take....). Just my prospective from higher ed.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4817950741070025542">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4817950741070025542" href="#4817950741070025542">15 October 2014 17:46</a>
              </span>
            </div>
            <div class="comment-content">Yep, and mission-critical apps will never move from mainframes to minicomputers (let alone PCs). Been there, heard that.<br /><br />It will just take a while longer in some environments than others.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5579131111211159677">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14904477081635905810" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5579131111211159677" href="#5579131111211159677">09 October 2014 19:44</a>
              </span>
            </div>
            <div class="comment-content">Nice responses to readers questions.I agree with the responses and additionally , state synchronization between the FWs in the both data centers and state sync between other stateful devices could be necessary if the RTO requirement is 0 or near 0 , in case one of the data center fail. <br /><br />But of course just you can do it does not mean you should do it. DCI link failures might cause more issue than what ot would solve.<br /><br />Also for the OTV edge devices ,what I would say to this commenter in addition to your excellent and meaningful response :) , although they are not single point of failure , they can not be used in load share scenario due to how it works to avoid forwarding loop. So the convergence time needs to be considered and maybe adjusted and also odd/even Vlan separation needs to be arranged carefully for multihoming ( Management complexity.! ).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="513248422472285225">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c513248422472285225" href="#513248422472285225">10 October 2014 02:04</a>
              </span>
            </div>
            <div class="comment-content">&quot;but you&#39;re protected from bridging failures going over your DCI&quot;<br /><br />You are only protected whilst nothing has failed. It doesn&#39;t matter what protocol/technology/features you are using, or what protection mechanisms you have in place. A faulty supervisor module can quite easily identify your single failure domain, I speak from experience ;) </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5745377697702235509">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03014282355010119539" rel="nofollow">Michael67</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5745377697702235509" href="#5745377697702235509">17 October 2014 22:41</a>
              </span>
            </div>
            <div class="comment-content">What about the rest? What do you think about H3C´s EVI or Huawei´s EVN?<br /><br />http://www.h3c.com/portal/download.do?id=1798081<br />http://enterprise.huawei.com/ilink/cnenterprise/download/HW_341904</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1035559682070520422">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1035559682070520422" href="#1035559682070520422">18 October 2014 12:46</a>
              </span>
            </div>
            <div class="comment-content">Same set of problems. Some things (like squaring the circle) cannot be solve using rational means.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6907903599439189521">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09348068006439724629" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6907903599439189521" href="#6907903599439189521">20 October 2014 15:45</a>
              </span>
            </div>
            <div class="comment-content">VXLAN is an overlay network that aims to extend Layer 2 over an IP network; that doesn&#39;t mean that this is a DCI solution. Same statement could be made with L2TPv3 or EoMPLS, just to list few, they aim to extend L2 segments, but are not natively DCI solution per se. And that&#39;s the reason why OTV or EVPN exists to offer natively a solid DCI solution. It’s not just a feature-to-feature comparison. That requires above all an architectural comparison; As described in your drawing, VXLAN is an end-to-end encapsulation meaning that from a transport point of view, it is a flat architecture. OTV is a DC WAN edge encapsulation, this means that it is an hierarchical architecture, and according to this principal, it better isolates the failure domain inside the DC and it offers better scalability and convergences. <br /><br />With OTV and FHRP localization, you can definitively reduce the tromboning effect for E-W traffic (inter-tier Apps workflows) without the need to deploy LISP IP mobility or other southbound optimization services. The Layer 3 gateway of interest is active on each site thus the traffic between application tiers is contained inside each DC without hair-pining workflows across sites. This is currently challenging with VXLAN alone.<br /><br />With OTV, the UU are not flooded. And storm-control configuration is simplified, because of failure isolation functionality natively available with OTV (for broadcast and unknown unicast). Thus a VM with a 10GE throughput that becomes &quot;broadcast-crazy&quot; should not impact the secondary site if the right tools are implemented accordingly. This is not completely true with VXLAN unfortunately, and it might be very challenging to rate limit an IP Multicast group. As the result with the current implementation of VXLAN with no control plane, the failure domain is extended throughout the whole VXLAN domain with a huge risk to saturate the DCI links and impacting the remote sites.<br />Note that if VXLAN requires an IP multicast network intra-DC which may be accepted by the enterprise, it would also impose IP multicast enabled for inter-DC network, which might be more challenging for the enterprise and/or the SP managing the inter-site IP network.<br /><br />OTV in conjunction with LISP IP mobility and FHRP localization offers a real optimization for long distances between DC’s. When LISP IP mobility is deployed with extended subnets, it is complementary to OTV but is not mandatory. It is a smart choice that the network mgr will take based on how the high latency induced by the long distances may impact the enterprise business. Anyhow, for most of enterprises deploying DCI, the WAN access network is an Intranet network where LISP is easy to deploy and rarely a global Internet core per se. <br /><br />What is said today may be a bit different in the future. Solutions for distributed virtual DC are evolving in order to improve sturdiness, efficiency and optimization, including FW clustering stretched across multiple locations. It is important for enterprises to pay attention to this fast evolution.<br /><br />Finally long story short, it is certainly worth reading the detailed posts I made that discuss about the current VXLAN release and DCI, including LISP mobility and other FW clustering articles.<br /><br />26 – Is VxLAN a DCI solution for LAN extension ? <br /> http://yves-louis.com/DCI/?p=648<br />23 – LISP Mobility in a virtualized environment <br /> http://yves-louis.com/DCI/?p=428<br />27 – Stateful Firewall devices and DCI challenges <br /> http://yves-louis.com/DCI/?p=785<br /></div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5687211153730648363">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5687211153730648363" href="#5687211153730648363">20 October 2014 16:35</a>
              </span>
            </div>
            <div class="comment-content">Hi Yves!<br /><br />Thanks for an extensive reply. We agree on quite a few topics, and will have to agree to disagree on a few others.<br /><br />First a nit: most overlay virtual networking implementations no longer require multicast, the only exception being vShield suite (which is a zombie anyway).<br /><br />More importantly, you haven&#39;t addressed my major concern: OTV requires a DC-wide bridging domain because it sits at the edge of a data center (and I will defer the LISP discussion for some other time ;).<br /><br />Finally, while I understand your excitement regarding the long-distance ASA cluster, do tell me what happens when the DCI link fails?<br /><br />Kind regards,<br />Ivan</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8804183978747364483">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09348068006439724629" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8804183978747364483" href="#8804183978747364483">20 October 2014 19:31</a>
              </span>
            </div>
            <div class="comment-content">This comment has been removed by the author.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="683042383479102558">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09348068006439724629" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c683042383479102558" href="#683042383479102558">21 October 2014 08:03</a>
              </span>
            </div>
            <div class="comment-content">/ <br />Let me split the answers in 3 different replies, otherwise it seems that the whole response is too long<br />/<br />Thank you Ivan for the fast reply. Let me give you here my thoughts. Please note that I’m replying to the comments made previously only, but several other requirements to build a solid DCI are described in the post I mentioned above.<br /> <br />Ivan&gt;&gt; First a nit: most overlay virtual networking implementations no longer require multicast, the only exception being vShield suite (which is a zombie anyway).<br /> <br />yves &gt;&gt; yes and no :) - But this point is crucial.<br /> <br />From a software-based VXLAN point of view, there are indeed several vendors who already offer unicast-only mode.<br />From a ToR-based VXLAN point of view, as far as I’m aware, most of vendors are still enabling VXLAN in Multicast mode (TBC).<br />We will certainly comeback in the near future to this last statement, but as I said things are moving quickly however I’m talking about what is available today.<br /> <br />Back to the software-based VXLAN. Leaving aside discussions on scalability and performances, no one wants/can deploy a fully virtual environment, it&#39;s always an hybrid network with communication with traditional platforms, hence we need VTEP Gateway (L2) for the VLAN ID &lt;=&gt; VNI mapping. Unfortunately, currently only one single active L2 gateway per VXLAN domain is supported in software-based VTEP. Thus the reason I left aside the software-based solution although it offers unicast-only mode. If that assumption is correct, then the software-based VXLAN cannot be a DCI solution as I explained in these posts below.<br /> <br />In short, if we want to extend a set of VLAN across 2 or more locations using VXLAN as the DCI protocol transport with only one L2 gateway that exists for the whole VXLAN domain, how a remote VLAN to be extended can hit the active L2 gateway. The ToR-based VXLAN can support today distributed active L2 gateways but does not offer currently a CP for the distribution and learning process.<br />26 – Is VxLAN a DCI solution for LAN extension?<br />http://yves-louis.com/DCI/?p=648 and<br />26-bis – VxLAN VTEP GW: Software versus Hardware-based<br />http://yves-louis.com/DCI/?p=742<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7368621365161724114">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09348068006439724629" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7368621365161724114" href="#7368621365161724114">21 October 2014 08:05</a>
              </span>
            </div>
            <div class="comment-content">Ivan&gt;&gt; More importantly, you haven’t addressed my major concern: OTV requires a DC-wide bridging domain because it sits at the edge of a data center (and I will defer the LISP discussion for some other time ;).<br /> <br />yves &gt;&gt; OTV is very flexible and actually it can sit at the aggregation PoD&#39;s  if we wish, where the L2/L3 boundary exists (maintaining the DC core a pure L3 network). If we look at the evolution of the network fabric, VXLAN intra-DC is one solution. We can contain the failure domain inside the ToR, extend the VLAN intra-DC using a VXLAN overlay from the ToR leaf and terminate the VXLAN tunnel at the DCI layer, and finally map the original VLAN to the OTV overlay for example, hence the bridging domain keeps limits to the access PoD and not widely spread across the whole DC infrastructure.<br />VLAN ⇐ToR_VTEP⇒ VXLAN ⇐Core_VTEP⇒ VLAN ⇐DCI_OTV⇒</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8180185235780599069">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09348068006439724629" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8180185235780599069" href="#8180185235780599069">21 October 2014 08:05</a>
              </span>
            </div>
            <div class="comment-content">Ivan&gt;&gt; Finally, while I understand your excitement regarding the long-distance ASA cluster, do tell me what happens when the DCI link fails?<br /> <br />yves&gt;&gt; This is a fair comment. Actually there are two levels of LAN extension when deploying Act/Act FW clustering stretched across multiple locations, one level for the ASA clustering (extending the cluster control link) and one level for the Apps and user data (traditional DCI), obviously all redundant network connections for a resilient solution must exist, but let’s assume the DCI dies.<br /> <br />The key components are that 1st of all, all FW appliances are active and secondly there is a way to control dynamically the direction In &amp; Out to the right DC supporting the Apps of interest.<br /> <br />Hence the best is a combination of ASA clustering stretched across two sites (using a single cluster control plane) with the LAN extension (such OTV offering stateful “hot” live migration), in conjunction with LISP IP mobility (ingress redirection) and FHRP localization (egress redirection).<br /> <br />If the DCI breaks, we will get a split brain like for any other clusters. But the main difference is that with the ASA clustering, all FW appliances (phy or virtual) are active, and thanks to LISP IP mobility and FHRP isolation, the traffic will follow the location of the apps per site and will stay stuck to the site where the apps resides as it will not be possible to address hot live migration anymore. However Disaster Recovery with cold migration should be supported. Both DC becomes two active independent DC’s while the same security rules will remain intact on both sites, but now with two FW control planes.<br /> <br />More to read in this article<br />27 – Bis – Path Optimisation with ASA cluster stretched across long distances – Part 2<br />http://yves-louis.com/DCI/?p=856<br /> </div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="9090897749903609043">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9090897749903609043" href="#9090897749903609043">15 October 2015 19:16</a>
              </span>
            </div>
            <div class="comment-content">Sadly Randall has set aside this time to embarrass himself publicly again. Sorry everyone. </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
