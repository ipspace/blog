{
   "comments": [
      {
         "date": "22 September 2023 09:52",
         "html": "<p>Thanks for the re-post, I would not have seen this otherwise. :-)</p>\n\n<p>Regarding &quot;EVPN [&hellip;] has some sort of loop protection mechanism&quot;, EVPN has loop prevention inside the EVPN network, but a loop outside the EVPN part of network can still affect the whole EVPN network.</p>\n\n<p>Regarding the complexity of EVPN implementations, I have seen a bug inside a vendor implementation create a loop inside the EVPN network, with the usual results.</p>\n\n<p>All in all I concur, large L2 domains are bad.</p>\n",
         "id": "1927",
         "name": "Erik Auerswald",
         "pub": "2023-09-22T09:52:49",
         "type": "comment"
      },
      {
         "date": "22 September 2023 07:44",
         "html": "<p>So I would argue that EVPN/VXLAN is another proof for RFC 1925 rule 6a.</p>\n",
         "id": "1928",
         "name": "Anonymous",
         "pub": "2023-09-22T19:44:51",
         "type": "comment"
      },
      {
         "date": "25 September 2023 03:11",
         "html": "<p>This post echos my viewpoint as well; you&#39;re probably well-aware of it by now. Of course it won&#39;t happen due to massive conflict of interest between vendors and customers. </p>\n\n<p>&quot;Fewer features means fewer places where problems can occur.&quot;</p>\n\n<p>This is a cold, hard fact, not even an opinion. The more parts (physical or otherwise), the bigger the probability of something failing at some point, let alone a failure that results from interaction of features, which is exponentially harder to troubleshoot.</p>\n\n<p>We&#39;ve been running EVPN with MLAG some 2 yrs now. Already there&#39;re cryptic errors happening no one knows why. In one case, it manifests in printers unable to email big scan jobs -- small ones go through fine. We tried lots of things, from the network to the Exchange Hub transport and mail servers. Some 8 months later, a colleague accidentally found out there seemed to be some problem with one of the MLAG links. Rebooting the switch having that MLAG fixed the problem. Still, no one knows why it manifested in that manner. And this is just one example. Feature-creep is just bad. Stick to the basics. KISS.</p>\n",
         "id": "1930",
         "name": " Minh",
         "pub": "2023-09-25T03:11:57",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "27 September 2023 05:00",
               "html": "<p>My cynical (you know me ;) take:</p>\n\n<ul>\n<li>The fabric wars were all about &quot;how could I extract the maximum revenue based on the stupid thing VMware is making customers do&quot;.</li>\n<li>The SDN/OpenFlow pipe dream was really about &quot;how do I make vendors deliver switches with enough of low-level functionality exposed so that I can easily change them for the cheapest offering&quot;. It worked well only for the organization that started the stampede, and that only because they already had a huge software engineering team. Most everyone else got what one gets when believing in fairy tales.</li>\n</ul>\n",
               "id": "1938",
               "name": "Ivan Pepelnjak",
               "pub": "2023-09-27T17:00:09",
               "ref": "1935",
               "type": "comment"
            }
         ],
         "date": "26 September 2023 06:41",
         "html": "<p>Good post, didn&#39;t we go through this L2 is bad so lets try to evolve(kludge it) it with the Fabric wars of 2009-13(FabricPath, Trill, etc )and then our SDN Open flow pipeline dreams of 2013-15?</p>\n",
         "id": "1935",
         "name": " jsicuran",
         "pub": "2023-09-26T18:41:32",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "27 September 2023 04:35",
               "html": "<p>Not if you do them right (= loopback interface advertised on all uplinks with a routing protocol). Of course that&#39;s not how VMware does stuff, see https://blog.ipspace.net/2020/02/do-we-need-complex-data-center-switches.html for details.</p>\n",
               "id": "1937",
               "name": "Ivan Pepelnjak",
               "pub": "2023-09-27T16:35:08",
               "ref": "1936",
               "type": "comment"
            },
            {
               "date": "28 September 2023 12:30",
               "html": "<p>Overlays inside virtualization hosts do not need to emulate transparent bridging (see, e.g., Azure).</p>\n\n<p>The common interface between servers and the network is IP over Ethernet, thus an overlay implemented in the network for the usual virtualization use cases&sup1; emulates transparent bridging. This also emulates all the problems of transparent bridging and creates a single failure domain.</p>\n\n<p>&sup1; e.g., moving a VM to a different host without changing IP address(es) of the VM</p>\n",
               "id": "1939",
               "name": "Erik Auerswald",
               "pub": "2023-09-28T12:30:10",
               "ref": "1936",
               "type": "comment"
            }
         ],
         "date": "27 September 2023 11:19",
         "html": "<p>&quot;Use pure L3 routing. No overlay in the fabric . All overlay should be inside the servers - in SDNs.&quot;\nWould running overlays (that emulate L2) inside virtualization hosts not have the same issues as running the overlay in the fabric?\nMainly referring to NSX here (although not specified by the author), and despite the fact that you cannot use routing between TOR and NSX TEP on ESX host.</p>\n",
         "id": "1936",
         "name": " Bram",
         "pub": "2023-09-27T11:19:44",
         "type": "comment"
      }
   ],
   "count": 5,
   "type": "post",
   "url": "2023/09/l2-bad.html"
}
