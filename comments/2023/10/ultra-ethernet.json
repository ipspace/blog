{
   "comments": [
      {
         "comments": [
            {
               "date": "04 October 2023 08:55",
               "html": "<p>From what I see, DC is a blanket term that has lost meaning, just like &#39;Cloud&#39;. Different kinds of DC have different kinds of workload and traffic pattern. Flowlets switching, including Cisco&#39;s own solution (forgot its name) are not designed for the scale of HPC, so they&#39;re untested solutions for that kind of environment and likely won&#39;t scale. </p>\n\n<p>Turbulence modeling in particular, is extremely data and network intensive due to the nature of Turbulence -- the number of degrees of freedom scale exponentially with model size, which is why weather forecasting cannot forecast very far, while climate modeling is totally hopeless. These are the ultimate Big Data scenarios, and as usual, almost never get mentioned in mainstream press because they&#39;re super hard and not trendy. But they require as much horsepower as the biggest supercomputer fabrics can muster just to solve a tiny fraction of their problems. So claims that AI/ML require more processing power than what comes before them -- which is the impression I&#39;m getting these days -- don&#39;t stand up to scrutiny.</p>\n\n<p>HPC DCs that work on things like Turbulence often use Dragonfly Fabric Topology -- to minimize network diameter -- with per-packet Adaptive Routing, so asymmetric routing and out-of-order delivery become a big problem the higher the fabric gets utilized. </p>\n\n<p>Circuit switching using optical paths does away with reordering, but it&#39;s physically expensive and therefore doesn&#39;t not get implemented in the big HPC fabrics, and is not mentioned in the Broadcom solution either. </p>\n",
               "id": "1954",
               "name": " Minh",
               "pub": "2023-10-04T08:55:28",
               "ref": "1952",
               "type": "comment"
            }
         ],
         "date": "03 October 2023 01:44",
         "html": "<p>i think that the amount of reordering caused by packet spraying in data centers is widely overestimated. see also: https://engineering.purdue.edu/~ychu/publications/infocom13_pktspray.pdf\nif one finds that the paths are too asymmetrical in delay, flowlets or something more advanced such as hula might be thinkable. \nat that scale though, it might be worth looking into optical circuit switching.</p>\n",
         "id": "1952",
         "name": " tmp",
         "pub": "2023-10-03T13:44:04",
         "type": "comment"
      }
   ],
   "count": 1,
   "type": "post",
   "url": "2023/10/ultra-ethernet.html"
}
