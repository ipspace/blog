<div class="comments post" id="comments">
  <h4>2 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1995">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1995" href="#1995">16 November 2023 09:58</a>
              </span>
            </div>
            <div class="comment-content"><p>It reads like they are going to migrate from centralized control plane to distributed control plane and aiming for partition tolerance. But according to CAP theorem they will have to either give up availability or consistency. At the same time they want to stick with &quot;the high availability cluster (if they rely on any of our core data centers)&quot;. To me these look like conflicting goals. Also it seems to me they are caught in a complexity trap. Thought that shop would do better specially with testing, recovery planning and organization.  </p>

<p>BTW I know of a outsourcing provider which test their diesel generators and UPS every month because they know for sure that their crappy &quot;active-active redundant data clusters&quot; will not survive a complete outage of one data center. So their weakest &quot;link&quot; really is electricity. They also have an impressive stock of diesel at the facility to be able to run independently for days.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1996">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Bela Varkonyi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1996" href="#1996">17 November 2023 09:52</a>
              </span>
            </div>
            <div class="comment-content"><p>As an auditor, I always tell if an individual server, router, switch, or other infrastructure element has a long uptime, that means for me that you have not exercised your redundancy failover as you should. 
Since they could be unforeseen surprises in complex systems (because people will never follow rules and principles properly), it is important to have frequent artificially induced failures in your production system. Then you should document the results of such exercises and make corrective actions. 
It is like with regular fire alarm exercises in office buildings. 
For critical infrastructure, a monthly or weekly major facility failure exercise is highly recommended. Individual server reboots might be done according to a rolling schedule almost each day.</p>

<p>BTW, it would also help with undetected errors in large memories. Your laptop memory is designed to be rebooted each night. Your server has ECC memory, but it still not a perfect protection. You might also have software memory leaks or other one time events, such as solar flares. So regular failure exercises and server reboots would also provide you a clean start and reduce your memory issues, too.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
