comments:
- date: 15 August 2009 23:03
  html: I don&#39;t drink this Cisco Kool Aid about interconnecting data centres using
    an IP backbone.  Rather use FC directly over DWDM instead of FCIP on MPLS.
  id: '1303836925686682816'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ronald Bartels
  profile: null
  pub: '2009-08-15T23:03:23.000+02:00'
  ref: '2921265287027413665'
  type: comment
- date: 16 August 2009 10:03
  html: In principle I agree with you ... transporting something natively is always
    better and cleaner than tunneling/encapsulation schemes. But native FC requires
    dark fiber and DWDM gear. If you have both, you&#39;d be stupid to use FCIP, if
    you can&#39;t get one or the other one is too expensive, you have to consider
    the alternatives.
  id: '3981479220644911030'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2009-08-16T10:03:30.000+02:00'
  ref: '2921265287027413665'
  type: comment
- date: 19 August 2009 14:45
  html: One aspect of extending broadcast domains (VLANs) between Data Centres is
    operational control of items like STP and bcast storms over this distance - I
    know cases where what was to be Disaster Recovery setup ended up in melt-down
    of both sites without any natural disaster. <br />New &#39;clustered&#39; things
    like VMWare ESX setups ask not only need sync/control/heartbeat VLANs extended,
    but also user/productive ones too. My feeling is that any reasonably sized environment
    will eventualy get into problem area, given some growth over time and constant
    changes..<br />Also, think about having consistency of HSRP with STP, included
    with which servers are active where - in ESX case this could even move automaticaly<br
    />There are so called geo-cluster technologies, e.g. with SUN or HPUX which can
    handle pure IP routing between DR sites, but this is not so cheap or simple to
    set-up. It is amazing how vendor landscape (server, OS, app vendors plus those
    like CISCO) is unwilling to work on solutions to remove expanding VLAN. Your post,
    Ivan, on missing &#39;TCP/IP session layer&#39; is very relevant in here too...<br
    />I can see why CISCO is happy to provide all the esoteric ways to &#39;bridge&#39;
    the distance - it forces enterprises to use more expensive equipment. Working
    on the other hand with other industry players to achieve pure &#39;IP only&#39;
    way is not that sexy, although was the initial CISCO &#39;mantra&#39;.
  id: '5973432720967205857'
  image: https://resources.blogblog.com/img/blank.gif
  name: Davor
  profile: null
  pub: '2009-08-19T14:45:01.000+02:00'
  ref: '2921265287027413665'
  type: comment
- date: 19 August 2009 18:44
  html: 'A lot of valid points. Thanks.<br /><br />As for &quot;why are server vendors
    using weird methods&quot;: the server SW/HW vendors (or application vendors) just
    want to get their job done and couldn&#39;t care less how their implementation
    will work in real life; it&#39;s important that it works in their lab and in the
    demo room. Once the sale is closed and the equipment delivered, the problem is
    &quot;transparently&quot; passed over to the networking team.<br /><br />Considering
    the recent head-to-head clashes in server, virtualization, UC and networking space,
    it&#39;s no wonder that Cisco is not too keen on educating other companies how
    to develop optimal implementations.'
  id: '8882348771006022229'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2009-08-19T18:44:52.000+02:00'
  ref: '2921265287027413665'
  type: comment
- date: 21 August 2009 06:58
  html: Another example of clueless OS vendor:<br /><br />http://blog.ioshints.info/2009/08/turn-switch-into-hub-microsoft-way.html
  id: '1562235267410269017'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2009-08-21T06:58:07.000+02:00'
  ref: '2921265287027413665'
  type: comment
- date: 18 November 2009 09:51
  html: If we go on customizing the network based on the black boxes (servers or any
    other appliance) that are connected to network devices, it will one day become
    nightmare to manage and even can&#39;t fallback and we will be blamed for not
    providing reliable network.<br /><br />Yes, we have to have adaptability but it
    should also be a collaborated effort from all domains.  Network teams have to
    deal with various operating systems running in various servers with multiple NICs
    with multiple high available implementations. In most cases we are reactive to
    incidents arising out of non-standard implementations.<br /><br />In my scenario,
    I have to connect two data centers within the same city using DCI (dark fiber
    / Cat 6500-VSS). As per Cisco design doc, I have to set aside 4 cat 6500-vss only
    for DCI, this is hard to justify to extend VLANs.<br /><br />Kindly see attached
    diagram for interconnecting DC using dark fiber and highlight if there are any
    caveats. <br /><br />Thanks, VJ
  id: '7176831012340420173'
  image: https://resources.blogblog.com/img/blank.gif
  name: Vijayaram Venkataraman
  profile: null
  pub: '2009-11-18T09:51:11.000+01:00'
  ref: '2921265287027413665'
  type: comment
- date: 18 November 2009 10:31
  html: Hi! Nothing too bad on the first look. If you&#39;d like to have a professional
    opinion, please contact our Professional Services team (see http://www.nil.com/go/contacts
    ).
  id: '3390276806885245547'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2009-11-18T10:31:06.000+01:00'
  ref: '2921265287027413665'
  type: comment
count: 7
id: '2921265287027413665'
type: post
url: 2009/08/data-center-interconnect.html
