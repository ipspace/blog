comments:
- comments:
  - date: 14 January 2015 09:19
    html: Thanks for the link. Updated the MPIO paragraph.
    id: '3036258676901068608'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-14T09:19:19.307+01:00'
    ref: '8104246003283560663'
    type: comment
  date: 13 January 2015 10:00
  html: 'Does your analyse include SMB3 Multichannel ?<br /><br />http://blogs.technet.com/b/josebda/archive/2012/05/13/the-basics-of-smb-multichannel-a-feature-of-windows-server-2012-and-smb-3-0.aspx<br
    /><br />NB : Samba implementation of SMB3 is a &quot;Work In Progress&quot;<br
    />http://blog.obnox.de/demo-of-smb3-multi-channel-with-samba/<br /><br />and it
    will take ages before finding it in off-the-shelves NAS boxes as long as everybody
    just check for CIFS/SMB compatibility although CIFS a the pre-2000 version of
    the MS protocol.<br />'
  id: '8104246003283560663'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: JP. Papillon
  profile: https://www.blogger.com/profile/02525316721072005803
  pub: '2015-01-13T10:00:15.570+01:00'
  ref: '4904961663557505089'
  type: comment
- comments:
  - date: 13 January 2015 15:52
    html: That feature solves suboptimal distribution of multiple flows, not load
      balancing of packets within a single flow.<br /><br />Blog post on that topic
      is already in the scheduling queue ;) - will add a link to Juniper&#39;s implementation.<br
      /><br />Thank you!
    id: '1389214183663114308'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-13T15:52:42.206+01:00'
    ref: '8177756939186012429'
    type: comment
  - date: 15 January 2015 04:13
    html: Hi Ivan,<br /><br />Juniper VCF actually uses an algorithm to detect TCP
      window transmissions and load balance each subflow/flowlet. So if you have a
      single elephant flow, it would be load balanced across all available uplinks.<br
      /><br />Doug
    id: '6205860614831376433'
    image: https://resources.blogblog.com/img/blank.gif
    name: Doug Hanks
    profile: null
    pub: '2015-01-15T04:13:27.260+01:00'
    ref: '8177756939186012429'
    type: comment
  - date: 15 January 2015 14:19
    html: Hi Doug,<br /><br />I&#39;m pretty familiar with the whole concept of flowlets,
      and have a hunch what can be done with in on Trident-2 chipset, but you still
      cannot send a single burst over two (or more) uplinks, which means that you&#39;re
      still limited by the bandwidth of a single uplink. <br /><br />Also, you cannot
      send two bursts in parallel, because that would cause packet reordering on the
      receiver end and kill TCP performance.<br /><br />Ivan
    id: '5014942377902331684'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-15T14:19:00.255+01:00'
    ref: '8177756939186012429'
    type: comment
  - date: 15 January 2015 18:54
    html: Hi Ivan,<br /><br />VCF looks at a single TCP flow and is able to create
      a table to keep track of the TX packets. If no data is transmitted, the counter
      is incremented. Basically if the counter is &gt;= the RTT time, we know that
      we&#39;re between TCP windows and the next transmission of packets is a new
      TCP window, so we can send it down a new uplink.<br /><br />In our case we see
      a burst as simply a transmission of traffic for a given TCP window. We&#39;ll
      transmit the entire TCP window down a single uplink, wait, then send the next
      TCP window down a different uplink. What you effectively have is a single ingress
      TCP flow that is hashed across a set of egress uplinks.<br /><br />We avoid
      packet reordering because VCF knows the network topology and RTT. We only hash
      flowlets when we know the topology is symmetrical and we can detect we&#39;re
      between TCP windows based on the RTT.<br /><br />It&#39;s a really cool technology.
      Hope that helps.
    id: '7312073349462035108'
    image: https://resources.blogblog.com/img/blank.gif
    name: Doug Hanks
    profile: null
    pub: '2015-01-15T18:54:29.160+01:00'
    ref: '8177756939186012429'
    type: comment
  - date: 15 January 2015 19:23
    html: 'Hi Ivan,<br /><br />I think you&#39;re making a point that from the perspective
      of the NAS or host, itself would be limited to a single uplink without the use
      of some sort of method such as multipath I/O or multiple TCP sessions.<br /><br
      />Agreed!<br /><br />VCF would only help in the case of hashing a single TCP
      flow into multiple flowlets when it egresses the TOR.<br /><br />We just need
      a method of creating flowlets from the Linux kernel, which should only be about
      20 lines of code. '
    id: '1774632854769127163'
    image: https://resources.blogblog.com/img/blank.gif
    name: Doug Hanks
    profile: null
    pub: '2015-01-15T19:23:13.893+01:00'
    ref: '8177756939186012429'
    type: comment
  date: 13 January 2015 13:46
  html: Juniper introduced a new feature called adaptive load balancing that helps
    with this exact issue. Take a read here - http://forums.juniper.net/t5/Data-Center-Technologists/Adaptive-Flowlet-Splicing-VCF-s-Fine-Grained-Adaptive-Load/ba-p/251674<br
    /><br />
  id: '8177756939186012429'
  image: https://resources.blogblog.com/img/blank.gif
  name: Darren
  profile: http://www.mellowd.co.uk/ccie
  pub: '2015-01-13T13:46:35.039+01:00'
  ref: '4904961663557505089'
  type: comment
- comments:
  - date: 13 January 2015 15:49
    html: Multilink PPP did a great job because it sliced frames into smaller fragments,
      sent fragments across all links in the bundle, and reassembled them at the other
      end. Doesn&#39;t work at speeds we consider reasonable for today&#39;s storage.
    id: '6005392853451930669'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-13T15:49:12.490+01:00'
    ref: '6233011813342804453'
    type: comment
  - date: 13 January 2015 17:09
    html: Is there a reason for that or just nobody cared to implement it ?<br />BTW,
      the captcha is stronger than I would like, or I&#39;m missing my human touch
      :)
    id: '118321418643107166'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2015-01-13T17:09:26.146+01:00'
    ref: '6233011813342804453'
    type: comment
  - date: 13 January 2015 17:12
    html: CPU resources. Too expensive for the fringe benefits you&#39;d get. Sometimes
      it&#39;s cheaper to buy faster ports.<br /><br />BTW, we all should be &quot;grateful&quot;
      to the infinite morons who think they can improve their rankings by posting
      irrelevant comments on anyone&#39;s blogs. No spammers - no captcha.
    id: '3997133820293066009'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-13T17:12:05.239+01:00'
    ref: '6233011813342804453'
    type: comment
  date: 13 January 2015 13:53
  html: Uf, and I thought multilink ppp did a good job. <br />I would say your &quot;must&quot;
    is stronger than I would like :)<br />-Carlos
  id: '6233011813342804453'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-01-13T13:53:51.204+01:00'
  ref: '4904961663557505089'
  type: comment
- comments:
  - date: 13 January 2015 15:47
    html: Sure. Fixed. Thank you!
    id: '8303884513315223741'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-13T15:47:52.040+01:00'
    ref: '1124657529918799197'
    type: comment
  date: 13 January 2015 14:32
  html: '&quot;Network attached storage uses TCP-based protocols (iSCSI, NFV, CIFS/SMB)
    to communicate with the attached hosts.&quot;<br /><br />Did you mean NFS instead
    of NFV?'
  id: '1124657529918799197'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/01337665810325556317
  pub: '2015-01-13T14:32:58.470+01:00'
  ref: '4904961663557505089'
  type: comment
- comments:
  - date: 14 January 2015 09:21
    html: Slicing the elephant is a solution to the problem assuming the slices are
      small enough ;) <br /><br />Every solution to a problem or even theory (including
      Newton mechanics, relativity or quantum mechanics) _usually_ works best under
      a reasonable set of assumptions (in our case, many small flows) and breaks down
      at border conditions (black holes etc.). It&#39;s thus important to know (A)
      when you&#39;re violating the assumptions and (B) how often those assumptions
      are violated.
    id: '7672372259989829899'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-14T09:21:55.150+01:00'
    ref: '2472099351778963984'
    type: comment
  - date: 14 January 2015 09:44
    html: I totally agree with that Ivan, it&#39;s more likely in a big cloud (as
      openstack tends to be made for) that this usecase will happen. <br /><br />What
      I meant was more in a case of &quot;per-vm-NFS/iSCSI-flow&quot; implementation
      of VMware in a SMB cloud for example, you wont get a real profit with this technique.
      <br /><br />But... at the end you&#39;re right, I think that would be a scenario
      for multipath I/O... :-)
    id: '450433304337618707'
    image: https://resources.blogblog.com/img/blank.gif
    name: Pirmin S
    profile: http://easyipv6.wordpress.com/
    pub: '2015-01-14T09:44:59.379+01:00'
    ref: '2472099351778963984'
    type: comment
  date: 14 January 2015 09:03
  html: Hi Ivan!<br /><br />Slice the elephant into smaller chunks is good enough
    to take it, but is not really a solution to the problem. If you have still one
    VM with way more vDisk accesses than another, and fills a physical link, then
    your still stuck in the base problem.<br /><br />I think slicing into smaller
    chunks is just a way (it should really be standard) to avoid murphy, i.e. when
    DRS moves all &quot;iops-eating&quot; VMs on the same host (DRS does no really
    base itself on iops/disk speed right?) and you got a big fat TCP stream in a result.
  id: '2472099351778963984'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pirmin s.
  profile: http://easyipv6.wordpress.com
  pub: '2015-01-14T09:03:43.087+01:00'
  ref: '4904961663557505089'
  type: comment
- comments:
  - date: 15 January 2015 14:28
    html: That solution (which, BTW, I totally like ;) solves the aggregate NAS bandwidth
      issue, but not the server-to-network issue.<br /><br />Also, keep in mind that
      Arista switches have pretty low number of flows, so you wouldn&#39;t be able
      to connect more than a few hundred servers to that solution (assuming only 2
      sessions per server, not session-per-VM approach).
    id: '8831401289945333876'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-01-15T14:28:43.179+01:00'
    ref: '1340488974410076965'
    type: comment
  - date: 15 January 2015 22:29
    html: At least I&#39;m not the only one who likes their solution :) But you&#39;re
      right, it does not solve the server-to-network issue.<br /><br />Although I
      haven&#39;t seen to many VM host that saturate a 40G link - which Coho does
      not support yet - but you might have come across some.
    id: '3267195489904158088'
    image: https://resources.blogblog.com/img/blank.gif
    name: Nikolai
    profile: null
    pub: '2015-01-15T22:29:09.643+01:00'
    ref: '1340488974410076965'
    type: comment
  date: 15 January 2015 09:29
  html: An other solution is to integrate some networking into your NAS box like Coho
    Data (http://www.cohodata.com/) is doing. They put an Arista Switch between the
    servers and the NAS, present a single IP on the outside and do traffic steering
    on the switch using Openflow. A VM is running on the switch to do parts of the
    work
  id: '1340488974410076965'
  image: https://resources.blogblog.com/img/blank.gif
  name: Nikolai
  profile: null
  pub: '2015-01-15T09:29:07.372+01:00'
  ref: '4904961663557505089'
  type: comment
- comments:
  - date: 14 June 2015 18:01
    html: http://blog.ipspace.net/2013/07/the-tools-that-i-use-drawings.html
    id: '1095160824056141771'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-06-14T18:01:15.337+02:00'
    ref: '4135179555450158637'
    type: comment
  date: 14 June 2015 14:03
  html: With which program you create this nice Pictures?
  id: '4135179555450158637'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-06-14T14:03:52.083+02:00'
  ref: '4904961663557505089'
  type: comment
count: 22
id: '4904961663557505089'
type: post
url: 2015/01/load-balancing-elephant-storage-flows.html
