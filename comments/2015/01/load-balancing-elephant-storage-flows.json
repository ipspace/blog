{
  "comments": [
    {
      "comments": [
        {
          "date": "14 January 2015 09:19",
          "html": "Thanks for the link. Updated the MPIO paragraph.",
          "id": "3036258676901068608",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-14T09:19:19.307+01:00",
          "ref": "8104246003283560663",
          "type": "comment"
        }
      ],
      "date": "13 January 2015 10:00",
      "html": "Does your analyse include SMB3 Multichannel ?<br /><br />http://blogs.technet.com/b/josebda/archive/2012/05/13/the-basics-of-smb-multichannel-a-feature-of-windows-server-2012-and-smb-3-0.aspx<br /><br />NB : Samba implementation of SMB3 is a &quot;Work In Progress&quot;<br />http://blog.obnox.de/demo-of-smb3-multi-channel-with-samba/<br /><br />and it will take ages before finding it in off-the-shelves NAS boxes as long as everybody just check for CIFS/SMB compatibility although CIFS a the pre-2000 version of the MS protocol.<br />",
      "id": "8104246003283560663",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "JP. Papillon",
      "profile": "https://www.blogger.com/profile/02525316721072005803",
      "pub": "2015-01-13T10:00:15.570+01:00",
      "ref": "4904961663557505089",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "13 January 2015 15:52",
          "html": "That feature solves suboptimal distribution of multiple flows, not load balancing of packets within a single flow.<br /><br />Blog post on that topic is already in the scheduling queue ;) - will add a link to Juniper&#39;s implementation.<br /><br />Thank you!",
          "id": "1389214183663114308",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-13T15:52:42.206+01:00",
          "ref": "8177756939186012429",
          "type": "comment"
        },
        {
          "date": "15 January 2015 04:13",
          "html": "Hi Ivan,<br /><br />Juniper VCF actually uses an algorithm to detect TCP window transmissions and load balance each subflow/flowlet. So if you have a single elephant flow, it would be load balanced across all available uplinks.<br /><br />Doug",
          "id": "6205860614831376433",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Doug Hanks",
          "profile": null,
          "pub": "2015-01-15T04:13:27.260+01:00",
          "ref": "8177756939186012429",
          "type": "comment"
        },
        {
          "date": "15 January 2015 14:19",
          "html": "Hi Doug,<br /><br />I&#39;m pretty familiar with the whole concept of flowlets, and have a hunch what can be done with in on Trident-2 chipset, but you still cannot send a single burst over two (or more) uplinks, which means that you&#39;re still limited by the bandwidth of a single uplink. <br /><br />Also, you cannot send two bursts in parallel, because that would cause packet reordering on the receiver end and kill TCP performance.<br /><br />Ivan",
          "id": "5014942377902331684",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-15T14:19:00.255+01:00",
          "ref": "8177756939186012429",
          "type": "comment"
        },
        {
          "date": "15 January 2015 18:54",
          "html": "Hi Ivan,<br /><br />VCF looks at a single TCP flow and is able to create a table to keep track of the TX packets. If no data is transmitted, the counter is incremented. Basically if the counter is &gt;= the RTT time, we know that we&#39;re between TCP windows and the next transmission of packets is a new TCP window, so we can send it down a new uplink.<br /><br />In our case we see a burst as simply a transmission of traffic for a given TCP window. We&#39;ll transmit the entire TCP window down a single uplink, wait, then send the next TCP window down a different uplink. What you effectively have is a single ingress TCP flow that is hashed across a set of egress uplinks.<br /><br />We avoid packet reordering because VCF knows the network topology and RTT. We only hash flowlets when we know the topology is symmetrical and we can detect we&#39;re between TCP windows based on the RTT.<br /><br />It&#39;s a really cool technology. Hope that helps.",
          "id": "7312073349462035108",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Doug Hanks",
          "profile": null,
          "pub": "2015-01-15T18:54:29.160+01:00",
          "ref": "8177756939186012429",
          "type": "comment"
        },
        {
          "date": "15 January 2015 19:23",
          "html": "Hi Ivan,<br /><br />I think you&#39;re making a point that from the perspective of the NAS or host, itself would be limited to a single uplink without the use of some sort of method such as multipath I/O or multiple TCP sessions.<br /><br />Agreed!<br /><br />VCF would only help in the case of hashing a single TCP flow into multiple flowlets when it egresses the TOR.<br /><br />We just need a method of creating flowlets from the Linux kernel, which should only be about 20 lines of code. ",
          "id": "1774632854769127163",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Doug Hanks",
          "profile": null,
          "pub": "2015-01-15T19:23:13.893+01:00",
          "ref": "8177756939186012429",
          "type": "comment"
        }
      ],
      "date": "13 January 2015 13:46",
      "html": "Juniper introduced a new feature called adaptive load balancing that helps with this exact issue. Take a read here - http://forums.juniper.net/t5/Data-Center-Technologists/Adaptive-Flowlet-Splicing-VCF-s-Fine-Grained-Adaptive-Load/ba-p/251674<br /><br />",
      "id": "8177756939186012429",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Darren",
      "profile": "http://www.mellowd.co.uk/ccie",
      "pub": "2015-01-13T13:46:35.039+01:00",
      "ref": "4904961663557505089",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "13 January 2015 15:49",
          "html": "Multilink PPP did a great job because it sliced frames into smaller fragments, sent fragments across all links in the bundle, and reassembled them at the other end. Doesn&#39;t work at speeds we consider reasonable for today&#39;s storage.",
          "id": "6005392853451930669",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-13T15:49:12.490+01:00",
          "ref": "6233011813342804453",
          "type": "comment"
        },
        {
          "date": "13 January 2015 17:09",
          "html": "Is there a reason for that or just nobody cared to implement it ?<br />BTW, the captcha is stronger than I would like, or I&#39;m missing my human touch :)",
          "id": "118321418643107166",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Anonymous",
          "profile": null,
          "pub": "2015-01-13T17:09:26.146+01:00",
          "ref": "6233011813342804453",
          "type": "comment"
        },
        {
          "date": "13 January 2015 17:12",
          "html": "CPU resources. Too expensive for the fringe benefits you&#39;d get. Sometimes it&#39;s cheaper to buy faster ports.<br /><br />BTW, we all should be &quot;grateful&quot; to the infinite morons who think they can improve their rankings by posting irrelevant comments on anyone&#39;s blogs. No spammers - no captcha.",
          "id": "3997133820293066009",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-13T17:12:05.239+01:00",
          "ref": "6233011813342804453",
          "type": "comment"
        }
      ],
      "date": "13 January 2015 13:53",
      "html": "Uf, and I thought multilink ppp did a good job. <br />I would say your &quot;must&quot; is stronger than I would like :)<br />-Carlos",
      "id": "6233011813342804453",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2015-01-13T13:53:51.204+01:00",
      "ref": "4904961663557505089",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "13 January 2015 15:47",
          "html": "Sure. Fixed. Thank you!",
          "id": "8303884513315223741",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-13T15:47:52.040+01:00",
          "ref": "1124657529918799197",
          "type": "comment"
        }
      ],
      "date": "13 January 2015 14:32",
      "html": "&quot;Network attached storage uses TCP-based protocols (iSCSI, NFV, CIFS/SMB) to communicate with the attached hosts.&quot;<br /><br />Did you mean NFS instead of NFV?",
      "id": "1124657529918799197",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/01337665810325556317",
      "pub": "2015-01-13T14:32:58.470+01:00",
      "ref": "4904961663557505089",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "14 January 2015 09:21",
          "html": "Slicing the elephant is a solution to the problem assuming the slices are small enough ;) <br /><br />Every solution to a problem or even theory (including Newton mechanics, relativity or quantum mechanics) _usually_ works best under a reasonable set of assumptions (in our case, many small flows) and breaks down at border conditions (black holes etc.). It&#39;s thus important to know (A) when you&#39;re violating the assumptions and (B) how often those assumptions are violated.",
          "id": "7672372259989829899",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-14T09:21:55.150+01:00",
          "ref": "2472099351778963984",
          "type": "comment"
        },
        {
          "date": "14 January 2015 09:44",
          "html": "I totally agree with that Ivan, it&#39;s more likely in a big cloud (as openstack tends to be made for) that this usecase will happen. <br /><br />What I meant was more in a case of &quot;per-vm-NFS/iSCSI-flow&quot; implementation of VMware in a SMB cloud for example, you wont get a real profit with this technique. <br /><br />But... at the end you&#39;re right, I think that would be a scenario for multipath I/O... :-)",
          "id": "450433304337618707",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Pirmin S",
          "profile": "http://easyipv6.wordpress.com/",
          "pub": "2015-01-14T09:44:59.379+01:00",
          "ref": "2472099351778963984",
          "type": "comment"
        }
      ],
      "date": "14 January 2015 09:03",
      "html": "Hi Ivan!<br /><br />Slice the elephant into smaller chunks is good enough to take it, but is not really a solution to the problem. If you have still one VM with way more vDisk accesses than another, and fills a physical link, then your still stuck in the base problem.<br /><br />I think slicing into smaller chunks is just a way (it should really be standard) to avoid murphy, i.e. when DRS moves all &quot;iops-eating&quot; VMs on the same host (DRS does no really base itself on iops/disk speed right?) and you got a big fat TCP stream in a result.",
      "id": "2472099351778963984",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Pirmin s.",
      "profile": "http://easyipv6.wordpress.com",
      "pub": "2015-01-14T09:03:43.087+01:00",
      "ref": "4904961663557505089",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "15 January 2015 14:28",
          "html": "That solution (which, BTW, I totally like ;) solves the aggregate NAS bandwidth issue, but not the server-to-network issue.<br /><br />Also, keep in mind that Arista switches have pretty low number of flows, so you wouldn&#39;t be able to connect more than a few hundred servers to that solution (assuming only 2 sessions per server, not session-per-VM approach).",
          "id": "8831401289945333876",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-01-15T14:28:43.179+01:00",
          "ref": "1340488974410076965",
          "type": "comment"
        },
        {
          "date": "15 January 2015 22:29",
          "html": "At least I&#39;m not the only one who likes their solution :) But you&#39;re right, it does not solve the server-to-network issue.<br /><br />Although I haven&#39;t seen to many VM host that saturate a 40G link - which Coho does not support yet - but you might have come across some.",
          "id": "3267195489904158088",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Nikolai",
          "profile": null,
          "pub": "2015-01-15T22:29:09.643+01:00",
          "ref": "1340488974410076965",
          "type": "comment"
        }
      ],
      "date": "15 January 2015 09:29",
      "html": "An other solution is to integrate some networking into your NAS box like Coho Data (http://www.cohodata.com/) is doing. They put an Arista Switch between the servers and the NAS, present a single IP on the outside and do traffic steering on the switch using Openflow. A VM is running on the switch to do parts of the work",
      "id": "1340488974410076965",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Nikolai",
      "profile": null,
      "pub": "2015-01-15T09:29:07.372+01:00",
      "ref": "4904961663557505089",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "14 June 2015 18:01",
          "html": "http://blog.ipspace.net/2013/07/the-tools-that-i-use-drawings.html",
          "id": "1095160824056141771",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-06-14T18:01:15.337+02:00",
          "ref": "4135179555450158637",
          "type": "comment"
        }
      ],
      "date": "14 June 2015 14:03",
      "html": "With which program you create this nice Pictures?",
      "id": "4135179555450158637",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2015-06-14T14:03:52.083+02:00",
      "ref": "4904961663557505089",
      "type": "comment"
    }
  ],
  "count": 22,
  "id": "4904961663557505089",
  "type": "post",
  "url": "2015/01/load-balancing-elephant-storage-flows.html"
}