comments:
- comments:
  - date: 21 April 2015 17:35
    html: You&#39;re absolutely right, although most solutions still use an anycast
      default gateway (same IP and MAC address present on all edge routers) just to
      make the host gets ARP replies when querying for default gateway MAC address.<br
      /><br />Things get a bit tricky regarding ARP - Windows Server (at least) uses
      unicast ARP to check remote host reachability, so Hyper-V and Amazon VPC reply
      to ARP requests with MAC address of remote host. Juniper Contrail uses the same
      MAC address for all IP addresses.
    id: '5952270462176904093'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-21T17:35:17.805+02:00'
    ref: '799257858706812197'
    type: comment
  date: 21 April 2015 14:44
  html: 'Help me think through this. What the host has configured for it&#39;s default
    gateway doesn&#39;t really matter, correct? Because the default gateway in traditional
    L2 access networks really isn&#39;t about the gateway&#39;s IP address, but the
    gateway&#39;s MAC address. The destination IP address in the packet header is
    always the end destination IP address, never the default gateway. The L3 access
    switch could just always route on the destination IP address ignoring the MAC
    destination in the Ethernet frame.  Am I thinking correctly?   '
  id: '799257858706812197'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: John Jackson
  profile: https://www.blogger.com/profile/14047803952264243329
  pub: '2015-04-21T14:44:27.750+02:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 22 April 2015 16:24
    html: Yep, that&#39;s another way of looking at it - and exactly what Cumulus
      Linux is doing with their Redistribute Neighbor feature.
    id: '7215101662903262601'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-22T16:24:42.666+02:00'
    ref: '2720379844456285362'
    type: comment
  - date: 22 April 2015 17:52
    html: What will then prevent an attacker to create a lot of IPv6 addressses and
      overload the routing protocol in the network?<br /><br />I did some measurement
      and in our campus network, there are more then 3 unique IPv6 addresses per a
      computer in 24 hour period due to privacy extension. Considering campus with
      5000 users, there will be more or less 15 000 of constantly changing routes.
      Which routing protocol is scalable enough to handle such large amount of prefixes
      and converges quickly? You are proposing BGP (iBGP?), but it means full mesh
      or route-reflector, convergence time could also be better etc. etc. :/
    id: '9008315016772712933'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2015-04-22T17:52:18.664+02:00'
    ref: '2720379844456285362'
    type: comment
  - date: 22 April 2015 18:08
    html: '#1 - You only need host routes within a mobility domain.<br />#2 - Existing
      L3 switches already have to handle that churn<br />#3 - Every routing protocol
      can carry 15K prefixes (OK, maybe not OSPF ;).<br />#4 - Update rates of 100/second
      are the norm on global Internet (see http://bgpupdates.potaroo.net/instability/bgpupd.html).
      Your iThingy users won&#39;t come anywhere close to that.<br />#5 - IBGP convergence
      times are not an issue these days. FIB update is the problem.'
    id: '2253043746843976424'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-22T18:08:36.153+02:00'
    ref: '2720379844456285362'
    type: comment
  - date: 22 April 2015 18:09
    html: '... oh, and on &quot;What will then prevent an attacker to create a lot
      of IPv6 addressses and overload the routing protocol in the network&quot; -
      what prevents an attacker to create a lot of MAC addresses causing network-wide
      unknown unicast flooding, or a lot of IPv6 addresses, causing ND cache overflows?
      Proper policing in first-hop switches.'
    id: '5875901059516993114'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-22T18:09:42.000+02:00'
    ref: '2720379844456285362'
    type: comment
  - date: 22 April 2015 18:45
    html: Thanks for the answers. I have doubts that the necessary functionality will
      be available anytime soon in switches for the access layer, but who knows. <br
      /><br />&quot;what prevents an attacker to create a lot of MAC addresses/nd
      cache overflow&quot; - in IPv4 world dhcp snooping, in IPv6 world unfortunately
      nothing as nd-snooping and other security features are missing or are not mature
      enough or are badly designed. Sigh.
    id: '1179232430123052591'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2015-04-22T18:45:57.305+02:00'
    ref: '2720379844456285362'
    type: comment
  date: 22 April 2015 15:37
  html: Do you mean that an access switch will create arp/nd cache and redistribute
    these entries as routes into a routing protocol?
  id: '2720379844456285362'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-04-22T15:37:45.210+02:00'
  ref: '7057435103960204679'
  type: comment
- date: 22 April 2015 21:54
  html: Ivan,<br />If we take the use case of virtualized Data center with all servers
    running Hypervisor of some sort then the number of End-VMs you can support seems
    to be limited by TCAM /Route table size of ToRs which it looks like 128000. <br
    />I have a question - If  number of VMs are less than 128K and it is what most
    of the large enterprises today have in their DC then why use Network Overlay technologies?
    I can think of only overlapping IP address as use case for using Network overlays.
    Any thoughts/comments on this??  I really don&#39;t see why one would use all
    these complicated Overlays since routing protocols are time tested and overly
    simple in terms of operations!!
  id: '4487263291596831058'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-04-22T21:54:09.283+02:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 23 April 2015 06:35
    html: I know and I think I said something along these lines during my Troopers
      IPv6 Microsegmentation talk. To add to irony, we had working TUBA code (running
      on Solaris and Cisco IOS) before they even agreed on IPv6 packet format. See
      http://tools.ietf.org/pdf/rfc1347.pdf<br /><br />
    id: '5411471234840152540'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-23T06:35:09.301+02:00'
    ref: '5265638209351606128'
    type: comment
  date: 23 April 2015 00:33
  html: What you describe is literally intra-area routing in CLNS.<br /><br />You
    need a protocol between routers. Some people seem to prefer BGP as the solution
    to any problem. But the most natural solution here, for IGP routing, would be
    to use an actual IGP. IS-IS would do a fine job. Not much worse than BGP. And
    less prone to configuration/network-design problems than BGP. You could advertise
    /32s. Or you could introduce a new TLV to make a proper destinction.<br /><br
    />It would be *minimal* work to actually implement this functionality into a proper
    IS-IS implementation. Just a small matter of programming.<br /><br />You need
    a protocol for the routers to find out which EndStations are directly connected.
    In CLNS, that would be ESIS. In IPv4 you could indeed use ARP. In IPv6 you could
    use NDP. Or you could build a new proper host-router protocol (and use ARP until
    it catches on). With proper security/authentication, VRRP-like and DHCP-like functionality.<br
    /><br />Of course all this will be impossible, because it reeks of CLNS. And therefor
    it must never ever be implemented in TCP/IP. Too bad, CLNS did it right.<br /><br
    />
  id: '5265638209351606128'
  image: https://resources.blogblog.com/img/blank.gif
  name: Henk
  profile: null
  pub: '2015-04-23T00:33:13.459+02:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 23 April 2015 19:41
    html: I would configure /24 on the clients (or anything along those lines) and
      let the switches handle the details.
    id: '5116592992687430292'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-23T19:41:08.336+02:00'
    ref: '2571833114755251107'
    type: comment
  date: 23 April 2015 19:27
  html: What would you configure as subnet mask on clients? I am thinking /31 on the
    hardware that supports it and something bigger with proxy arp on the hardware
    that doesn&#39;t support /31
  id: '2571833114755251107'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/02305236928990443506
  pub: '2015-04-23T19:27:31.229+02:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 24 August 2015 18:44
    html: Cumulus Linux has this functionality in experimental stage:<br /><br />http://blog.ipspace.net/2015/08/layer-3-only-data-center-networks-with.html<br
      /><br />Cisco has something similar in its DFA architecture (and probably in
      ACI as well). See also the links I published with the Cumulus podcast (link
      above) for even more details.
    id: '2611085034383709410'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-08-24T18:44:56.528+02:00'
    ref: '7353042378700757777'
    type: comment
  date: 14 May 2015 16:56
  html: I really like the idea of L3-Only network (fabric) but I wonder what would
    be the best way to push L3 down to hosts in leaf-and-spine arhitecture where you
    would have hosts connected to pair of ToR (leaf) switches for redundancy (instead
    of forming one L2 domain with MLAG for all hosts within a rack in combination
    with some kind of first hop redundancy protocol).<br /><br />Does this even make
    sense? <br />Any suggestions? <br />Would it make sense to limit host routes (&quot;mobility&quot;
    domain) to pair of ToR and advertise only /64 (in IPv6 world) with routing protocol
    to spine switches?<br /><br />I would image that hosts would have to run a routing
    protocol (or maybe not?) and the actual host adress would be assinged to loopback
    interface with default routes to both ToR switches or am I thinking in a totaly
    wrong direction? :)
  id: '7353042378700757777'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-05-14T16:56:08.853+02:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 15 March 2016 10:47
    html: Please be precise. What does &quot;path is switched&quot; means? The marketing
      teams of major vendors made that totally ambiguous.<br /><br />In any case,
      in a pure L3 network, all switches have to do full L3 lookup. No multi-hop L2
      anywhere. Problem solved.
    id: '6201046997552869339'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2016-03-15T10:47:18.115+01:00'
    ref: '659575226194015141'
    type: comment
  date: 15 March 2016 07:35
  html: Hi Ivan,<br /><br />I had a query. let us assume there are two leaf&#39;s
    L1 and L2 and two spines S1 and S2. Now assume that there are independent switches
    (no fabric) and we are trying to have only l3 inside these connections.<br /><br
    />A host connected to leaf L1 sends out an IP Packet. L1 routes the packet. Now
    let us assume that it reaches via the link to C2. Now C2 has to again route it
    and send it to L2 and L2 routes it to the end host H2 which was connected to it.<br
    />However, if we assume that the path from L1 to L2 is switched then we will have
    the same problem of L2 domains within the leaf-spine.<br />Is this understanding
    Correct?<br /><br />Sincerely,<br />Sudarsan.D
  id: '659575226194015141'
  image: https://resources.blogblog.com/img/blank.gif
  name: Sudarsan Duraiswamy
  profile: null
  pub: '2016-03-15T07:35:59.118+01:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 26 August 2016 08:30
    html: It wouldn&#39;t. One of the obsessions of our industry is to try to find
      a one-size-fits-everything solutions. It&#39;s like trying to design something
      that could be a mountain bike today and an Abrams tomorrow. Reality doesn&#39;t
      work that way.<br /><br />Most data centers don&#39;t have 128K guest VMs, and
      if you need more than that, you shouldn&#39;t be looking for solutions on public
      blog sites ;)
    id: '8036892339630061866'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2016-08-26T08:30:58.035+02:00'
    ref: '8420911626344337588'
    type: comment
  date: 25 August 2016 22:27
  html: Ivan,<br /><br />When dealing with guests on each host, if each host injects
    a /32 for each guest, by the time the routes are on the spine, you&#39;re potentially
    well past the 128k route limit. I&#39;ve read about overlays like vxlan but I
    still don&#39;t see how that would avoid the problem on the spines, as each spine
    would have to know about every /32, which could be on any one of the leaves below.
    Can you elaborate on how this can scale beyond 128k routes?
  id: '8420911626344337588'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Andrew
  profile: https://www.blogger.com/profile/16594624559681706818
  pub: '2016-08-25T22:27:06.559+02:00'
  ref: '7057435103960204679'
  type: comment
- comments:
  - date: 11 February 2017 21:23
    html: 'Unless you configure it otherwise, the hypervisor sends gratuitous ARP
      or RARP packet when the VM is moved on behalf of the VM.<br /><br />Here&#39;s
      a pretty good packet walk describing the nastier case (RARP instead of GARP):
      https://yves-louis.com/DCI/?p=1502'
    id: '799359363308764649'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-02-11T21:23:59.064+01:00'
    ref: '8555769434905684577'
    type: comment
  - date: 13 February 2017 19:11
    html: Hi Ivan,<br />The link you mention assumes that the DCI interconnect is
      L2. My query is specifically when the DCI is only L3.(L3DCI).<br /><br />Sincerely,<br
      />Sudarsan.D
    id: '8385704215996191645'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Sudarsan
    profile: https://www.blogger.com/profile/13569391474860461806
    pub: '2017-02-13T19:11:13.698+01:00'
    ref: '8555769434905684577'
    type: comment
  - date: 13 February 2017 19:12
    html: Similar, but wouldn&#39;t work with RARPs (but would work with ARPs)
    id: '8926099863625683671'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-02-13T19:12:28.756+01:00'
    ref: '8555769434905684577'
    type: comment
  date: 11 February 2017 18:50
  html: Hi Ivan,<br /><br />In a pure L3 only network where ever ToR/Spine switch
    does L3 forwarding, how is the routing to the end host done when the end host
    (VM) moves to another ToR and becomes silent as well.<br /><br />To be clear,
    let us assume there are two ToR T1 and T2.<br />Host H1 is connected to T1 and
    H2 to T2. H1 and H2 are in different subnets.<br />IP connectivity happens between
    H1 and H2 by the fact that the ToRs and the spines do IP forwarding.<br />Now
    let us assume that the host H2 moves to ToR1 and is also silent as well.<br />Now
    how does ToR1 detect this condition? Without the dection ToR1 would route the
    packet to one of thespines, which would again route it to ToR2 and ToR2 would
    probably blackhole the traffic.<br />Please let me know<br />Sincerely,<br />Sudarsan.D
  id: '8555769434905684577'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Sudarsan
  profile: https://www.blogger.com/profile/13569391474860461806
  pub: '2017-02-11T18:50:19.477+01:00'
  ref: '7057435103960204679'
  type: comment
count: 23
id: '7057435103960204679'
type: post
url: 2015/04/rearchitecting-l3-only-networks.html
