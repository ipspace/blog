comments:
- date: 02 April 2015 10:15
  html: You&#39;re in this field for too long, Ivan. You&#39;re seeing the flaws in
    everything.<br />That being said, a very interesting point. I never looked at
    it that way, and now I understand it better.
  id: '3410873371392061084'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-04-02T10:15:47.443+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 02 April 2015 10:59
  html: Connect your servers with a /31 to the local L3 device and either use the
    /31 local address as source or advertise a loopback and source from there. Need
    to move a server? No problem. You can even anycast like this. Nice and scalable
  id: '3223343665405151051'
  image: https://resources.blogblog.com/img/blank.gif
  name: Darren
  profile: https://www.mellowd.co.uk/ccie
  pub: '2015-04-02T10:59:44.614+02:00'
  ref: '1031056806778392101'
  type: comment
- comments:
  - date: 02 April 2015 14:37
    html: 10/8 gives you 16.7 million IPs. Half this for /31 and you can still address
      8.3 million servers.<br /><br />However I agree with your v6 statement
    id: '1637353617460246083'
    image: https://resources.blogblog.com/img/blank.gif
    name: Darren
    profile: https://www.mellowd.co.uk/ccie
    pub: '2015-04-02T14:37:18.148+02:00'
    ref: '1415844262978556559'
    type: comment
  date: 02 April 2015 13:19
  html: One concern (historically) was the burn of IPv4 addresses.  Even /30s or /31s
    in conjunction with RFC1918 wasn&#39;t enough for truly large environments.  IP
    Unnumbered helps, when/where supported ...  <br /><br />Additionally, legacy applications
    (think old school MS SLB) sometimes *required* broadcast reachability.  Bad decisions
    from the get-go, but sometimes reality fails to achieve the Best, or even a Good,
    answer(s).<br /><br />IPv6 really makes this easy - Link Local addresses are already
    there, and GUAs are (effectively) limitless as well.  <br /><br />And moving forward,
    methinks IP fabrics will take everything else over (atleast in the data center
    space).<br /><br /><br />/TJ
  id: '1415844262978556559'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/18433992102893508928
  pub: '2015-04-02T13:19:58.496+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 02 April 2015 17:13
  html: "You&#39;re saying what I have thought dozens of times. I just haven&#39;t\
    \ been courageous enough to try to design something better than ethernet \uFFFD\
    \uFFFD "
  id: '7237277305835064502'
  image: https://4.bp.blogspot.com/-JXMUyJFF5iY/UkmoOMGUIVI/AAAAAAAAAGk/6jDTL3tcsI4/s32/1240469_10202242095038376_1044617343_n.jpg
  name: Sander Steffann
  profile: https://www.blogger.com/profile/17446363221396052047
  pub: '2015-04-02T17:13:46.546+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 03 April 2015 03:16
  html: Pretend you are a device receiving a stream of bits.  After you receive some
    inter-frame spacing bits, whatever comes next is the 2nd layer; whether that is
    Ethernet, native IP, CLNS/CLNP, whatever.  Perhaps the question should be &quot;Are
    we using the right layer 2 protocol?&quot; rather than &quot;Why are we still
    using layer 2?&quot;
  id: '667786938914629841'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jeff Behrns
  profile: http://www.jeffbehrns.com
  pub: '2015-04-03T03:16:00.585+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 04 April 2015 13:55
  html: 'Let&#39;s ask another question: do we really need Layer 3 as seen in TCP/IP
    model?<br /><br />Named Data Networking:<br />http://named-data.net/project/archoverview/'
  id: '4826381041547820422'
  image: https://resources.blogblog.com/img/blank.gif
  name: Bogdan Golab
  profile: http://yahoo.com
  pub: '2015-04-04T13:55:20.285+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 04 April 2015 14:06
  html: It seems that Cisco is interested in NDN (a replacement for TCP/IP):<br /><br
    />http://www.networkworld.com/article/2602109/lan-wan/ucla-cisco-more-join-forces-to-replace-tcpip.html
  id: '7967959861276868572'
  image: https://resources.blogblog.com/img/blank.gif
  name: Bogdan Golab
  profile: http://yahoo.com
  pub: '2015-04-04T14:06:13.331+02:00'
  ref: '1031056806778392101'
  type: comment
- comments:
  - date: 06 April 2015 12:45
    html: Speaking of IPv6 microsegmentation ;) https://www.youtube.com/watch?v=2zvrzgGzyYw
    id: '460840837375566713'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-06T12:45:09.570+02:00'
    ref: '9157219165501432944'
    type: comment
  - date: 07 April 2015 10:45
    html: 'Layer 2 is the best to connect MPLS to SDH or DWDM or Mivrowve because
      it make line for trouble shooting '
    id: '6374231719887410541'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: abdulmajeed
    profile: https://www.blogger.com/profile/13944884585257800655
    pub: '2015-04-07T10:45:35.525+02:00'
    ref: '9157219165501432944'
    type: comment
  date: 05 April 2015 16:27
  html: First of all I&#39;m with you!.  But as someone who works to reduce layer
    2 sprawl with customers daily I would say the security discussion is missing.  Microsegmentation
    can replace (and improve on) the isolation of VLANs but that means you need a
    microsegmentation architecture to reduce layer 2.  Microsegmentation (for now)
    is difficult to implement without virtualization or similar abstraction of compute.  Also,
    even if you are super progressive with Docker, nesting and traditional virtualization
    everywhere; there will be some edge network devices and hypervisors that need
    to be placed on *some kind of layer 2 segment.  This does not mean we have to
    use the legacy protocols of the past.  Rather I think it means we need to revisit
    how to implement a progressive, reduced layer 2 footprint where appropriate and
    eliminate it wherever it is not necessary.  <br /><br />On the flip side, I would
    ask why layer 2 overlays (VxLAN et. al) are necessary at all, as given this argument
    it seems we are bringing some of the crutches of the past into the virtualized
    world.   Why not rely on microsegmentation and be done with it?
  id: '9157219165501432944'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: shaniac
  profile: https://www.blogger.com/profile/03997071858123545211
  pub: '2015-04-05T16:27:13.749+02:00'
  ref: '1031056806778392101'
  type: comment
- comments:
  - date: 09 April 2015 17:48
    html: Yeah, there was that... and I remember IP-over-SONET (POS?).
    id: '7590974918508672123'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-09T17:48:30.067+02:00'
    ref: '8798365536878128705'
    type: comment
  date: 09 April 2015 17:19
  html: 'I recall reading about IP directly over DWDM  at one point back in early
    00s. '
  id: '8798365536878128705'
  image: https://resources.blogblog.com/img/blank.gif
  name: jsicuran
  profile: http://www.amilabs.com
  pub: '2015-04-09T17:19:15.031+02:00'
  ref: '1031056806778392101'
  type: comment
- comments:
  - date: 12 April 2015 18:20
    html: L2 lookup is almost exactly as fast as L3 lookup these days... at least
      on decent hardware. OK, maybe there&#39;s a few nsec difference if your hardware
      supports cut-through switching.
    id: '6273435405792026073'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-04-12T18:20:10.478+02:00'
    ref: '5166720344740509836'
    type: comment
  date: 12 April 2015 17:30
  html: l2 is faster? every l3 hop adds delay?<br />faster for storage is better?
  id: '5166720344740509836'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-04-12T17:30:19.298+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 13 April 2015 13:45
  html: oh.. ok, thank you.
  id: '8864270737871545141'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2015-04-13T13:45:44.115+02:00'
  ref: '1031056806778392101'
  type: comment
- date: 24 April 2015 22:20
  html: Seems to me that for both servers and clients, particularly running legacy
    operating systems, the hardened stack runs over L2 using an Ethernet NIC.  While
    developing and hardening a new stack for Linux is feasible, for the legacy OS&#39;s
    I&#39;d argue any new stack is just not going to happen.<br /><br />The last two
    times I was involved (however tangentially) in alternate Layer 2&#39;s for the
    data center -- Fibre Channel and InfiniBand -- did not change the outcome of the
    competition of networks in the 1980s and early 1990s, which was that Ethernet
    (by which I mean IEEE 802.1, 3, 11) won.<br /><br />Concur that the L3 boundary
    is moving to the first hop Access switch (the ToR) in some leading edge applications.  Note
    that this adds a lot of participants to the Layer 3 routing protocols, leading
    to at least some centralized control (regardless of whether it&#39;s called SDN,
    or a route reflector, or some other name) not unlike the centralized control which
    would have been needed for large Layer 2 installations.<br /><br />There is room,
    particularly in specialized applications like supercomputers, for innovation at
    Layer 2.  I don&#39;t mean arbitrarily framing packet headers differently, I mean
    actually making a big difference for some class of application, or taking a lot
    of cost out of the solution.  A compelling enough design could be mainstreamed,
    but it&#39;s tough to dislodge Ethernet.
  id: '5659684461579830087'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Steve Chalmers
  profile: https://www.blogger.com/profile/03172563417086934763
  pub: '2015-04-24T22:20:13.253+02:00'
  ref: '1031056806778392101'
  type: comment
- comments:
  - date: 01 May 2015 01:17
    html: We need to think about this problem not as network people, but as application
      users.  I have a user somewhere out on the Internet, using an application in
      my data center.  I have to create the illusion that my application never goes
      down, particularly when my application is serving ads and I don&#39;t get paid
      if they don&#39;t show up.<br /><br />This translates to a set of interesting
      -- I almost might say &quot;grand challenge&quot; -- problems for networking.  VM
      migration is useful; keeping a TCP session alive is sometimes useful; being
      able to migrate a (block of) IPs from one data center to another live is useful.  But
      these are tools in a toolbox, not solutions in their own right.<br /><br />The
      &quot;solution&quot; is that my gmail window never hangs; my google maps application
      never hangs; my netflix movie never hangs and has to be restarted; Amazon never
      goes away, or wipes my shopping cart, or hangs in a way that my web browser
      or mobile application needs to be closed and restarted.
    id: '1812245961975033544'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Steve Chalmers
    profile: https://www.blogger.com/profile/03172563417086934763
    pub: '2015-05-01T01:17:07.636+02:00'
    ref: '8087436972036172762'
    type: comment
  - date: 01 May 2015 01:38
    html: Completely agree on the &quot;solution&quot; aspect. I want to understand
      how this translates to IP mobility at the back end. Invariably the application
      that serves the user requests is behind a load balancer and IP mobility might
      not be that big deal for this (North/South traffic). I suspect it is more relevant
      for East-West traffic. Is that true?
    id: '2761767796475731671'
    image: https://resources.blogblog.com/img/blank.gif
    name: Somnath Mani
    profile: null
    pub: '2015-05-01T01:38:57.621+02:00'
    ref: '8087436972036172762'
    type: comment
  - date: 01 May 2015 02:05
    html: 'Remember, I&#39;m an old server and storage guy with only a few years&#39;
      experience in networking: take the network side of this answer with a grain
      of salt.<br /><br />North/South: once the client has resolved the server to
      a single IP address, that IP address needs to keep responding to the client,
      regardless of particular servers / load balancers / routers / electrical transformers
      finding themselves engulfed in flames or otherwise offline.<br /><br />If the
      protocol is stateless (meaning the application was written in the last decade
      rather than in the 1990s) then it doesn&#39;t matter who&#39;s home, just somebody
      has to be.<br /><br />From an East/West perspective, if a collection of services
      which depend on each other is being live migrated, say to new hardware (which
      could be in a different data center) then keeping both IP&#39;s and MAC addresses
      intact will be important.  My guess is Google is able to open new capacity and
      then shut down old capacity, so doesn&#39;t do such migrations.  Again, traditional
      Enterprise is very different: there will be a single instance of an application,
      with resources spent to make it reliable: very difficult to shut down the SAP
      back end to migrate it.<br /><br />It could be that the key east-west case is
      consolidated storage.  Remember a couple of years ago when a network partition
      in a storage chunk of an Amazon data center caused a bunch of storage servers
      to simultaneously think they were the single surviving copy of data, and start
      emergency replication, bringing not just that storage chunk but the AWS instances
      depending on that storage to their knees?  Yes, relative to people who&#39;d
      done big disk arrays that was a novice specification oversight in an exception
      path in a piece of software.  Back to the point: with that as backdrop, 5 years
      from now, with AWS live, migrate all of the contents of that chunk of storage
      to new hardware so the obsolete (not to mention worn-out) disks can be junked.  Regardless
      of whether IP addresses are preserved, the East-West server to storage traffic
      has to stay live during the entire migration.'
    id: '5262324560740949341'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Steve Chalmers
    profile: https://www.blogger.com/profile/03172563417086934763
    pub: '2015-05-01T02:05:45.490+02:00'
    ref: '8087436972036172762'
    type: comment
  date: 01 May 2015 00:29
  html: Is IP mobility that big a deal really? Are there enough use cases for maintaining
    TCP sessions with VM mobility?<br />Even if it were the case, if you look at it,
    you would typically need it for VMs acting as servers (client/server context).
    Can we not solve this with a good service discovery solution or even a load balancer,
    in which case really the load balancer alone needs to know how to get to the service?
  id: '8087436972036172762'
  image: https://resources.blogblog.com/img/blank.gif
  name: Somnath Mani
  profile: null
  pub: '2015-05-01T00:29:04.447+02:00'
  ref: '1031056806778392101'
  type: comment
- comments:
  - date: 01 May 2015 05:19
    html: 'Watch my IPv6 Microsegmentation presentation for an explanation of why
      that&#39;s not a problem: http://blog.ipspace.net/2015/04/video-ipv6-microsegmentation.html<br
      /><br />See also this blog post on host routes and ARP: http://blog.ipspace.net/2014/02/this-is-not-host-route-youre-looking-for.html<br
      /><br />Finally, here&#39;s a follow-up article to this one: http://blog.ipspace.net/2015/04/rearchitecting-l3-only-networks.html<br
      /><br />Hope this helps,<br />Ivan'
    id: '4567327650367332040'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2015-05-01T05:19:59.356+02:00'
    ref: '6618135505655754591'
    type: comment
  date: 01 May 2015 00:31
  html: Besides, while advertising host routes (including /31s for that matter) is
    technically feasible, all the routers would need to know all the routes which
    might not be practical in a large datacenter because of TCAM limitations. One
    of the reasons network virtualization might be a good approach.
  id: '6618135505655754591'
  image: https://resources.blogblog.com/img/blank.gif
  name: Somnath Mani
  profile: null
  pub: '2015-05-01T00:31:00.938+02:00'
  ref: '1031056806778392101'
  type: comment
count: 23
id: '1031056806778392101'
type: post
url: 2015/04/what-is-layer-2-and-why-do-we-need-it.html
