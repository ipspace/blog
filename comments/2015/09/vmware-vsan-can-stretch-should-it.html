<div class="comments post" id="comments">
  <h4>2 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="7622622673584695106">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/04441771667073544925" rel="nofollow">Anders</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7622622673584695106" href="#7622622673584695106">17 September 2015 16:45</a>
              </span>
            </div>
            <div class="comment-content">Note any kind of synchronous replication also suffers from the extra network latency. Having said this, vmware&#39;s VSAN must be designed for a local network only. <br />They do have dedicated products for asynchronous remote replication, and one can probably combine VSAN with them. But please don&#39;t ignore the added latency and physics :-)<br /><br />As a worst-case example: your HDD has an average rotational latency of around 2-3ms - the time until a sector can be read or written. Assuming the sector is written instantly, it will still take 2ms on an average write operation.<br /><br />If you&#39;re doing replication in the metro area with a millisecond roundtrip of overall network latency, this latency will add up for any write requests: your remote HDD probably won&#39;t have its data commited in 2ms, but in 2+1=3ms.<br /><br />Depending on what your application actually does and how often synchronous data is forced onto disk, sync replication in this setup may be functionally decrease the overall hard disk performance by up to 50%.<br /><br />Of course, in real life the various writeback-caches in operating systems, hypervisors, RAID controllers and hard disks lie about having something &quot;really&quot; written onto disk, so those 50% are &quot;worst case&quot; for &quot;every single sector/block is forced to disk&quot;.Even if the write is not forced to disk, the network latency still adds up before the remote system can promise &quot;having it written&quot;. So overall, the network latency adds up to the access time.<br /><br />However, even in a standard OLTP mix (70% read, 30% write), the impact of high-latency writes is obvious: the read performance doesn&#39;t change, the write performance gets noticably worse.<br /><br />If your application doesn&#39;t cope with extra latency on writes and you still do require synchronous writes, you may need to switch from HDD to SSD (reducing the local access time close from 2ms to zero, leaving you with pure network latency).<br /><br />With more remote locations, the problem becomes worse: 3ms is negligible in world of WAN, but if your 2ms hard disk suddenly takes 5ms before some data can be written, it is a considerable decrease.<br />And when your top-notch high performance database&#39;s average write latency suddenly jumps from 0.1ms (SSD) to 3.1ms (remote SSD), someone will probably notice (+3000%).</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4001643800915168085">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09333432630691386455" rel="nofollow">Duncan Epping (VMware)</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4001643800915168085" href="#4001643800915168085">23 September 2015 09:45</a>
              </span>
            </div>
            <div class="comment-content">** Duncan Epping - VMware Employee - Storage and Availability BU **<br /><br />Also replied on the blog ivan wrote, want to post it here as well for those who don&#39;t see that one.<br /><br />Just to point out, Virtual SAN always writes to SSD. That is how the architecture has been designed from the start. We take advantage of the SSD, buffer the writes and then destage them when needed. The write is acknowledged to the Guest OS / Application as soon as it hits the SSD buffer. So the latency for a write to a device like this will not be 2ms but much lower than that.<br /><br />I understand what you are saying, but we are forgetting that we are trying to solve a business problem here and not introduce one. Any stretched storage platform has the same challenge when it comes to latency, yet NetApp Metro, EMC VPLEX, 3Par etc etc are still relatively popular solutions. Why? Well simply because in many cases it is 10x easier to provide this level of resiliency through an infrastructure level solution rather than to rely on 3rd party application providers to change their full architecture to provide you the resiliency you need. As you know getting large vendors to change their application architecture isn&#39;t easy, and can take years... if at all.<br /><br />These types of solutions are developed for relatively short distances, and also relatively low latency. Sure it has been validated to be able to incur a hit of 5ms, that doesn&#39;t mean that from a customer point of view this would (or should) be acceptable. That decision is up to the customer. Same applies to bandwidth, what can your afford, what is available in your region / between sites etc.<br /><br />Stretched infrastructures are not easy to architect, or deploy for that matter, but I truly believe with Virtual SAN we made the storage aspects 10x easier to manage and deploy than they have ever been before.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
