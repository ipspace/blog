{
  "comments": [
    {
      "comments": [
        {
          "date": "22 August 2015 19:38",
          "html": "What&#39;s wrong with print-as-PDF from the browser?",
          "id": "1189712129988546036",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-08-22T19:38:16.484+02:00",
          "ref": "789356772795626237",
          "type": "comment"
        }
      ],
      "date": "21 August 2015 20:44",
      "html": "Hi Ivan, firstly, thanks a lot of very informative articles. I have benefited immensely from you sharing your knowledge. Secondly, a different request. Would it be possible to provide a link for *printable* form of the post? Say, a pdf version or something? ",
      "id": "789356772795626237",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/17518789884565931384",
      "pub": "2015-08-21T20:44:55.689+02:00",
      "ref": "4054788351967144195",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "22 August 2015 10:36",
          "html": "If I understand it right, each host interface is in a separate L2 domain because every connection between a ToR and host interface interface is a different L2 domain (this is the point of L3 only network right?). <br /><br />Therefore ToR 1 only has MAC address of host interface 1 (the one connected to ToR 1) in ARP cache and ToR 2 only MAC address of host interface 2 (the one connected to ToR 2). Inbound traffic that gets to ToR 1 (for that particular host) is always forwarded to host interface 1 and traffic that gets to ToR 2 is always forwarded to host interface 2. Traffic is actually load balanced on spine switches (when ToR are leafs or leaf switches when we have 3-tier clos - spine, leaf, ToR) between both ToRs with ECMP which means that is also load balanced between both host interfaces. <br /><br />Outbound traffic is of course load balanced on the host itself between both interfaces with ECMP.<br /><br />Correct me if I am wrong :) ",
          "id": "8555890019706067136",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Jan",
          "profile": null,
          "pub": "2015-08-22T10:36:14.681+02:00",
          "ref": "4309996186512725503",
          "type": "comment"
        },
        {
          "date": "22 August 2015 19:37",
          "html": "@Jan: You absolutely got it. Thanks for answering this one!",
          "id": "9006491220755735646",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-08-22T19:37:42.011+02:00",
          "ref": "4309996186512725503",
          "type": "comment"
        }
      ],
      "date": "22 August 2015 02:38",
      "html": "If the same IP address is assigned to both interfaces then how does inbound traffic to host work? The ToR will ARP for the host IP address and one of the interface wins? So Inbound traffic is limited to one interface and outbound uses both interfaces?",
      "id": "4309996186512725503",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2015-08-22T02:38:08.134+02:00",
      "ref": "4054788351967144195",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "22 August 2015 19:40",
          "html": "It depends. NSX for vSphere rides on top of vDS, and so you have two options:<br /><br />A) MLAG (or equivalent)<br />B) VXLAN traffic pinned to one uplink<br /><br />So you could build a L3-only network assuming you&#39;re ok with NSX using one of the uplinks at any time.<br /><br />BTW, I covered the vDS uplink options in great details in VMware Deep Dive webinar:<br /><br />http://www.ipspace.net/VSphere_6_Networking_Deep_Dive",
          "id": "197291826397629651",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-08-22T19:40:43.226+02:00",
          "ref": "5653766762270695524",
          "type": "comment"
        }
      ],
      "date": "22 August 2015 13:24",
      "html": "Is it possible to use this in vSphere+NSX scenario? I think it would make sense to have pure L3 fabric with NSX but I am not familiar with all the details of NSX. If I understand correctly you still need L2 within a rack and MLAG on ToRs if you want dual homed hosts because VMware does not allow you to configure unnumbered interfaces with default route through both of them the way you can do it on Linux. Or is there a way to do it?",
      "id": "5653766762270695524",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Jan",
      "profile": null,
      "pub": "2015-08-22T13:24:43.256+02:00",
      "ref": "4054788351967144195",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "25 August 2015 20:46",
          "html": "Hyper-V and KVM (supposedly?) send ARPs. The only thing they have to do is to sniff packets sent from the VM to find its IP address. Obviously that doesn&#39;t work quite so well for some network services VMs.<br /><br />Anyhow, if someone sends a gratuitous ARP when the VM is moved, the adjacent physical switch could create a host route for the moved IP address (assuming that&#39;s what you want to do). With RARP there&#39;s nothing you can do.",
          "id": "1121789959006600349",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-08-25T20:46:06.862+02:00",
          "ref": "4174784584412339940",
          "type": "comment"
        },
        {
          "date": "26 August 2015 04:20",
          "html": "I think it&#39;s naive to believe that a hypervisor can *really know* what&#39;s going on IP-wise inside a VM it&#39;s hosting, and pretending that it can gets dangerous. What if the VM has lots of IP addresses?<br /><br />Is your beef with the RARP that it doesn&#39;t help in the scenario covered in the show, because it doesn&#39;t populate the ARP table?<br /><br />I&#39;m thinking that this whole scheme only works because of a curious Linux optimization: A Linux host (or switch!) answering an ARP query *adds* an ARP entry for the requestor at the same time as answering the query. See here (lines 728-733):<br />http://lxr.free-electrons.com/source/net/ipv4/arp.c<br /><br />When the host speaks, it queries for the gateway, which populates the switch ARP table because of the optimization, and can then be redistributed.<br /><br />If, on the other hand, the host never speaks (say, it&#39;s a server), then nobody can *ever* talk to it, because there&#39;s no route to it anywhere in the L3 domain.",
          "id": "4500915355761284481",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "chris marget",
          "profile": "https://www.blogger.com/profile/06646973209424821070",
          "pub": "2015-08-26T04:20:06.058+02:00",
          "ref": "4174784584412339940",
          "type": "comment"
        }
      ],
      "date": "25 August 2015 15:20",
      "html": "I wonder how they handle failure modes where link stays up, but traffic doesn&#39;t pass in one direction. Without regular protocol keepalives, it seems like a BFD-like feature is appropriate here. Of course, &quot;mode=1&quot; bonding has the same problem, so maybe it&#39;s not a priority.<br /><br />Ivan, what&#39;s your beef with VMware&#39;s &quot;notify&quot; feature using RARP? It doesn&#39;t seem to me that it matters what they put in those post-vMotion MAC table helpers, does it?<br /><br />The hypervisor is not in a position to send a gratuitous ARP, because it doesn&#39;t necessarily know what IPs are in use by the OS. Heck, it&#39;s only *barely* in a position to send the RARP, because the it knows what unicast MACs are loaded in the vNIC interest registers. If the guest OS was something that leverages promiscuous mode (like ESX does), then even the RARP would be impossible!",
      "id": "4174784584412339940",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "chris marget",
      "profile": "https://www.blogger.com/profile/06646973209424821070",
      "pub": "2015-08-25T15:20:37.753+02:00",
      "ref": "4054788351967144195",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "08 September 2015 13:02",
          "html": "Hi Paul,<br /><br />#1 - For whatever weird reason neither server admins nor network admins fancy running a routing protocol between a server and a ToR switch. That was a SOP 20 years ago (every IBM mainframe connected to IP network was running RIP or OSPF in those days), but times have changed.<br /><br />#2 - If I understood the architecture correctly, a userspace process (Quagga, for example) tells the Linux kernel to modify (for example) a forwarding table, and the Cumulus daemon listens to that conversation and implements the same change in hardware (or not). <br /><br />I would prefer a more explicit architecture (which I understand is not available at the moment) where the Cumulus daemon would have a say in the process (like: I can&#39;t do that, please rollback the change).<br /><br />Or maybe I got it all wrong...<br /><br />#3 - you&#39;d have to ask someone that actually knows something about Multicast ;)",
          "id": "3063361207713535460",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2015-09-08T13:02:13.951+02:00",
          "ref": "9074114386481855517",
          "type": "comment"
        }
      ],
      "date": "08 September 2015 09:24",
      "html": "Hi Ivan, Bob, &amp; Dinesh,<br /><br />Firstly, thanks for an awesome episode and for getting right down into the nitty gritty technical detail.  This was the most fun I&#39;ve had with a technical podcast for quite a while. :-)<br /><br />Now to some questions:<br /><br />1. Given that there are host-level modifications required, wouldn&#39;t it be a simpler and more easily-accessible solution to run OSPF/BGP + BFD on the host?  (I&#39;ve pinged Dinesh about this on Twitter, but would be happy to hear from anyone who has the Cumulus BFD implementation working on a server-side distro.)<br /><br />2. Ivan, what was your issue with the hardware offload the use of netlink?  You seemed to imply that it&#39;s a bit hacky, but netlink is just an API which is used to talk between kernel &amp; userspace networking.<br /><br />3. Could you solve the multicast issue by using dual link-local multicast networks alongside the globally-addressible IP on the same physical links?<br /><br />Thanks in advance,<br />Paul<br />",
      "id": "9074114386481855517",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Paul Gear",
      "profile": "https://twitter.com/paulgear1",
      "pub": "2015-09-08T09:24:42.156+02:00",
      "ref": "4054788351967144195",
      "type": "comment"
    }
  ],
  "count": 12,
  "id": "4054788351967144195",
  "type": "post",
  "url": "2015/08/layer-3-only-data-center-networks-with.html"
}