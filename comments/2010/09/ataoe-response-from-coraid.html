<div class="comments post" id="comments">
  <h4>9 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="4749649217449883466">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Etherealmind</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4749649217449883466" href="#4749649217449883466">28 September 2010 18:52</a>
              </span>
            </div>
            <div class="comment-content">The &quot;no security by intention&quot; is a mistake. In an open network, this means that anyone can connect to a storage destination and start using it. Just from an administrative standpoint, allowing every Windows administrator to connect to any disk drive is stupid. Consider connecting to a live LUN hosting your accounting system and then formatting it. <br /><br />This forces a separate physical ethernet network for storage to achieve some security. Not what we want at all!!<br /><br />AoE achieves it&#39;s features by stupid simplicity (UDP- connectionless, no security, dumb MTU handling) not because of it&#39;s inherent greatness.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="483831489421644279">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c483831489421644279" href="#483831489421644279">05 December 2013 19:50</a>
              </span>
            </div>
            <div class="comment-content">AoE is a cheaper, faster alternative to fiber channel.  Just like fiber channel, it should never be used without the same physical security measures sane people use on the SCSI busses inside your servers.<br /><br />It has no security.  Also, it is not a pony, and will not make you taller or get the crabgrass out of your lawn.<br /><br />If you need a storage protocol that you can expose to malicious unauthenticated users on the Internet, you&#39;re looking in the wrong category.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6052802342016615812">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Mouse</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6052802342016615812" href="#6052802342016615812">04 March 2011 17:19</a>
              </span>
            </div>
            <div class="comment-content">This protocol is designed for local network use.  It can be used across WANs, but it is expected that you would also use common sense and setup your firewall/vpn correctly.  The fact that it doesn&#39;t include a bunch of duplicated security code is an advantage.  Why should it duplicate the tools that your OS provides for security and/or routing Ethernet packets?  This is the reason it&#39;s much faster than iSCSI.  The mentality of including 1 million features in every piece of software is the reason for Window&#39;s security problems.  This is an ATA over Ethernet system.  It&#39;s not a VPN or a secure authentication system.  Both of those services are separate pieces of software that their developers can focus on exclusively.  This is the reason for why Unix type systems are so much more secure than Windows.  The Unix world doesn&#39;t re-invent the wheel with every piece of software, nor do we like bloated software that tries to do one million things.  Do one thing and do it very well, the rest will take care of it&#39;s self...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8703916471313072441">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8703916471313072441" href="#8703916471313072441">12 October 2012 15:21</a>
              </span>
            </div>
            <div class="comment-content">Comment 1 re<br />&quot;I have to admit I’ve missed the tag field in the ATAoE packets which does allow parallel requests between a server and a storage array, solving some of the sequencing/fragmentation issues. I’m still not convinced,.....&quot;<br />Why are you still not convinced? Not clear what the reasons are.<br /><br />Comment 2<br />Their customer list just keeps growing. Maybe it does work and work well.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1688616394278747346">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1688616394278747346" href="#1688616394278747346">05 July 2013 09:59</a>
              </span>
            </div>
            <div class="comment-content">One of things that people who have not deployed ATAoE think is that it&#39;s intended to be just plugged into any old network port on any old switch you have lying around the building. That&#39;s far from the typical case - an isolated SAN attached only to provide storage. You don&#39;t have authentication on your SAS extenders, so why have it on ATAoE?<br /><br />We&#39;ve adopted it over iSCSI (initially running in tandem with iSCSI, but after a few  months the benefits were very clearly in favour of ATAoE) for all of our (oil/gas geological) bulk storage uses for 2 years now. You could probably call it Big Data, at over 3000 disks total and 5 Petabytes of data per site. <br /><br />An unplanned side effect of this is that we have now abandoned all the Microsoft storage and use Debian and FreeBSD, giving us a lot less trouble overall, which was quite a surprise here. <br /><br />At the time you wrote your blog post, we were just commissioning the systems, so I read it with great interest and I worried. I have to say that in practice, we haven&#39;t hit any of the worries you raised. The speeds over iSCSI on the same architecture were a big plus and it was far simpler to set up and in use is totally transparent and hasn&#39;t given a single problem down to the protocol choice, which is more than we can say for the iSCSI half of the project, which was disappointing in comparison.<br /><br />Note that we don&#39;t use Coraid, so this is purely a comment on Debian-based ATAoE itself. </div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="6367462156844292005">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6367462156844292005" href="#6367462156844292005">05 July 2013 11:42</a>
              </span>
            </div>
            <div class="comment-content">Great to hear ATAoE works for you (and a dedicated network usually saves the day ;). What are you using as ATAoE storage? JBOD Debian servers?</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5913173387673563628">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5913173387673563628" href="#5913173387673563628">05 July 2013 22:57</a>
              </span>
            </div>
            <div class="comment-content">We normally use each physical disk as an individual LUN and RAID them at the initiator side with  mdadm. This works surprisingly well and is simple, being straight out of the EtherDrive HOWTO. We use RAID6 with several spares per array forming a RAID 6+0 resultant.  So, 1 drive per LUN at the shelf side, combined into RAID6+0 at the initiator side.<br /><br />An alternative approach using the hardware RAID in the server has even shown slightly higher levels of failed hashes over time (every file is hashed and re-checked on a rolling basis when the load is below a certain threshold using idle time). This setup gave identical performance, so it was decided to move to individual drives per LUN for all of the servers while we investigated. The failures stopped, so we retested with hardware RAID. Failures came back, small numbers of random fails, no common cause. We then trialed the JBOD LUN method for 6 months on the same suspect hardware and since the hashing anomalies stopped immediately and never came back. All sites were affected to much the same extent, very low levels of unseen corruption of individual files over time. The sites were commissioned independently using no common staff, desings or vendors/hardware.<br />Very curious indeed. It&#39;s now all 1 drive per LUN permanently at all sites and will stay that way.<br />Since the same hardware was in use all the time this tends to suggest that the hardware raid is actually less reliable than mdadm, while performing identically for ATAoE, probably due to the transfer limits being dictated by the network fabric rather than the controllers themselves. A surprising result indeed.<br /><br />The hardware is a homogeneous mix at all sites as initially we thought we&#39;d need a much more complex layout to get the desired performance on some racks for certain applications. In the end it didn&#39;t matter though - it all was fine though some was strained by iSCSI before we abandoned it for ATAoE.<br /><br />Ease of admin has been excellent throughout and we have learned a lot about convenience along the way. We are now looking into ZFS which we earlier thought was far too complex and slow. It turns out that one of our competitors has been using both ZFS and ATAoE for a similar purpose for 3+ years so we have some catching up to do !<br /><br />The end-cycle audits have shown TRC/TCO of just under half what was allocated and expected so we will never go back to the bad old ways again, though the primary drive was for reliability alone. The simplicity of ATAoE appears to be the key to the success. <br />We are very pleased we were ordered to try it in tandem for a comparison trial. The ATAoE model has just been adopted globally at the end of the 2-year trial with Asia running smoothly for almost 4 months now using an identical approach but just over 19PB so far.<br /><br />The vendor-free approach has been extremely flexible though I can see some major problems for the salesmen due to this - they were very hostile and the scare stories were getting tedious. They have gone strangely quiet now  :-)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1235283970043454776">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://blog.cnidus.net" rel="nofollow">Doug Youd</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1235283970043454776" href="#1235283970043454776">17 July 2013 09:47</a>
              </span>
            </div>
            <div class="comment-content">I&#39;m currently in the process of finalizing the design for a Coraid (AoE) deployment for consumption by a reasonable size VMware ESXi farm.<br /><br />My 2c:<br />In practice, most SAN deployments rely fairly heavily on security of the network segments the protocols run over. <br /><br />NFS is usually run without very much in the way of Auth beyond IP of the requester, All the deployments of iSCSI I&#39;ve seen in practice don&#39;t use CHAP etc.<br /><br />The Coraid implementation of AoE allows for &#39;masking&#39; of resources to a specific initiator. IMO, if the storage fabric is effectively a trusted/protected zone, masking may be &#39;good enough&#39; for a lot of use cases.<br /><br />From what I&#39;m told, it should also be possible to lock what MAC&#39;s are allowed on a given vlan @ the switching layer too, or atleast alert if previously unseen MACs are detected. If that&#39;s achievable, then accidental misconfiguration allowing untrusted initiators on the fabric should be able to be effectively mitigated.<br /><br />So given that, maybe I&#39;m missing something.... what&#39;s the problem? (that isn&#39;t realistically a problem with other converged fabrics in use currently).<br /><br />For example, in an ESXi cluster (based on NFS or iSCSI storage), ANY of the hosts can delete pretty much all data that&#39;s presented to them.... as they are the primary custodians of that data anyway. Is that a risk, certainly.... but its not increased by using AoE LUNs, as far as I can tell.<br /><br />All of that is assuming a fairly typical use case: Virtualization providing the abstraction / segregation between untrusted and trusted zones. Now if we&#39;re talking about presenting the converged fabric out to potentially untrusted initiators.... then yes, more security (and a fatter protocol) is probably required. <br /><br />In this particular design, AoE is not supported by the ESXi cluster natively... we could add in an AoE HBA&#39;s, but in a blade environment, that means retrofitting.. which we wanted to avoid.<br /><br />So we use AoE to present up to ZX headers. These use ZFS to aggregate the storage and it is presented to the ESXi cluster using NFS over a converged 10g fabric.<br /><br />This effectively means the attack surface for AoE is just the ZX headers and the SRX shelves themselves. <br /><br />The big advantage of this architecture is that we can chose to add other presentation headers/gateways in later, utilizing the same AoE back-end disk trays... and not necessarily a Coraid header solution either. <br /><br />Headers can then add on any of the features we want to implement without any rip-replace of the backend disk trays. A simple, lightweight protocol is a big advantage in that model.<br /><br />Proof of the pudding will be in the eating though, granted :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5559168919499893411">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5559168919499893411" href="#5559168919499893411">19 October 2018 05:50</a>
              </span>
            </div>
            <div class="comment-content">I&#39;m thinking about deploying a solution based on AOE.<br />For those which use it, still you happy with this ? is it reliable ?<br />I&#39;m thinking for OpenSource based AOE. Which software are you using ? which OS ?<br /><br />Do you have some advice to build a stable OpenSource AOE infrastructure ?</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
