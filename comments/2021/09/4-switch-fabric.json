{
   "comments": [
      {
         "comments": [
            {
               "date": "23 September 2021 11:11",
               "html": "<p>Simple routed fabric (with EVPN if L2/overlay is needed) would do just fine</p>\n",
               "id": "745",
               "name": "Jeff Tantsura",
               "pub": "2021-09-23T23:11:52",
               "ref": "744",
               "type": "comment"
            },
            {
               "date": "24 September 2021 07:21",
               "html": "I would go for an even simpler design: routed fabric with VXLAN on top using static source replication list instead of EVPN.<br />\n\n\n",
               "id": "747",
               "name": "Ivan Pepelnjak",
               "pub": "2021-09-24T07:21:48",
               "ref": "745",
               "type": "comment"
            },
            {
               "date": "24 September 2021 11:44",
               "html": "<p>IMHO the question is what to do with the worrisome extra links between switches of a pair that turns the topology into a full mesh instead of just collapsing the spines into the leafs.</p>\n\n<p>I think good answers are to remove them, or to not use them for IP forwarding and/or underlay transport, <em>unless required</em> (because of failed links), but be able to make use of those links if necessary (i.e., do not omit them from IP and/or underlay forwarding).</p>\n\n<p>This would remove the doubts about traffic requirements noted in the blog post.</p>\n\n<p>(YMMV depending on make and model of your switches and NOS.)</p>\n",
               "id": "749",
               "name": " Erik Auerswald",
               "pub": "2021-09-24T11:44:13",
               "ref": "747",
               "type": "comment"
            },
            {
               "date": "25 September 2021 08:19",
               "html": "<p>I don&#39;t understand why you find those links worrisome. There is no central switching fabric in this design, and the full mesh is probably the best you can do assuming you have no prior information on traffic flows. The proof is left as an exercise for the reader &#x1F61C;</p>\n\n<p>Also, it&#39;s a small fabric, so it&#39;s worth keeping complexity to a minimum, thus no MLAG.</p>\n",
               "id": "751",
               "name": "Ivan Pepelnjak",
               "pub": "2021-09-25T08:19:37",
               "ref": "749",
               "type": "comment"
            }
         ],
         "date": "23 September 2021 03:44",
         "html": "<p>The traffic would flow over the optimal paths if the two switch pairs were connected via MLAG, and all servers were connected to one switch pair via MLAG. But this would be a boring legacy setup that may allow for interesting, but rare, <a href=\"https://www.unix-ag.uni-kl.de/~auerswal/mlag_split_brain/\">failure scenarios</a>. Additionally, VMware still prefers MAC based redundancy and load sharing instead of using LACP based LAGs. ;-)</p>\n\n<p>In a more fashionable layer 3 setup with VXLAN overlay (or an SPB-M based fabric &agrave; la Extreme), the link between switches of the same pair could be omitted unless an MLAG construct requires it (some vendors support <em>virtual</em> MLAG peer links), or it could be configured with a higher metric value such that it is not used for IP (resp. MAC-in-MAC) forwarding unless the two links to the other switch pair both fail. This again would result in optimal forwarding.</p>\n\n<p>If later there is a need to add more switches, a spine layer could be added &quot;in the middle.&quot;</p>\n\n<p>Thus with care for the details the physical full mesh is either not needed or not detrimental, depending on circumstances (which include the selected switch models and network operating systems, among other things).</p>\n",
         "id": "744",
         "name": " Erik Auerswald",
         "pub": "2021-09-23T15:44:41",
         "type": "comment"
      }
   ],
   "count": 1,
   "type": "post",
   "url": "2021/09/4-switch-fabric.html"
}
