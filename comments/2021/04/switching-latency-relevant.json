{
   "comments": [
      {
         "date": "15 April 2021 10:45",
         "html": "<p>My bet for the elephant in the room is congestion delay. Using a congestion control mechanism that keeps buffer utilization (and thus buffer delay) low, such as Datacenter TCP, is probably much more relevant than having a low forwarding latency in the intermediate devices. For example, an average occupancy of 20 packets per input buffer increases latency by 24 &mu;s when using 10G, way larger than typical zero-load forwarding latency.</p>\n",
         "id": "520",
         "name": " Enrique Vallejo",
         "pub": "2021-04-15T10:45:13",
         "type": "comment"
      },
      {
         "date": "15 April 2021 10:52",
         "html": "<p>Buffering delay? Maybe packet size? Hard to guess the elephant.</p>\n\n<p>Some HFTs are looking at using LASERs in space. Speed of light in a vacuum is faster again :) </p>\n",
         "id": "521",
         "name": " James",
         "pub": "2021-04-15T10:52:22",
         "type": "comment"
      },
      {
         "date": "15 April 2021 12:45",
         "html": "<p>Perhaps elephant flows that delay reception of more latency sensitive mice flows</p>\n",
         "id": "522",
         "name": " Tim",
         "pub": "2021-04-15T12:45:51",
         "type": "comment"
      },
      {
         "date": "15 April 2021 09:15",
         "html": "<p>There is a really good Cisco Live from Lucien Avramov (BRKDCT-2214) about low latency networking with a few numbers and measurements. He also points out that network latency is way lower than middelware and application latency, which is probably where one should start instead of optimising a few nano-seconds on a switch.</p>\n",
         "id": "525",
         "name": " Jean-Baptiste",
         "pub": "2021-04-15T21:15:12",
         "type": "comment"
      },
      {
         "date": "15 April 2021 09:20",
         "html": "<p>I think the elephant is &quot;but how fast can you actually process the packets when you receive them?&quot; &#x1F642;</p>\n",
         "id": "526",
         "name": "Sander Steffann ",
         "pub": "2021-04-15T21:20:28",
         "type": "comment"
      },
      {
         "date": "15 April 2021 10:57",
         "html": "<p>Excellent article (you always find interesting topics).\nMy bet for the elephant will be QoS to lower the latency even further.</p>\n",
         "id": "527",
         "name": "Daniel Larsson",
         "pub": "2021-04-15T22:57:42",
         "type": "comment"
      },
      {
         "date": "16 April 2021 11:59",
         "html": "Great topic Ivan : ))! I found Cisco apparently manages to scale port-to-port latency down to 250ns for L3 switching, which is astonishing, and way less (sub 100ns) for L1 and L2, here (page 7):<br />\n\n<p>https://www.cisco.com/c/dam/en/us/products/collateral/switches/nexus-3550-series/esg-white-paper-ultralowlatency.pdf</p>\n\n<p>I don&#39;t know where FPGA fits into this ultra low-latency picture, because FPGA, compard to ASIC, is bigger, and a few times slower, due to the use of Lookup Table in place of gate arrays, and programmable interconnects. In any case, looking at their L2 and L1 numbers, it&#39;s too obvious the measurement was taken in zero-buffer and non-contentious situations. In the real world, with realistic heavily bursty, correlated traffic, they all perform way less than their ideal case. But regardless, L3 switching at 250ns even under ideal condition is highly impressive, given Trident couldn&#39;t achieve it in any of their testing scenarios. </p>\n\n<p>Again, I&#39;m not bashing Broadcom. It&#39;s just I find it laughable reading their apologies in their report you linked to, wrt how they don&#39;t &quot;optimize&quot; for 64-byte packets (love their wording), and how they manage to find a way to make their competitor finish far behind in the tests. Granted, Mellanox was trying the same thing in their test against Broadcom, so they&#39;re all even, and we should only take these so-called vendor-released testing reports with a grain of salt.</p>\n\n<p>The elephant in the room that you alluded to, is most likely endpoint latency. It&#39;s pretty irrelevant to talk about ns middlebox-latency when the endpoints operate in the ms range :p . And endpoint latency gets even worse when features like interrupt coalescing and LSO/GRO are in place. Must be part of the reason why the Cloud&#39;s performance for scientific apps sucks, and funny enough, they actually admit it as I found out recently. </p>\n\n<p>But IMO, that only means the server operating system, the hypervisor, the software switch etc, are the ones that need innovation and up their game, instead of using their pathetic latency figures as an excuse not to keep bettering routers&#39; and switches&#39; performance. Overlay model is notoriously slow because it&#39;s layer on top of layer (think BGP convergence vs IGP convergence), and as mentioned in your previous post, Fail fast is failing fast: &quot;If you live in a small town with a walking commute home, you can predictably walk in the front door at 6:23 every evening. As you move to the big city with elevators, parking garages, and freeways to traverse, it&#39;s harder to time your arrival precisely,&quot;  that kind of overburderned, complex architecture is not deterministic and no good for applications with real-time requirements. Infiniband shied away from TCP/IP for the same reason, and used a minimal-stack model to keep their latency low.</p>\n\n<p>The Cloud and their overlay model is a definitely a step backward in terms of progress. By doing it cheap, they make it slow. Good for their greedy shareholders, sucks for consumers who truly need the numbers. Well, I guess I can stop complaining now that bare-metal instances are a thing. But yeah, taken as a whole, basically echnology winter sems to continue. These days about the only kind of progress we have is corporate-PR progress. </p>\n\n<p>Speaking of HFT, there seems to be a lot of fanfare going on there when it was big some 10 yrs ago. FPGA was often mentioned as the way they sped up their end-to-end latency. But I ran across comments of some of the guys who actually did HFT for a living sometime back, and they said it&#39;s all hot-air, with most of the stuff they try to optimize being on the software level, such as doing away with message queuing (and so, safely getting rid of locks) to unburden their programs of concurrency synchronization, which is a big latency killers. Staying away from language that performs garbage collection is another thing, as there&#39;s no one-size-fit-all garbage collection algorithm that&#39;s optimized for all use case, and regardless, it&#39;s an additional layer compared to explicit memory management, and more layer means slower.</p>\n\n<p>From what I know of RenTech, one of the biggest if not the biggest HFT (they also do other algorithmic trading besides HFT), they rely on software with big-data models, not fancy hardware. </p>\n",
         "id": "529",
         "name": "Minh Ha",
         "pub": "2021-04-16T11:59:19",
         "type": "comment"
      },
      {
         "date": "17 April 2021 09:08",
         "html": "<p>Yet another true gem from Ivan - Thanks ever so much   </p>\n\n<p>I guess the Elephant is TCP and the correct router buffer sizing. Too small and you get drops and thus retransmission and thus delay, too big and you get the bufferbloat phenomenon and thus delay again. Having said that, you need to buffer if you are not fast enough at switching and/or if your chipset design is crap (e.g. use of external memory for lookups) and thus perhaps this might be an elephant too ?    </p>\n\n<p>Just one last thing on buffer sizing in a SP environment. I also hope to engage Minh Ha on this as I know he&#39;s passionate about this subject too (he once wrote about selfsimilarity of internet traffic) but maybe on another post of Ivan - Ivan permitting of course ;)!!! <br />\nI found this paper &quot;https://www.semanticscholar.org/paper/Internet-Traffic-is-not-Self-Similar-at-Timescales-Telkamp-Maghbouleh/6ca16fcd9959eb1bca89a52be63bf5cfbb3fcc00&quot; cited at CiscoLive and also associated to multicast and video transport that is my major area of interest as we speak. The CiscoLive PPT is this one @ https://www.ciscolive.com/c/dam/r/ciscolive/emea/docs/2015/pdf/BRKSPV-1919.pdf <br />\nBasically the Telkamp&#39;s paper is a theoretical paper and states that Internet Traffic is not SelfSimilar @ timescales relevant to QoS. This means that it is instead markovian and thus not bursty at timescale that really counts. <br />\nI also found though a pretty recent empirical paper from AT&amp;T Labs that managed (not an easy task at all) to look into real Internet backbone traffic @ very small scale (relevant to router QoS and thus @ ms grain) that observed that real internet traffic is instead indeed self-similar and thus bursty even at ms level. <br />\nThe AT&amp;T lab paper is @ http://buffer-workshop.stanford.edu/papers/paper18.pdf       </p>\n\n<p>Would love to know what your take is on this pretty complex but tremendously important subject. Hope I haven&#39;t derailed too much from the main topic as DC traffic is totally different from SP traffic...unless you do ..... Telco Cloud  :) ?  In that case I hope it can be treated separately in a different post maybe - Ivan permitting of course !!    </p>\n\n<p>Cheers/Ciao <br />\nAndrea    </p>\n",
         "id": "533",
         "name": "andrea di donato",
         "pub": "2021-04-17T09:08:23",
         "type": "comment"
      }
   ],
   "count": 8,
   "type": "post",
   "url": "2021/04/switching-latency-relevant.html"
}
