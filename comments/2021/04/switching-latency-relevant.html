<div class="comments post" id="comments">
  <h4>9 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="520">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Enrique Vallejo</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c520" href="#520">15 April 2021 10:45</a>
              </span>
            </div>
            <div class="comment-content"><p>My bet for the elephant in the room is congestion delay. Using a congestion control mechanism that keeps buffer utilization (and thus buffer delay) low, such as Datacenter TCP, is probably much more relevant than having a low forwarding latency in the intermediate devices. For example, an average occupancy of 20 packets per input buffer increases latency by 24 &mu;s when using 10G, way larger than typical zero-load forwarding latency.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="521">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> James</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c521" href="#521">15 April 2021 10:52</a>
              </span>
            </div>
            <div class="comment-content"><p>Buffering delay? Maybe packet size? Hard to guess the elephant.</p>

<p>Some HFTs are looking at using LASERs in space. Speed of light in a vacuum is faster again :) </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="522">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Tim</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c522" href="#522">15 April 2021 12:45</a>
              </span>
            </div>
            <div class="comment-content"><p>Perhaps elephant flows that delay reception of more latency sensitive mice flows</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="525">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Jean-Baptiste</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c525" href="#525">15 April 2021 09:15</a>
              </span>
            </div>
            <div class="comment-content"><p>There is a really good Cisco Live from Lucien Avramov (BRKDCT-2214) about low latency networking with a few numbers and measurements. He also points out that network latency is way lower than middelware and application latency, which is probably where one should start instead of optimising a few nano-seconds on a switch.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="526">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Sander Steffann </a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c526" href="#526">15 April 2021 09:20</a>
              </span>
            </div>
            <div class="comment-content"><p>I think the elephant is &quot;but how fast can you actually process the packets when you receive them?&quot; &#x1F642;</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="527">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Daniel Larsson</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c527" href="#527">15 April 2021 10:57</a>
              </span>
            </div>
            <div class="comment-content"><p>Excellent article (you always find interesting topics).
My bet for the elephant will be QoS to lower the latency even further.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="529">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c529" href="#529">16 April 2021 11:59</a>
              </span>
            </div>
            <div class="comment-content">Great topic Ivan : ))! I found Cisco apparently manages to scale port-to-port latency down to 250ns for L3 switching, which is astonishing, and way less (sub 100ns) for L1 and L2, here (page 7):<br />

<p>https://www.cisco.com/c/dam/en/us/products/collateral/switches/nexus-3550-series/esg-white-paper-ultralowlatency.pdf</p>

<p>I don&#39;t know where FPGA fits into this ultra low-latency picture, because FPGA, compard to ASIC, is bigger, and a few times slower, due to the use of Lookup Table in place of gate arrays, and programmable interconnects. In any case, looking at their L2 and L1 numbers, it&#39;s too obvious the measurement was taken in zero-buffer and non-contentious situations. In the real world, with realistic heavily bursty, correlated traffic, they all perform way less than their ideal case. But regardless, L3 switching at 250ns even under ideal condition is highly impressive, given Trident couldn&#39;t achieve it in any of their testing scenarios. </p>

<p>Again, I&#39;m not bashing Broadcom. It&#39;s just I find it laughable reading their apologies in their report you linked to, wrt how they don&#39;t &quot;optimize&quot; for 64-byte packets (love their wording), and how they manage to find a way to make their competitor finish far behind in the tests. Granted, Mellanox was trying the same thing in their test against Broadcom, so they&#39;re all even, and we should only take these so-called vendor-released testing reports with a grain of salt.</p>

<p>The elephant in the room that you alluded to, is most likely endpoint latency. It&#39;s pretty irrelevant to talk about ns middlebox-latency when the endpoints operate in the ms range :p . And endpoint latency gets even worse when features like interrupt coalescing and LSO/GRO are in place. Must be part of the reason why the Cloud&#39;s performance for scientific apps sucks, and funny enough, they actually admit it as I found out recently. </p>

<p>But IMO, that only means the server operating system, the hypervisor, the software switch etc, are the ones that need innovation and up their game, instead of using their pathetic latency figures as an excuse not to keep bettering routers&#39; and switches&#39; performance. Overlay model is notoriously slow because it&#39;s layer on top of layer (think BGP convergence vs IGP convergence), and as mentioned in your previous post, Fail fast is failing fast: &quot;If you live in a small town with a walking commute home, you can predictably walk in the front door at 6:23 every evening. As you move to the big city with elevators, parking garages, and freeways to traverse, it&#39;s harder to time your arrival precisely,&quot;  that kind of overburderned, complex architecture is not deterministic and no good for applications with real-time requirements. Infiniband shied away from TCP/IP for the same reason, and used a minimal-stack model to keep their latency low.</p>

<p>The Cloud and their overlay model is a definitely a step backward in terms of progress. By doing it cheap, they make it slow. Good for their greedy shareholders, sucks for consumers who truly need the numbers. Well, I guess I can stop complaining now that bare-metal instances are a thing. But yeah, taken as a whole, basically echnology winter sems to continue. These days about the only kind of progress we have is corporate-PR progress. </p>

<p>Speaking of HFT, there seems to be a lot of fanfare going on there when it was big some 10 yrs ago. FPGA was often mentioned as the way they sped up their end-to-end latency. But I ran across comments of some of the guys who actually did HFT for a living sometime back, and they said it&#39;s all hot-air, with most of the stuff they try to optimize being on the software level, such as doing away with message queuing (and so, safely getting rid of locks) to unburden their programs of concurrency synchronization, which is a big latency killers. Staying away from language that performs garbage collection is another thing, as there&#39;s no one-size-fit-all garbage collection algorithm that&#39;s optimized for all use case, and regardless, it&#39;s an additional layer compared to explicit memory management, and more layer means slower.</p>

<p>From what I know of RenTech, one of the biggest if not the biggest HFT (they also do other algorithmic trading besides HFT), they rely on software with big-data models, not fancy hardware. </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="533">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">andrea di donato</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c533" href="#533">17 April 2021 09:08</a>
              </span>
            </div>
            <div class="comment-content"><p>Yet another true gem from Ivan - Thanks ever so much   </p>

<p>I guess the Elephant is TCP and the correct router buffer sizing. Too small and you get drops and thus retransmission and thus delay, too big and you get the bufferbloat phenomenon and thus delay again. Having said that, you need to buffer if you are not fast enough at switching and/or if your chipset design is crap (e.g. use of external memory for lookups) and thus perhaps this might be an elephant too ?    </p>

<p>Just one last thing on buffer sizing in a SP environment. I also hope to engage Minh Ha on this as I know he&#39;s passionate about this subject too (he once wrote about selfsimilarity of internet traffic) but maybe on another post of Ivan - Ivan permitting of course ;)!!! <br />
I found this paper &quot;https://www.semanticscholar.org/paper/Internet-Traffic-is-not-Self-Similar-at-Timescales-Telkamp-Maghbouleh/6ca16fcd9959eb1bca89a52be63bf5cfbb3fcc00&quot; cited at CiscoLive and also associated to multicast and video transport that is my major area of interest as we speak. The CiscoLive PPT is this one @ https://www.ciscolive.com/c/dam/r/ciscolive/emea/docs/2015/pdf/BRKSPV-1919.pdf <br />
Basically the Telkamp&#39;s paper is a theoretical paper and states that Internet Traffic is not SelfSimilar @ timescales relevant to QoS. This means that it is instead markovian and thus not bursty at timescale that really counts. <br />
I also found though a pretty recent empirical paper from AT&amp;T Labs that managed (not an easy task at all) to look into real Internet backbone traffic @ very small scale (relevant to router QoS and thus @ ms grain) that observed that real internet traffic is instead indeed self-similar and thus bursty even at ms level. <br />
The AT&amp;T lab paper is @ http://buffer-workshop.stanford.edu/papers/paper18.pdf       </p>

<p>Would love to know what your take is on this pretty complex but tremendously important subject. Hope I haven&#39;t derailed too much from the main topic as DC traffic is totally different from SP traffic...unless you do ..... Telco Cloud  :) ?  In that case I hope it can be treated separately in a different post maybe - Ivan permitting of course !!    </p>

<p>Cheers/Ciao <br />
Andrea    </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="535">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c535" href="#535">19 April 2021 10:33</a>
              </span>
            </div>
            <div class="comment-content"><p>Hi Andrea,</p>

<p>I cannot get access to the paper as only its abstract is available. Nevermind :)). Self-similarity/long-range dependency of traffic arrival, like almost everything else in the grand scheme of Internetworking, should be treated with a big-picture perspective. So basically if someone tries hard enough, they can prove that at a certain timescale, the traffic is not self-similar, or not that self-similar, you know, with some mathematical twists and &quot;proved theorems&quot; ;). Not that it changes the true nature of things, i.e. Internet traffic is still bursty on many timescales, as proven by microburst for the fine-grain level, and so, due to this many-timescale burstiness, using big buffer as a form of low-pass filter to smooth out traffic, doesn&#39;t work most of the time. It can increase throughput, but may not goodput (due to loss and retransmission), and at the cost of considerable latency. Not good.</p>

<p>Also, a lot of switch-scheduling algorithms work well with uniform traffic, but their performance starts to degrade when bursty models like self-similarity is used. Add to that multicast traffic when enabled, and optimum switch-fabric scheduling is still an open problem. That&#39;s why building a high-end router is a lot more than just writing some network OS and sticking it on top of off-the-shelf hardware, hoping for the best. And that&#39;s why I commented above, that the Cloud is a step-back in terms of progress, with their commodity-based network. Not that it matters, we&#39;re in the midst of a technology winter anyway, with scams like Quantum Computing and AI being hailed left and right as the saviours of the day ;).</p>

<p>So in a word, instead of trying to dispute the fact that Internet traffic is bursty on many levels, and use that as an excuse to justify their under-performing equipment, researchers and vendors would do better to face facts, and work hard to create products that work best for worst-case scenarios, as that&#39;s how routers were originally bench-marked anyway. Good, high-performance products never go out of style.</p>

<p>Back to our topic, for DC environment, when the RTT is in the low us, point-solutions like DC-TCP along with intelligent load balancing (flowlet, or even better, packet-level ECMP) can be used to both improve delay and throughput, plus negate the necessity of big buffer. Solutions like this are not viable in the Internet due to bigger delay and jitter, causing reordering and serious performance degradation.</p>

<p>For Telco/carrier Cloud environments, where NFVs (which are much slower than hardware SGW) get used a lot, latency is higher with a lot of jitter due to the nature of software and the varying link speeds, so DC-level near-zero buffer is not applicable. But all the same, very big buffer causes a lot of trouble there too. You might like this paper that discusses this particular issue:</p>

<p>http://lig-membres.imag.fr/heusse/WirelessNet/Papers/2012Jiang.pdf</p>

<p>A few years ago, Codel was proposed as the solution for bufferbloat in Internet/SP environment, and was highly touted. But with all of its fanfare, it&#39;s not without its weaknesses (like everything else) and its superiority against good old RED was challenged here:</p>

<p>https://core.ac.uk/download/pdf/33663637.pdf</p>

<p>In the end, I think there&#39;s no substitute for a good hardware and an over-provisioned network if you want to guarantee SLA, esp. for rainy days. If you have to live with a mix of legacy equipment whose performance lacks the rest of the fleet, you can use some tricks like bigger buffer to work around that, but it&#39;s a band-aid, not a solution.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
