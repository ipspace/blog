{
   "comments": [
      {
         "date": "27 May 2021 10:49",
         "html": "<p>Hi Ivan,\nas this is one of my fav topis, I couldn&#39;t <em>not</em> leave a comment.\nAFAIK, what networking vendors use is normally called &quot;resilient hashing&quot;. I&#39;ve no PhD in this area but it looks like consistent hashing is much more advanced and is still an active area of research (see https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8). My very simplified understanding is that resilient hashing is a similar idea but due to a small number of buckets and limited TCAM space cannot satisfy all of the properties of a consistent hashing algorithm. </p>\n\n<p>&gt; Is there an easy-to-deploy software solution out there that would allow you to build large-scale anycast TCP services\nIf we assume that &quot;large-scale&quot; and &quot;easy&quot; are not mutually exclusive, I believe there is. Due to how networking is implemented inside Kubernetes, you always get two layers of load-balancers (similar to cloud-scalers) with the first layer doing some form of consistent hashing and second layer doing the standard L7 LB (ingress). One prime example is Cilium https://cilium.io/blog/2020/11/10/cilium-19#maglev and although I don&#39;t have the exact numbers, I believe they test their releases against some pretty large number of nodes.</p>\n",
         "id": "595",
         "name": " Michael Kashin",
         "pub": "2021-05-27T10:49:46",
         "type": "comment"
      },
      {
         "date": "27 May 2021 03:20",
         "html": "<p>To add to my first comment, I have once tried to implement maglev-style load-balancing on an Arista switch. I used ECMP to spread the flows over multiple buckets and openflow agent to catch the first packet and install openflow (directflow) entries with timeouts (implementing the connection tracking part of maglev). This worked perfectly fine, I was able to add and remove anycast destinations without disrupting the existing traffic flows and it even survived the leaf switch outage (the second leaf in a pair would hash the flows in the same way). </p>\n\n<p>My dreams got shattered when I realised that the maximum number of directflow entries supported was limited to 1-2k and, since I was installing exact match entries, this would not usable in real life.</p>\n",
         "id": "596",
         "name": " Michael Kashin",
         "pub": "2021-05-27T15:20:01",
         "type": "comment"
      },
      {
         "date": "27 May 2021 04:03",
         "html": "<p>Similar to Michael&#39;s comment and re: the Cilium bits, we also drop an L4 IPVS layer in front of our k8s services that need access from outside a cluster. \nhttps://github.com/Demonware/balanced</p>\n\n<p>A BGP speaker on the IPVS nodes anycast the VIP to the network. IPVS has a consistent hashing algo (HRW / rendezvous hash). That way you don&#39;t depend on the network fabric to do any type of resilient hashing; you have a thin L4 software LB handle you resilience. This is done without coordination between the IPVS nodes (no flow state sync).</p>\n\n<p>With a consistent hash like that, you can still be susceptible to a partial rehash on backend add/drop, e.g. if you have 9 existing backends and add a tenth, then 1/10 of the connections from the existing backends will now hash to the new backend. However, IPVS does carry a tiny little bit of state in each node to also effectively implement resilient hashing to tolerate backend change. If you have a backend add/drop, if your flow is still traversing the same IPVS node then it will continue to forward your traffic to the same backend.</p>\n\n<p>If you hit a backend scale up/down or failure, <em>and</em> you have an ECMP rehash to land on a different IPVS node, and you&#39;ve maintained a long-lived TCP connection through both of those events, then you <em>might</em> have impact to your connection if you are one of the subset of flows that got rehashed on the backend change (e.g. that 1/10 bit noted above).</p>\n\n<p>In practice, meeting all of those conditions is rare enough, and the efforts needed to mitigate for that as well (e.g. adding another indirection layer to punt to the previous host like Fastly implemented), that for our use case this type of setup is sufficient. </p>\n",
         "id": "597",
         "name": " Hugo Slabbert",
         "pub": "2021-05-27T16:03:14",
         "type": "comment"
      }
   ],
   "count": 3,
   "type": "post",
   "url": "2021/05/tcp-anycast-hard.html"
}
