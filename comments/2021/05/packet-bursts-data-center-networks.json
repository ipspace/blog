{
   "comments": [
      {
         "date": "18 May 2021 10:51",
         "html": "<p>Note that 200 ms is the default value for the RTOmin parameter, this is, the minimum value that the Retransmission TimeOut can be assigned dynamically, based on the estimation of RTTs. Decreasing this value (instead of the initial RTO) has been reported to hugely improve TCP goodput in presence of incast - because it reduces the dramatical impact of packet losses and retransmissions. See:</p>\n\n<p>https://www.cs.cmu.edu/~dga/papers/incast-sigcomm2009.pdf</p>\n\n<p>This study explores values for RTOmin of 1 ms and lower. Note that, in practice, you can be limited by the kernel clock resolution -- we have found some systems in the past where it prevented you from setting any value lower than 5 ms. In any case, the benefit from the default RTOmin = 200 ms is huge. </p>\n",
         "id": "577",
         "name": "Enrique Vallejo",
         "pub": "2021-05-18T10:51:38",
         "type": "comment"
      },
      {
         "date": "18 May 2021 02:42",
         "html": "<p>Datacenter links can normally support those server bursts, but at the end there is a traffic server to client that cannot support those bursts. Clients are 1G, servers are 10G/25G. Most of access switches can not suport all those servers bursts neither. Is still better drop than use pause frames on access switches? </p>\n",
         "id": "578",
         "name": " Xavier",
         "pub": "2021-05-18T14:42:02",
         "type": "comment"
      },
      {
         "date": "18 May 2021 04:34",
         "html": "<p>@Enrique: Thanks a million! I love how much I learn from the comments ;)) Will fix the text and add a link.</p>\n\n<p>@Xavier: As most traffic toward the client is client-generated in the first place, you wouldn&#39;t expect to see a massive incast problem, and as the TCP bursts gradually increase in size as the peers are trying to figure out what works, eventually the network-to-client link will get into packet drop territory and the inbound TCP sessions will reach an equilibrium.</p>\n\n<p>I would strongly suggest you figure out (A) how much buffering there is on your access switches and (B) what&#39;s the percentage of outbound packet drops caused by output queue overflow. You might not have that problem at all.</p>\n",
         "id": "579",
         "name": "Ivan Pepelnjak",
         "pub": "2021-05-18T16:34:08",
         "type": "comment"
      },
      {
         "date": "24 May 2021 08:32",
         "html": "<p>Hi Ivan, spot on about deep buffering not required and a bit of drop (not too much) is fine in DC environments, where RTTs tend to be in us rather than ms range. Also, one implicit assumption on a lot of these discussions, is what we describe generally applies to busy to very busy DCs. For low-utilization DCs, it doesn&#39;t matter the design, just throw in hardware in a basic topology and often times it works acceptably :)).</p>\n\n<p>That said, congestion in busy DCs doesn&#39;t require saturation of every single port of a switch, core or leaf. Take the incast scenario -- the same effect can happen to a hang server. Say when A,B,C and a few others on L1 send to X simultaneously and X fails to respond due to whatever reason, leading to output queue saturation on X port of L2 switch. That will cause all other traffic destined to X (say X is hosting several busy VMs) to be dropped as well, causing big delay for other traffic. And the buffer saturation will now move back toward that of S1-L2 port, still on L2, causing VOQ saturation on that input port. Depending on how the VOQs are built, if they&#39;re quota-restricted, they won&#39;t cause buffer fill-up on that port, but if the packet buffer is shared among all VOQs with not quota enforced on each one, then buffer hogging by the saturating VOQ will occur, leading to more packet loss for all sources sending to that uplink as well. So basically all traffic sent to L2 from S1 will suffer, while S1 can be under-utilized. </p>\n\n<p>Also, another assumption we tend to make in high-level discussion is fabric scheduler&#39;s efficiency. But schedulers are not perfect, far from it. These schedulers, the majority of them some variants of iSLIP/PIM, tend to work well with uniform and uncorrelated traffic. When the traffic is bursty and correlated, plus there&#39;s congestion going on at some port, the scheduler&#39;s performance can drop pretty quickly. Also these schedulers are generally designed with the assumption of symmetry in mind. When switches have uplink ports whose link bandwidth are bigger than other ports, port scheduling can get inefficient. So in reality, all of these factors can cause congestion and increase delay greatly even when the core switches are under-utilized. </p>\n\n<p>Vendors also tend to post best-case performance numbers, so when they post things like 5 billions of packets in total for product xyz, we should expect the thing to perform worse in practice. Their method of calculation can be questionable as well -- they might test only a subset of ports, then multiply the result and call that total switching capacity, when in fact, due to scheduler&#39;s performance variation between low and high offered loads, plus all other reasons mentioned above, realistic performance doesn&#39;t even come close to that. </p>\n\n<p>Section 3.2 in this paper documents what happens when the authors attempt to create several traffic patterns on a leaf-spine topology. It&#39;s not a big setup, yet it can get ugly fast:</p>\n\n<p>https://arxiv.org/abs/1604.07621</p>\n\n<p>In big DCs with much bigger fan-in traffic, this situation can get worse. High fan-in can also cause work-conservation problem in switches as well. Say L1 in your diagram. When lots of ports all send to 1 or 2 uplinks at the same time, oversubscribing the uplinks, the scheduler performance can bog down because it&#39;s too hard to do efficient scheduling for that kind of traffic pattern, resulting in the uplinks not receiving as many packets as they can send out, aka they&#39;re not work-conserving. All of this can happen while L1&#39;s total load is nowhere near 100%. </p>\n\n<p>Work-conservation is a fundamental issue in switch design, and AFAIK, there&#39;s no scheduler to date that can deal with this optimally, and that for unicast traffic alone. Mixing in multicast traffic, and things can get messy pretty quickly. That&#39;s why I don&#39;t put much stock in the beautiful performance figures vendors put out. That said, we don&#39;t generally have these kinds of problems in the average enterprise DC because switches are pretty underused there :)). </p>\n",
         "id": "583",
         "name": "Minh Ha",
         "pub": "2021-05-24T08:32:53",
         "type": "comment"
      },
      {
         "date": "24 May 2021 12:36",
         "html": "<p>Thanks a million for another batch of &quot;further reading&quot;. Just two minor details:</p>\n\n<p>&gt; Say when A,B,C and a few others on L1 send to X simultaneously and X fails to respond due to whatever reason, leading to output queue saturation on X port of L2 switch.</p>\n\n<p>I hope we are not talking about lossless traffic here. Under the usual <em>IP is unreliable, use TCP</em> assumptions, if X does not respond, the traffic sent to it is lost (or at least unacknowledged), and the congestion on that port should be reduced (apart from occasional retransmissions).</p>\n\n<p>&gt; That will cause all other traffic destined to X (say X is hosting several busy VMs) to be dropped as well, causing big delay for other traffic.</p>\n\n<p>And that is solved by rate-limiting or shaping the traffic toward a VM within the server, making sure congestion detection/avoidance kicks in well before the physical uplink is saturated. Yeah, I&#39;m sure there are people out there <em>not</em> doing that, but that&#39;s a totally different story &#x1F937;&zwj;&#x2642;&#xFE0F;</p>\n",
         "id": "584",
         "name": "Ivan Pepelnjak",
         "pub": "2021-05-24T12:36:51",
         "type": "comment"
      },
      {
         "date": "25 May 2021 08:05",
         "html": "<p>Hi Ivan, no, I was not talking about lossless fabric, just lossy ones; I intentionally left that one out because with lossless fabric, situations like this get much worse due to congestion spreading and HOL blocking :)) . Might even cause deadlock due to circular buffer dependency.</p>\n\n<p>You&#39;re right the traffic sent to X is lost and congestion should eventually reduce due to TCP congestion control and exponential backoff, but TCP RTOMin is in the ms range even when tuned, while the port buffer can fill up in us range, due to both high bandwidth and high fan-in, so quite a bit of loss can result and back up in the meantime. In that situation, traffic shaping doesn&#39;t help as cumulative shaped bandwidth of high fan-in can still overrun OQ buffer at X in us timeframe. </p>\n\n<p>Unfortunately, looks like data about DC congestion incidents are kept hush-hush -- if you know of any, pls let me know -- so we can&#39;t look deeper into specific cases and examine the dynamics. That would be a great way to improve our knowledge and experience, as one lifetime is too short to be able to witness all situations first hand :) . </p>\n",
         "id": "588",
         "name": "Minh Ha",
         "pub": "2021-05-25T08:05:31",
         "type": "comment"
      }
   ],
   "count": 6,
   "type": "post",
   "url": "2021/05/packet-bursts-data-center-networks.html"
}
