<div class="comments post" id="comments">
  <h4>1 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="670">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Erik Auerswald</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c670" href="#670">17 June 2021 09:48</a>
              </span>
            </div>
            <div class="comment-content">On the question of possible flooding problems:<br />

<p>It&#39;s hard to get good information on the practical problems with link-state routing in &quot;large scale data centers&quot; (whatever you may consider &quot;large&quot;&hellip;). The best I have found so far (linked to from this blog IIRC) is &quot;What I&#39;ve learned about scaling OSPF in Datacenters&quot; from Justin Pietsch (https://elegantnetwork.github.io/posts/What-Ive-learned-about-OSPF/).</p>

<p>There is an interesting bit regarding flooding: &quot;Which means you have to be very careful about what is flooded, what are the areas, and how things are summarized. I didn&rsquo;t understand this until I had a simulation and I could try things out with areas or without, and the effect was dramatic.&quot;</p>

<p>In I think this March Dinesh Dutt did a Webinar on ipSpace called &quot;Using OSPF in Leaf-and-Spine Fabrics&quot; where he added some details on how to make areas and summarization work in this setting (there are subtle, but important differences in the results of this and the RFC 7938 BGP setup).</p>

<p>Another bit of information regarding the alleged flooding problems can be gleaned from Google paper &quot;Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google&rsquo;s Datacenter Network&quot;. Section 5.2.2 describes how they prevent(ed?) flooding problems (back then?). One could use IS-IS Mesh Groups as described in the informational RFC 2973 and implemented by many vendors to build something similar.</p>

<p>The expired Openfabric draft from Russ White attempted to reduce flooding. That makes one wonder why Russ white did not mention this point in his &quot;Rethinking BGP in the Data Center&quot; presentation.</p>

<p>Anyway, most &quot;Enterprise&quot; data centers should be of sufficiently small size to not show any problems with a basic deployment of OSPF or IS-IS in the underlay. I&#39;d like to point at tables 2 and 3 of RFC 2329 from 1998 regarding OSPF scalability information over two decades ago. But please observe that this report does not mention the number of full adjacencies per router, which is the stated problem in Clos style networks.</p>

<p>I&#39;d say the classic approach of an IGP (e.g., OSPF or IS-IS) in the underlay and MP-BGP in the overlay is a simple and proven approach. I think it is easier to understand and work with than using BGP twice, one set of sessions as underlay IGP and another for the overlay. Using BGP once for a combination of underlay and overlay (the FRR way as pushed by Cumulus^WNvidia) is fine, too, if supported by your vendor.</p>

<p>In another recent webinar (&quot;Multi-Vendor EVPN Deployments&quot; from May) on ipSpace, Dinesh Dutt listed OSPF+BGP as the most commonly supported basis for EVPN on data center switches.</p>

<p>Thanks,
Erik</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
