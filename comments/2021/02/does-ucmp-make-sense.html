<div class="comments post" id="comments">
  <h4>5 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="421">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Pete Lumbis</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c421" href="#421">25 February 2021 04:32</a>
              </span>
            </div>
            <div class="comment-content"><p>One valid use case for UCMP is anycast services. If I have 4 anycast nodes behind leaf1 and 1 behind leaf2 I want the spines to UCMP that traffic to the leafs. </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="422">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c422" href="#422">25 February 2021 06:26</a>
              </span>
            </div>
            <div class="comment-content"><p>@Pete: and all of a sudden the weird DMZ Bandwidth features in Arista EOS make perfect sense. Thanks a million!</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="424">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Scott O'Brien</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c424" href="#424">26 February 2021 04:40</a>
              </span>
            </div>
            <div class="comment-content"><p>How about even Large CLOS networks with the same interface capacity, but accounting for things to fail; fabric cards, links or nodes in disaggregated units. You can either UCMP or drain large parts of your network to get the most out of ECMP.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="427">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c427" href="#427">01 March 2021 08:30</a>
              </span>
            </div>
            <div class="comment-content"><p>Use-cases aside, I think the bigger question is: is it technically viable to implement UCMP in Link State Protocols? LFA, in its simplest form, is the LS&#39; version of UCMP, and it&#39;s computationally intensive when you do it for 1 leaf destination. When you generalize it to calculate UCMP paths to every destination in the network, and the number of nodes gets really huge, into the thousands, this can be computationally intractable. That&#39;s why vendors have to take shortcuts, for ex, not implementing TI-LFA for the case of node failure, only link-failure. </p>

<p>The architects of SPB also faced this computational problem earlier on during their design phase. SPB&#39;s SPF calculation is way more painful than that of OSPF/ISIS, because instead of doing it once from the perspective of the computing router, a SPB switch has to do all-pair SPF calculations, once for each pair of source-destination in the whole topology. Even though they claim that the all-pair SPFs can be trivially parallelized on multi-core CPUs using B-VIDs as the tree identifiers, I don&#39;t believe SPB has ever been tested on real topology of 1000s of nodes. And there&#39;s a limit to multi-core, as the inter-core bus contention and the LLC become the dominating bottleneck factor. Finally, SPB is fundamentally incapable of doing UCMP because the requirement of LFA conflicts with SPB&#39;s path congruency requirement and RPFC will make sure LFA won&#39;t work. </p>

<p>So even if there are situations that require the use of UCMP, one of which brought up by Pete, I don&#39;t think it&#39;s worth the pain for vendors to come up with a UCMP solution for LS protocols Ivan. This is because Link-state protocols essentially work on a common LSDB of global info, and this centralized model won&#39;t scale to very large DB sizes, just like Openflow has tried and failed. Advanced distance vector protocols handle this way more elegantly. </p>

<p>Come to think of it, from the very beginning, in order to scale LS, their inventors had to resort to areas, basically turning LS protocols into distance-vector ones, inter-area-wise. And RIFT, in order to scale better than existing LS protocols in flood-heavy environments, also has to resort to distance-vector principles :)). BGP, the most scalable routing protocol, is a distance-vector protocol after all. </p>

<p>And yet, protocols like EIGRP often got put down over the years in favor of OSPF:</p>

<p>https://www.networkworld.com/article/2347622/eigrp-vs-ospf.html</p>

<p>https://www.networkworld.com/article/2347735/two-more-perspectives-on-eigrp.html</p>

<p>I feel at some point, you might want to write a detailed post on EIGRP to clarify these points raised by Jeff in the 2 articles, as you&#39;re the EIRGP guy Ivan ;) . I find the last sentence in the 2nd article kind of a hidden admission that deep down, Jeff knows EIRGP happens to be the better one among the two though. </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="428">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c428" href="#428">01 March 2021 09:03</a>
              </span>
            </div>
            <div class="comment-content"><p>Re MPTCP, I think it (somewhat) provides an answer to your long-time quest for a session protocol for TCP Ivan :p.  And while MPCTP indeed has its use, in mobile networks where the RTT can vary big time between Wifi and 4G, MPTCP can suffer from HOL blocking on both sender and receiver side, when one sub-flow has to wait for another carrying earlier packets to arrive, basically out-of-order problem. Also, when the flow size is small and bandwidth is high, MPTCP might not have time to utilize the sub-flows or use them effectively, resulting in similar or even worse latency than vanilla TCP. </p>

<p>In DC scenarios, again results vary. For workloads that make use of long-lived elephant flows, MPTCP can provide throughput benefits. For mice flows, which also account for a lot of traffic in DCs, MPTCP&#39;s setup and scheduling overhead outweighs its benefit, so latency performance can be lower than that of single-flow TCP. Also, for disk-bound workloads, since the network is much faster than the disk access time, using MPTCP again provides no benefit, as the bottleneck lies with the disk system.</p>

<p>OTOH, MPTCP can cause starvation/fairness issues in partition/aggregate workload, like MapReduce, or any kind of many-to-one traffic. This kind of traffic can result in Incast scenarios, and TCP Incast can result in TCP Outcast problem for small flows destined to the same host as big flows.  MPTCP makes Outcast worse because of its multi-flow nature, adding to the total number of competing flows. </p>

<p>I think wrt to DCs, DC-TCP provides a much better solution than MPTCP. It works well; it mitigates Incast and Outcast problems, and it does away with big buffer, improving latency. Practically speaking, DCs already have much smaller RTT latencies than WAN/Internet, so solutions that are not viable on the later can be applied here. I&#39;m referring specifically to per-packet ECMP. Per-packet ECMP provides close to ideal network capacity utilization, at the expense of potential reordering. But since DC links have plenty of capacity, DC networks also have much lower latencies, reordering is less likely (due to lots of bandwidth) and can be tolerable (due to much lower RTTs). Coupling that with DC-TCP which deals efficiently with congestion, and we can have a network that makes the best use of traffic load balancing, all without the need for MPTCP, which can be complex to implement.    </p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
