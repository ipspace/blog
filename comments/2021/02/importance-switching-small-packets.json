{
   "comments": [
      {
         "date": "11 February 2021 10:38",
         "html": "<p>Hello Minh, <br />\nAlways a pleasure reading your stuff. You and Ivan are are an explosively powerful mix. :) <br />\nTotally agree with you on small packets. <br />\nA couple of things though for the time being: <br />\nI just want to take a stand for once in defense of the high-end vendors as they are asked for hundreds of sometimes pretty exotic features by their big Service Provider customers and therefore the code and the chipset gets as you pointed out extremely populated with stuff. I have the impression it is inevitable and it&#39;s not just made on purpose to make more money as such.  I reckon they&#39;d rather avoid it if they could. <br />\nThe other thing is if you can open up another blog soon on the interesting. \nstatement: &quot;Self-similar traffic also makes big buffer mostly useless,&quot; :)    </p>\n\n<p>Last thing: I might be naive but our market is in need even more urgently now that the scenario is getting fuzzier and fuzzier of an independent testing facility for functionality AND performance.  Universities should be involved and the vendors too of course. Stringent test procedures and well documented and thus reproducible. In short, proper stuff.   </p>\n\n<p>Cheers/Ciao <br />\nAndrea  </p>\n",
         "id": "398",
         "name": " Andrea Di Donato",
         "pub": "2021-02-11T10:38:33",
         "type": "comment"
      },
      {
         "date": "11 February 2021 11:08",
         "html": "<p>IMIX can also serve as a baseline. It would be better benchmark than for 64B size packets. More important thing is to do multidimensional scale and performance tests on the same OS. Not like unidimensional tests on a tuned OS version per test as some Chinese vendors got used to do it. </p>\n\n<p>In a test with a single feature the performance may not drop significantly. A router OS image packed with many features requires additional CPU cycles to check IF statements and bypass the code, even if it is not turned on but its software module is on the HW line card. Some vendors used a pre-canned OS images to use just the code optimised for a particular tested feature. In reality we turn on many features at the same time and the similar setup should be validated during the test. There are customers who are waiting several years for one official image. Be careful with such vendors.</p>\n",
         "id": "399",
         "name": " Piotr Jablonski",
         "pub": "2021-02-11T11:08:27",
         "type": "comment"
      },
      {
         "date": "11 February 2021 11:16",
         "html": "<p>One more - It&#39;s better to do test based on IMIX as you can have a higher probability of detecting malfunctions and bad architectures. With a fixed size of packets a vendor can use a simpler dedicated ASICs which perform better for this size but not the other. At the end of the day DDOS is still IMIX.</p>\n",
         "id": "400",
         "name": " Piotr Jablonski",
         "pub": "2021-02-11T11:16:55",
         "type": "comment"
      },
      {
         "date": "12 February 2021 08:20",
         "html": "<p>@Andrea,</p>\n\n<p>Thx for the kind words :))! Yeah, I too am aware of vendors being asked by certain high-end SP customers to put all sorts of features on to their chipsets, in kitchen-sink style RFPs. There&#39;re always such customers and it&#39;s fine to address their requirements as such. But that&#39;s exactly why vendors have product lines, like Broadcom creates different chipsets for different markets: Trident for Enterprise, Jericho for SP, Tomahawk for Cloud which is light in feature and powerful in forwarding capacity. Hypothetically speaking, if once-great market leaders like Cisco executed well, it&#39;d not be struggling the way it is now. And I do mean struggling, because Cisco&rsquo;s revenue seems to have been stuck at the current level for a few years now, this year slightly worse than 2019. Looks like Cisco&rsquo;s core business is being slowly eaten away by Cloudification and whiteboxing. It&#39;s been trying hard to diversify in recent years, something it should have started 20 yrs ago when its spirit was still vibrant. </p>\n\n<p>I personally think the problem goes deeper than tech, because if it&#39;s just tech, Broadcom should be in the same rabbit hole. The problem, IMO, lies with vendor strategy, allocation of resources, and execution. Let&#39;s dissect Cisco. The company was a pioneer in the golden age of networking, the 90s. Cisco in its young life was prized, among other things, for its innovative strength and its dynamic culture. IOS for ex, was written by high-class programmers. EIRGP was another great invention, a protocol if rightly configured and designed, is superior to link-state protocols in scalability, and downright better in tree topologies, esp. if one wants to implement valley-free routing, without resorting to BGP unnecessarily. </p>\n\n<p>But Cisco couldn&rsquo;t take advantage of them and of its incumbent advantage the way MS did, making Windows the de-facto standard for Desktop OS. IOS got nowhere near that, and instead, got chopped into all sorts of variants, like IOS-XE, IOS-XR etc etc. It&rsquo;s too complicated and slowly customers started to complain about the complexity of it all. That&rsquo;s just plain stupid IMO, and opened the doors for new entrants to come in and attempt to &ldquo;differentiate with their NOS&rdquo;. Cisco dug its own grave on that one. </p>\n\n<p>As for EIRGP, as promising/elegant as it was, Cisco was never able to impose its own weight and make it into a quasi standard either. Terrible execution, due in part to the then mgmt&rsquo;s arrogance and excessive focus on stock price and addressing &ldquo;shareholder value&rdquo;, I must say. Lucent was a prime example of how following such misguided practice could lead to destruction. Cisco is wiser, but just, because right now, its public image is just a tired aging company, instead of a dynamic technology leader it once was in its younger years. At one point, March 2000, Cisco was the most valuable company in the world, bigger than MS; now its market cap is one-eighth of MS&rsquo; size. Is that the result of sound strategy, good prioritization of resources, and excellent execution? Hardly :p.</p>\n",
         "id": "403",
         "name": " Minh Ha",
         "pub": "2021-02-12T08:20:53",
         "type": "comment"
      },
      {
         "date": "12 February 2021 08:27",
         "html": "<p>And speaking of the market getting fuzzier and in need of indepdendent testing facility, I can&rsquo;t agree with you more! The other day I saw someone mentioned Innovium Teralynx ASIC reaching its 1 million ports on shipment milestone, so I decided to dig around on Teralynx. What I came across was a bunch of superficial whitepapers and articles praising its prowess, no worthwhile architectural info. Here&rsquo;s a sample of those totally worthless articles:</p>\n\n<p>https://www.nextplatform.com/2020/05/11/innovium-stays-on-broadcoms-heels-with-teralynx-8-switch-chips/</p>\n\n<p>I did managed to find one nice piece of info though: &ldquo;As for latency, Khemani says the typical port to port hop is on the order of 500 nanoseconds, but for typical alternatives it is more like 1 microsecond; the important thing is that this number is much lower than what Tomahawk 4 will deliver.&rdquo; So a high-density 400GE switch, offers essentially the same port-to-port latency as 10GE switch? And Tomahawk is worse than that? And I thought the point of having 400G as a standard was so that things could improve on all fronts :p. Looks like the only things that did improve was the Serdes frequency and the serialization speed. This is basically the same as in computing. Intel (and others) comes up with superfast parallel CPUs, only to have them dragged down by memory speed bottleneck, and application latency remains mostly unchanged. </p>\n\n<p>These situations are allowed to happen because the whole networking industry relies on a handful of companies providing them with chipsets/ASICs, the backbone of their products. It stiffens innovation and leads to obfuscation of info as you rightly mentioned, and it doesn&rsquo;t matter since these guys, being the only sources the industry can turn to, make their own rules and get to throw their weight around. </p>\n\n<p>And just like Piotr brought up, tests should be multi-dimensional, because even on hardware levels, there&rsquo;s no chipset architecture that works best for all scenarios. An ASIC may test well with one feature or at a certain level of offered load, only to fall apart as more features are activated or at higher utilizations. Out of order packets for ex, are one outcome that shows up when buffered-crossbar chipsets are stress-tested heavily. Cisco probably learned from this and so, when they designed their CRS platform, they ruled out bit-slicing/cell spraying, sacrificing throughput for in-order port-to-port delivery and better latency:</p>\n\n<p>https://www.cisco.com/c/en/us/td/docs/iosxr/crs/hardware-install/crs-1/8-slot/system-description/b-crs-crs1-8-slot-line-card-chassis-system-description/b-crs-crs1-4-slot-line-card-chassis-system-description_chapter_0100.html  </p>\n\n<p>Re my statement about self-similar traffic and big buffer, basically self-similar/LRD traffic means traffic arrival is bursty on many time scales. And since it&rsquo;s bursty on many time scales, having big buffer helps little with the situation (how much bigger can you make your buffer if the burstiness persists for long), not to mention the bigger the buffer and the more it gets occupied, the larger the latency becomes. And by necessity, by having big buffer, these memories will have to be off-chip, and most likely made of DRAM/RLDRAM, further increase RTT and add more to total device latency.  Even the use of HMC here</p>\n\n<p>https://www.juniper.net/assets/jp/jp/local/pdf/whitepapers/2000599-en.pdf</p>\n\n<p>won&rsquo;t help because HMC trades latency for bandwith. So yes, big buffer has serious latency implications and is best avoided. </p>\n\n<p>For more evidence on the existence of self-similarity in modern network, you can read this one here:</p>\n\n<p>http://conferences.sigcomm.org/imc/2010/papers/p267.pdf</p>\n\n<p>Toward the end, it said &ldquo;Next, we studied the transmission properties of the applications in terms of the flow and packet arrival processes at the edge switches. We discovered that the arrival process at the edge switches is ON/OFF in nature where the ON/OFF durations can be characterized by heavy-tailed distributions.&rdquo; That&rsquo;s the sign of self-similarity, or bursty on different time scales. This paper also mentioned small packets as well, about 50% of the packets they sampled are of the small varieties. </p>\n\n<p>And wrt the futility of using big buffer to address burstiness, esp. Microburst in the DC, you can take a look here, this one involved MS research so its findings are empirical as well:</p>\n\n<p>https://arxiv.org/pdf/1604.07621.pdf </p>\n",
         "id": "404",
         "name": " Minh Ha",
         "pub": "2021-02-12T08:27:08",
         "type": "comment"
      }
   ],
   "count": 5,
   "type": "post",
   "url": "2021/02/importance-switching-small-packets.html"
}
