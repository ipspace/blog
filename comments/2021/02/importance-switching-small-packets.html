<div class="comments post" id="comments">
  <h4>4 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="398">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Andrea Di Donato</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c398" href="#398">11 February 2021 10:38</a>
              </span>
            </div>
            <div class="comment-content"><p>Hello Minh, <br />
Always a pleasure reading your stuff. You and Ivan are are an explosively powerful mix. :) <br />
Totally agree with you on small packets. <br />
A couple of things though for the time being: <br />
I just want to take a stand for once in defense of the high-end vendors as they are asked for hundreds of sometimes pretty exotic features by their big Service Provider customers and therefore the code and the chipset gets as you pointed out extremely populated with stuff. I have the impression it is inevitable and it&#39;s not just made on purpose to make more money as such.  I reckon they&#39;d rather avoid it if they could. <br />
The other thing is if you can open up another blog soon on the interesting. 
statement: &quot;Self-similar traffic also makes big buffer mostly useless,&quot; :)    </p>

<p>Last thing: I might be naive but our market is in need even more urgently now that the scenario is getting fuzzier and fuzzier of an independent testing facility for functionality AND performance.  Universities should be involved and the vendors too of course. Stringent test procedures and well documented and thus reproducible. In short, proper stuff.   </p>

<p>Cheers/Ciao <br />
Andrea  </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="399">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c399" href="#399">11 February 2021 11:08</a>
              </span>
            </div>
            <div class="comment-content"><p>IMIX can also serve as a baseline. It would be better benchmark than for 64B size packets. More important thing is to do multidimensional scale and performance tests on the same OS. Not like unidimensional tests on a tuned OS version per test as some Chinese vendors got used to do it. </p>

<p>In a test with a single feature the performance may not drop significantly. A router OS image packed with many features requires additional CPU cycles to check IF statements and bypass the code, even if it is not turned on but its software module is on the HW line card. Some vendors used a pre-canned OS images to use just the code optimised for a particular tested feature. In reality we turn on many features at the same time and the similar setup should be validated during the test. There are customers who are waiting several years for one official image. Be careful with such vendors.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="400">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c400" href="#400">11 February 2021 11:16</a>
              </span>
            </div>
            <div class="comment-content"><p>One more - It&#39;s better to do test based on IMIX as you can have a higher probability of detecting malfunctions and bad architectures. With a fixed size of packets a vendor can use a simpler dedicated ASICs which perform better for this size but not the other. At the end of the day DDOS is still IMIX.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="403">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c403" href="#403">12 February 2021 08:20</a>
              </span>
            </div>
            <div class="comment-content"><p>@Andrea,</p>

<p>Thx for the kind words :))! Yeah, I too am aware of vendors being asked by certain high-end SP customers to put all sorts of features on to their chipsets, in kitchen-sink style RFPs. There&#39;re always such customers and it&#39;s fine to address their requirements as such. But that&#39;s exactly why vendors have product lines, like Broadcom creates different chipsets for different markets: Trident for Enterprise, Jericho for SP, Tomahawk for Cloud which is light in feature and powerful in forwarding capacity. Hypothetically speaking, if once-great market leaders like Cisco executed well, it&#39;d not be struggling the way it is now. And I do mean struggling, because Cisco&rsquo;s revenue seems to have been stuck at the current level for a few years now, this year slightly worse than 2019. Looks like Cisco&rsquo;s core business is being slowly eaten away by Cloudification and whiteboxing. It&#39;s been trying hard to diversify in recent years, something it should have started 20 yrs ago when its spirit was still vibrant. </p>

<p>I personally think the problem goes deeper than tech, because if it&#39;s just tech, Broadcom should be in the same rabbit hole. The problem, IMO, lies with vendor strategy, allocation of resources, and execution. Let&#39;s dissect Cisco. The company was a pioneer in the golden age of networking, the 90s. Cisco in its young life was prized, among other things, for its innovative strength and its dynamic culture. IOS for ex, was written by high-class programmers. EIRGP was another great invention, a protocol if rightly configured and designed, is superior to link-state protocols in scalability, and downright better in tree topologies, esp. if one wants to implement valley-free routing, without resorting to BGP unnecessarily. </p>

<p>But Cisco couldn&rsquo;t take advantage of them and of its incumbent advantage the way MS did, making Windows the de-facto standard for Desktop OS. IOS got nowhere near that, and instead, got chopped into all sorts of variants, like IOS-XE, IOS-XR etc etc. It&rsquo;s too complicated and slowly customers started to complain about the complexity of it all. That&rsquo;s just plain stupid IMO, and opened the doors for new entrants to come in and attempt to &ldquo;differentiate with their NOS&rdquo;. Cisco dug its own grave on that one. </p>

<p>As for EIRGP, as promising/elegant as it was, Cisco was never able to impose its own weight and make it into a quasi standard either. Terrible execution, due in part to the then mgmt&rsquo;s arrogance and excessive focus on stock price and addressing &ldquo;shareholder value&rdquo;, I must say. Lucent was a prime example of how following such misguided practice could lead to destruction. Cisco is wiser, but just, because right now, its public image is just a tired aging company, instead of a dynamic technology leader it once was in its younger years. At one point, March 2000, Cisco was the most valuable company in the world, bigger than MS; now its market cap is one-eighth of MS&rsquo; size. Is that the result of sound strategy, good prioritization of resources, and excellent execution? Hardly :p.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
