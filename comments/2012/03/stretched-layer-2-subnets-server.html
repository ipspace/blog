<div class="comments post" id="comments">
  <h4>6 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="309030374862414937">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Kris</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c309030374862414937" href="#309030374862414937">27 March 2012 15:56</a>
              </span>
            </div>
            <div class="comment-content">we have a link to our remote l2 subnetzs over l2tpv3 for l2 ids (the tco of dedicated sensors would&#39;ve been higher).<br />thats not an application example the two of you are meant, but sometimes a single reason is enough... and the documentation battle has another hillto capture. =)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="9063565340542761145">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">old timer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9063565340542761145" href="#9063565340542761145">04 April 2012 06:07</a>
              </span>
            </div>
            <div class="comment-content">There are many reasons why layer 2 is so over played and over stretched. Layer 3 invoves routing and routing protocols which a lot of people don&#39;t understand,  so they avoid it as much as they can. In the metro space, customers are deploying layer 2 Metro swtiches instead of layer 3, because layer 3 routers have the features but not wire speed packet throughput performances (think about Cisco ISR routers). Today I see a lot of folks are trying to solve problems that have been solved so elegantly by routing long time ago.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="788938685283881377">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Ed</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c788938685283881377" href="#788938685283881377">20 April 2012 20:49</a>
              </span>
            </div>
            <div class="comment-content">Wait...solve the problem of changing IPs when moving VMs across an L3 connection between data centers using...routing protocols?  I&#39;m not saying you&#39;re wrong but I can&#39;t understand how that would be done, except to maybe use NAT and/or tunneling?  What do you mean that this problem can be solved by using routing protocols?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4938966896272269188">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4938966896272269188" href="#4938966896272269188">20 April 2012 20:55</a>
              </span>
            </div>
            <div class="comment-content">IP addresses on server loopback interfaces and servers running routing protocols with the first-hop switches. That&#39;s how server interface redundancy has been done before we forgot how to do it properly and decided L2 tricks make more sense.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="783476849858965662">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://packetwrangler.wordpress.com/" rel="nofollow">PacketWrangler</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c783476849858965662" href="#783476849858965662">25 January 2013 19:34</a>
              </span>
            </div>
            <div class="comment-content">That&#39;s flippin&#39; brilliant! I&#39;d never thought of that as even an option.  Makes me want to go try it now though.  Would each server advertise a /32 or /128?  Wouldn&#39;t this result in a rather large routing table?</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8596367554064390992">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Jon Hudson</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8596367554064390992" href="#8596367554064390992">21 April 2012 11:28</a>
              </span>
            </div>
            <div class="comment-content">Let me add a few logs to the fire ;-)<br /><br />First off it&#39;s even worse than many realize in many ways. The reason you need the ipaddr to stay the same is because you don&#39;t want to terminate connections/sessions. You goal of all of this is to be able to move an application without the users connecting to that application from noticing anything has happened. This may mean a web page reload, but state is maintained and transactional records are kept and continue. <br /><br />Application Affinity is also required. You can&#39;t move a single VM that has any local dependencies. You are moving 6, 10, 20 VMs at once. You have to move the web servers, the app servers, the databases and whatever support services it depends on. Apps must be moved either as a whole, or in pre-established pods or clusters that are wholly contained. <br /><br />When you consider that a single VM moving can eat a 10G link, you can imagine the bandwidth needed to move 20? Good thing 100GbE is so economical ;-)<br /><br />This is NOT for every application. Most applications can handle (from an SLA point of view) a few minutes of downtime. For those apps, you don&#39;t need to do this. You want to do snapshot based point in time array based replication. In this model you just bring up clones of the VMs at another site that are maybe 5-50min old. So you lose some data. This model works today, we have evidence of it working very well, especially in cases where warning can be given of the event. And it&#39;s much more within the budget. <br /><br />The main gain of this method is that complexity is reduced, cost is reduced, latency limitations go from 10ms to basically 350ms. And you don&#39;t really need to stretch anything. The clones can be brought up in a &quot;quarantined&quot; environment and their ipaddr/netmask/gateway changed before they are &quot;promoted&quot; and allowed to accept connections. A GSLB can then all that server to the pool and direct connections to the new location. <br /><br />For the top end of applications, and in the beginning just for that top end of companies/agencies the ability to move a pack of VMs from one location to another is not only possible, it&#39;s happening. I personally have clients doing it. It&#39;s not easy. It&#39;s VERY expensive. And it&#39;s pretty damn cool. <br /><br />But for most application and for most customers a point in time recovery solution is just fine and MUCH easier. <br /><br />The whole reason behind much of this is business continuity. It&#39;s about having applications that can just be migrated from data center to data center allowing that hardware to be worked on while the application migrates on to greener pastures. This is the Ferrari solution. Initially this is being done by large government agencies and large corporations where for them, the cost and complexity are justified. <br /><br />However, while everyone wants a Ferrari, a nice VW gets to you to work and back perfectly fine.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
