comments:
- date: 09 February 2012 11:50
  html: Amazing insights here Ivan.<br /><br />You mention a limit of 409x ports in
    the port-group, tho I assume that this is a limit per Host/OVS? Now for sensible
    designs 409x hosts is more than enough, let alone 409x multiplied by a max of
    32 hosts in a cluster, tho I can picture some instances where this may be beneficial.
  id: '7222783992569460667'
  image: https://resources.blogblog.com/img/blank.gif
  name: Kurt Bales
  profile: null
  pub: '2012-02-09T11:50:57.724+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 12:00
  html: That&#39;s the total number of VMs you can connect to the port group (across
    all hosts with the same vDS). They need per-VM VLAN to create a P2P link between
    VM and OVS-VM, and you only have 4K VLANs (and you can&#39;t recycle them because
    someone could vMotion a VM to another host).
  id: '1162605474426709770'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-02-09T12:00:35.789+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 13:04
  html: "Do you have a source for this claim? &quot;(you can\u2019t push more than\
    \ a few Gbps through userland).&quot; My understanding and experience has been\
    \ that ESX can push as much as the OS can handle, and easily saturates 10Gpbs\
    \ with things like vMotion if the physical network can handle it. Obviously, different\
    \ interfaces and kernels here. I&#39;m just wondering if perhaps you might be\
    \ underestimating or downplaying the potential capabilities..."
  id: '4438330385911839197'
  image: https://resources.blogblog.com/img/blank.gif
  name: Loren Gordon
  profile: null
  pub: '2012-02-09T13:04:34.539+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 14:12
  html: In my understanding and according to your previous blog post (http://blog.ioshints.info/2011/06/test-your-vmware-networking-skills.html
    ) we can&#39;t reuse VLANs even across different port groups, because port groups
    don&#39;t provide isolation.
  id: '9190931295733821752'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anton Marchenko
  profile: null
  pub: '2012-02-09T14:12:48.251+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 14:12
  html: I don&#39;t (yet) have a consistent theory behind anecdotal evidence and a
    few data points ... and the fact that every time someone describes a VM-based
    networking appliance solution to me I ask &quot;and the performance is around
    a few Gbps&quot; ... and get &quot;yeah&quot; as an answer.<br /><br />Two data
    points I already wrote about:<br />http://blog.ioshints.info/2011/11/junipers-virtual-gateway-virtual.html<br
    />http://www.ipspace.net/Embrane_heleos:_scale-out_distributed_virtual_appliance
  id: '3101750156308528300'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-02-09T14:12:54.866+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 14:14
  html: '... also, please note that the &quot;few Gbps&quot; applies to VMs doing
    network-layer packet forwarding. Server VMs can easily saturate 10 Gbps uplink
    without consuming a whole core.'
  id: '1979265374259469218'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-02-09T14:14:50.534+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 14:19
  html: 'Good one. Absolutely true. You can however reuse them across different vSwitches/vDS
    (because they are independent bridging domains).<br /><br />Summary: create a
    totally new vDS for Nicira&#39;s needs.'
  id: '3550732108206638851'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-02-09T14:19:49.673+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 14:59
  html: Actually it means that to scale to more than 4K VMs you have create several
    vDS. Does it also mean that you have to provision a different OVS VM per vDS on
    the same ESX host or you can reuse the same VLANs across different vNIC trunks
    coming from different vDS to the same OVS VM?
  id: '5807267127922175894'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anton Marchenko
  profile: null
  pub: '2012-02-09T14:59:57.495+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 18:35
  html: A traditional vSwitch is just as much a SPOF, right? In fact it&#39;s worse
    if it runs inside the VMkernel.
  id: '7101374179710136821'
  image: https://resources.blogblog.com/img/blank.gif
  name: wmf
  profile: null
  pub: '2012-02-09T18:35:00.135+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 20:04
  html: Very Impressive break-down Ivan  ;)
  id: '1284421911644474340'
  image: https://resources.blogblog.com/img/blank.gif
  name: Tommy P
  profile: null
  pub: '2012-02-09T20:04:24.528+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 09 February 2012 21:12
  html: Thanks for this clarification, it wasn&#39;t until I read this that it clicked
    about the VLAN usage and p2p to the OVS VM. Originally I was thinking like Kurt
    if this was per host. But per 32 host cluster/VDS makes sense and does scale pretty
    well. ~126-7 VMs per host isn&#39;t too shabby.
  id: '7670120774594923262'
  image: https://resources.blogblog.com/img/blank.gif
  name: DanielG
  profile: null
  pub: '2012-02-09T21:12:01.580+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 02:04
  html: Nicira + Open vSwitch + VMware = DOA (unfortunately)
  id: '188939034988149998'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2012-02-10T02:04:29.211+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 02:39
  html: Agree 100%.
  id: '66805272231077057'
  image: https://resources.blogblog.com/img/blank.gif
  name: cloudtoad
  profile: null
  pub: '2012-02-10T02:39:12.241+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 09:12
  html: This was true until x86 leaders came with new data plane architecture. We
    are a proved example that you can deliver dozens of Gbps with virtual networking
    appliance on userland. also very important, independent from he packet size (so
    consider the pps benchmarks!). We delivered all around the world high performance
    SDN for mobile core network and are ramping up now on the Cloud space...
  id: '11482377547165107'
  image: https://resources.blogblog.com/img/blank.gif
  name: David Le Goff
  profile: null
  pub: '2012-02-10T09:12:13.661+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 09:43
  html: Sounds absolutely interesting. If you&#39;re willing to tell me more, please
    contact me directly:<br /><br />http://www.ipspace.net/Contact
  id: '2992628193466207124'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-02-10T09:43:02.240+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 10:01
  html: done  :)
  id: '7632673379150605779'
  image: https://resources.blogblog.com/img/blank.gif
  name: David Le Goff
  profile: null
  pub: '2012-02-10T10:01:36.101+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 13:58
  html: I wish I had 10GbE to the servers in my lab...this would be a dead simple
    test. Set up a test VM configured as a router and see what we get!
  id: '7304362985981442708'
  image: https://resources.blogblog.com/img/blank.gif
  name: Loren Gordon
  profile: null
  pub: '2012-02-10T13:58:44.076+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 10 February 2012 14:14
  html: VM userland &gt; dozens Mpps with 2vCPU (L3 forwarding), dozens Gbps with
    2vCPU (IPsec).  Scales linearly with number of cores assigned, no crypto engine,
    pure software.  8-) we have a booth at MWC (Hall 2 - 2B122)
  id: '1881158899704955857'
  image: https://resources.blogblog.com/img/blank.gif
  name: David Le Goff
  profile: null
  pub: '2012-02-10T14:14:06.283+01:00'
  ref: '4582942090096066804'
  type: comment
- date: 14 February 2012 05:34
  html: Great post! Love the graphics. I labbed up GRE tunnels on a couple OpenVswitch
    boxes with KVM to test out some V-2-V migrations. Still trying to wrap my head
    around scale and op management.<br />Notes from the setup for anyone needing a
    primer to test themselves in their environment.<br />http://wp.me/p1AOVJ-2O
  id: '6681750003884305057'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brent Salisbury
  profile: null
  pub: '2012-02-14T05:34:53.393+01:00'
  ref: '4582942090096066804'
  type: comment
- comments:
  - date: 05 May 2014 18:22
    html: You can use VLAN tagging in vSwitch and vDS, but what we need here is the
      ability to have every port within a single port group in a different VLAN, and
      port attributes are only available in vDS (vSwitch has only port group attributes).
    id: '3579452442629709205'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2014-05-05T18:22:59.767+02:00'
    ref: '4677800369073856731'
    type: comment
  date: 05 May 2014 18:15
  html: Hi<br /><br />Haven&#39;t tested but i think you can tag all the vlans to
    a VM in a standard virtual switch (not distributed)<br /><br />look here<br /><br
    />http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=1004252<br
    /><br />Regards,
  id: '4677800369073856731'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2014-05-05T18:15:04.892+02:00'
  ref: '4582942090096066804'
  type: comment
count: 21
id: '4582942090096066804'
type: post
url: 2012/02/nicira-open-vswitch-inside-vsphereesx.html
