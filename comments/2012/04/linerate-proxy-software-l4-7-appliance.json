{
  "comments": [
    {
      "date": "17 April 2012 14:40",
      "html": "Would be really cool if they ran the open vswitch on the southbound interfaces, and partnered with Nicira and/or Big Switch, so that the appliance could be used as a gateway in Overlay based clouds such as, um, Rackspace.",
      "id": "1041307680405047697",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Brad Hedlund",
      "profile": null,
      "pub": "2012-04-17T14:40:13.611+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "17 April 2012 14:47",
      "html": "Hear, hear!",
      "id": "7969063468927118039",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "ccie15672",
      "profile": null,
      "pub": "2012-04-17T14:47:12.322+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "17 April 2012 14:52",
      "html": "Yeah, that would be cool, but I wonder what their TCP stack performance would be with extra MAC-over-IP encapsulation not supported by NIC HW, so it would be either STT or lower performance (assuming they rely on TCP offload to get their numbers, which remained a gray area during my briefing).",
      "id": "8855111238263575624",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2012-04-17T14:52:28.307+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "17 April 2012 18:33",
      "html": "I&#39;m very wary about the SSL performance as that&#39;s been one aspect that we&#39;ve relied heavily on SSL ASICs for many years. With the migration to 2048 bit certs, which eats up about 5x as many CPU cycles as 1024 bit keys, I&#39;m going to need proof that generic x86 CPU cycles can scale out with 2048-bit SSL keys.<br /><br />Each new SSL connection will require an asymmetric operating to occur, and that&#39;s the part that has blasted CPUs before. Putting that onto an ASIC has helped greatly over the years. Most load balancing vendors have that, usually as a PCI card from Cavium. <br /><br />Admittedly though, it&#39;s been a while since I know anyone that&#39;s tested SSL performance with modern hard are. I&#39;m willing to challenge the &quot;need ASIC&quot; assumption, but I&#39;m going to want proof.",
      "id": "2941513697533320173",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "tbourke",
      "profile": null,
      "pub": "2012-04-17T18:33:42.520+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "17 April 2012 20:20",
      "html": "It&#39;d be cool if SSL is using AES-NI engine which can offload AES instruction to host CPU (even from VM) and configure the load balance to favor AES encryption.",
      "id": "7728221217283881283",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2012-04-17T20:20:59.187+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "17 April 2012 22:47",
      "html": "I wonder if a GPU could do this job...",
      "id": "8560987688118677908",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "ccie15672",
      "profile": null,
      "pub": "2012-04-17T22:47:28.127+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "17 April 2012 22:53",
      "html": "This is Manish at LineRate.  We don&#39;t use any special TCP offload hardware.  We typically measure peak performance using Intel&#39;s 82599 NICs, so pretty standard stuff.",
      "id": "4403334818494089331",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Manish Vachharajani",
      "profile": null,
      "pub": "2012-04-17T22:53:29.279+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 04:48",
      "html": "Brad, I spoke with Radware earlier at their booth at ONS and think they are sort of accomplishing what you are referring to here.  They have and always had their Service Delivery Controller, managing their own ADCs, but now are integrating it with an OpenFlow controller that is then in turn controlling the OF switches in the environment.  With this integration between the OF and Radware controllers, they are able to map tenants to specific virtual or physical appliances.  Nice way for L4-7 insertion.<br /><br />This kind of controller to controller integration will be paramount for dynamic gateway and L4-7 insertion.  No reason to openflow enable everything, just integrate the head ends on each side.",
      "id": "7161241714911513030",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "@jedelman8",
      "profile": null,
      "pub": "2012-04-18T04:48:06.517+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 07:01",
      "html": "20 - 40Gbps throughput on a single CPU? That is just not possible. The highest performance single socket Sandy Bridge can do only about 25Gbps with just packet in/out without any thing useful running, on bare metal. If this throughput is on multiple CPUs, why stop at 20 -40Gbps, shouldn&#39;t this be able to scale out -- like indefintely (at least for a while)?",
      "id": "122820706594874115",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "old timer",
      "profile": null,
      "pub": "2012-04-18T07:01:49.047+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 07:47",
      "html": "Obviously they need multiple cores to reach that throughput. As for &quot;indefinite scaling&quot;, you hit a number of other limitations (listen to one of the recent Packet Pushers podcasts on server architectures). At a certain point, it makes more sense to deploy a second box than to increase the performance of a single server.",
      "id": "4176009441934598856",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2012-04-18T07:47:32.960+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 07:53",
      "html": "82599 can do TCP segmentation (including Receive Side Coalescing) and checksum offload. Not a full TCP offload, but the time-consuming functionality is implemented in hardware.",
      "id": "7357316650763422391",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2012-04-18T07:53:02.954+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 08:15",
      "html": "Service insertion: we need a shiny new protocol because the old ones (like WCCP) wouldn&#39;t ever work and because things like MPLS warp space-time continuum. Makes me sick.",
      "id": "5328601688855037244",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2012-04-18T08:15:18.909+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 13:16",
      "html": "Unfortunately, AES-NI can only help with the symmetric encryption. The hard part (and CPU-blasting part) of an SSL connection is the asymmetric part, which is required for each new SSL connection, and AES-NI doesn&#39;t help with asymmetric encryption.",
      "id": "5291208025271976895",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "tbourke",
      "profile": null,
      "pub": "2012-04-18T13:16:25.773+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "18 April 2012 15:46",
      "html": "Maybe the real money is simply in a clean and easy UI and use &quot;old&quot; protocols such as MPLS. :)",
      "id": "3323801680507013059",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "@jedelman8",
      "profile": null,
      "pub": "2012-04-18T15:46:01.673+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "19 April 2012 05:38",
      "html": "I probably didn&#39;t make myself clear. Single CPU implies the multi-core CPU but single socket. To achieve a 20-40Gbps throughput on a sinlge socket with service is not possible. If you run on multiple sockets, these multiple sockets don&#39;t have to reside on the same physical machine, they are mostly likely independant machines connected by the network. If these software only solutions were to have real value and beauty, they should be able to run on multiple machines, to truly scale out -- like indefintely :-)<br /><br />BTW, How many companies are doing this kind of stuff? There is Zeus from Riverbed, Embrane (spell it right?) and this LineRate. Any more?",
      "id": "4922847123569442311",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "old timer",
      "profile": null,
      "pub": "2012-04-19T05:38:32.184+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "20 April 2012 04:03",
      "html": "Those throughputs are very much possible on a single CPU.  New software architectures coupled with Sandy Bridge EN hardware allow for orders of magnitude increases in performance.  On those systems we are seeing 11+Mpps per core (not CPU) for L3 forwarding even with a full LPM on every packet.  Linear scalability is limited by other factors in the system, but it&#39;s much higher than 40Gbps.  Packets rates decrease as complexity of the services increases, however it is still in the Mpps/Core range.  Even more interesting is packet latency.  Compared to a standard Linux or BSD stack, we see min/avg/max something like 6/12/200 microseconds.  Equivalent max times in Linux are sometimes 500ms under load.",
      "id": "4795767701926794223",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Robert Bays",
      "profile": null,
      "pub": "2012-04-20T04:03:49.515+02:00",
      "ref": "8850714851314952199",
      "type": "comment"
    },
    {
      "date": "07 November 2012 01:05",
      "html": "I confirm the values and a single CPU can do much more. With our networking acceleration platform, we get 11Mpps/ core wih linear scalability when you add cores.<br />On a dual sandy bridge CPU, we demonstrate 162Mpps. Performance independent from packet size.<br />",
      "id": "7739983206967490989",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "David Le Goff",
      "profile": "http://www.6wind.com",
      "pub": "2012-11-07T01:05:30.491+01:00",
      "ref": "8850714851314952199",
      "type": "comment"
    }
  ],
  "count": 17,
  "id": "8850714851314952199",
  "type": "post",
  "url": "2012/04/linerate-proxy-software-l4-7-appliance.html"
}