comments:
- date: 16 April 2012 16:49
  html: THANK YOU!  You just put into words the idea that has been bouncing around
    in my mind for the past few months.  Ever since I read about Brocade (Foundary)
    VCS, I&#39;ve been struggling with the thought that &quot;something isn&#39;t
    right&quot; with full mesh.  As far as I can tell, Brocade is advocating full
    mesh because they have no large VCS capable switches.  Their answer seems to be
    to just mesh the ToR switches and argue that you don&#39;t need a large switch
    for aggregation.  Since I&#39;ve been seeing that same argument in several places,
    I thought that I must either be dumber than I thought or just missing something
    very obvious!
  id: '3403049232250330727'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jamie Burch
  profile: null
  pub: '2012-04-16T16:49:48.895+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 16 April 2012 18:03
  html: A very simple change to forwarding model, known as Valiant Load Balancing
    (VLB) makes full-mesh topology (aka k-ary 1-flat butterfly) fully load-balanced.
    VLB is fairly easy to implement on a proprietary fabric, as long as underlying
    chipset supports encap-decap functionality (in fact I did thate even in dynamips
    lol). There is a tradeoff, of course - the throughput is reduced by 50%, though
    this still way better as compared to minimal routing.<br /><br />It is also possibly
    to utilize practically full capacity in the fully meshed fabric by using adaptive
    routing methods, which are, however, harder to implement. For instance, you can
    run a full-mesh of MPLE-TE tunnels ;) Though this model has way too slow response
    time to be used in data-center. <br /><br />Overall, adaptive routing has been
    a big area of research in 70-90s for HPC systems, and a lot of interesting results
    could be borrowed from there. It&#39;s just that modern DC&#39;s are built from
    commodity gear which normally has pretty limited functionality.
  id: '1300649545572741324'
  image: https://resources.blogblog.com/img/blank.gif
  name: plapukhov
  profile: null
  pub: '2012-04-16T18:03:35.518+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 17 April 2012 01:30
  html: Jamie,<br />In a proper CLOS Leaf/Spine topology you can scale the fabric
    pretty large using existing 1RU and 2RU switches.
  id: '7504367818889713761'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2012-04-17T01:30:16.523+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 17 April 2012 02:07
  html: First off, great write up as usual.<br /><br />And I will in fairness say
    that there could be applications written in the future that take advantage of
    more full mesh designs. And it does depend on your design goals, bandwidth vs
    latency vs consistency vs resiliency etc. As Plapukhov stated above, there are
    ways to do full mesh to one&#39;s benefit today. That is just not how we do things
    today in datacenters, and my guess is we won&#39;t any time soon. <br /><br />But
    for the most part ever since Charles Clos released his work in 1953 http://en.wikipedia.org/wiki/Clos_network
    communications have proven time and time again that a CLOS based architecture
    gives the best performance. <br /><br />This is illustrated (somewhat humorously)
    by the fact that if you build a full mesh fabric today that most if not all of
    the switches you would be using to do so have inside them asics on a board that
    are in CLOS configuration. <br /><br />For most applications today CLOS ROCKS.
  id: '3373021824038552265'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jon Hudson
  profile: null
  pub: '2012-04-17T02:07:24.893+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 17 April 2012 02:36
  html: Hmm... I&#39;m not sure what you were told, all of the VCS reference designs
    that I know of are NOT full all-2-all mesh networks. There are usually 2 tier
    clos networks. <br /><br />I personally do talk in some of my presentations that
    things like TRILL mean you can more safely look at new topologies like full mesh
    and others and that we may see more of that in the future. But I know of no datacenter
    application today that would be helped by a full mesh topology.<br /><br />If
    you can point me to where you saw this I&#39;m more than happy to look into it.
    It&#39;s very possible that it&#39;s simply the miss use of the phrase &quot;full
    mesh&quot;.
  id: '4070577013132497252'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jon Hudson
  profile: null
  pub: '2012-04-17T02:36:19.440+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 17 April 2012 04:48
  html: i&#39;m assuming that you&#39;re saving the more interesting components of
    a clos fabric for your webinar. (sorry, i won&#39;t be able to make that) but
    at least a drive-by reference to some of the merits of a fat tree clos and the
    enhancements in asics to facilitate respectable ECMP might put a little more balance
    on the topic.  to jump from full mesh to a 2-node spine seems to leave out some
    of the more interesting capabilities of the topology. ;)
  id: '3223191064665185503'
  image: https://resources.blogblog.com/img/blank.gif
  name: steve ulrich
  profile: null
  pub: '2012-04-17T04:48:17.936+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 17 April 2012 08:21
  html: Hi Steve!<br /><br />I wasn&#39;t actually holding back - I just wanted to
    make a quick comparison between two topologies. There will be more blog posts
    on Clos fabrics.<br /><br />Also, if you do have something on ASICs you can share,
    that would be most appreciated. It&#39;s getting harder and harder to get access
    to HW documentation without NDA (or similar restrictions).
  id: '8954635524819730779'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-04-17T08:21:44.587+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 18 April 2012 18:16
  html: I honestly cannot point to a specific reference to full mesh, so that may
    have been something that started in my mind and had positive reinforcement bias
    as I have been reading several articles about full mesh (or even partial mesh)
    recently.<br /><br />Not to go too far off of the topic of this blog, the main
    issue that jumped out to me was the limitation of only 24 switches in a VCS fabric.  If
    2+ switches are used as spine nodes, that would leave 22 or less for leaf nodes.  I
    guess that led me to the thought that you would want to be able to use all available
    nodes as leaf nodes, and my mind constructed the idea of a full mesh to accomplish
    this.  <br /><br />Given that their largest switch supporting VCS (that I know
    of) is the VDX6730-76 with 60 ethernet ports, I would assume 4 spine nodes to
    allow full bandwidth within the fabric (2 spine nodes wouldn&#39;t have enough
    ports for 4 uplinks each from all 22 leaf nodes).  That leaves only 20 leave nodes,
    with 52 ports each as your maximum access port count, and even then it&#39;s at
    6.5:1 oversubscription.
  id: '2689458833038340937'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jamie Burch
  profile: null
  pub: '2012-04-18T18:16:27.202+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 18 April 2012 18:44
  html: 'Here&#39;s what you can do: 4 spine switches, 15 edge switches (all VDX 6720
    with 60 10GE ports). Use 16 ports for uplinks on edge switches, 44 ports for server
    connectivity --&gt; 660 10GE ports = 330 servers with dual 10GE uplinks. Very
    close to the VMware vDS limit. Accidental perfect sizing, I would say :D'
  id: '2861781509327583475'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-04-18T18:44:45.590+02:00'
  ref: '1438486463259618950'
  type: comment
- date: 10 February 2013 17:44
  html: Try looking at the Plexxi solution. Plexxi does not care about equal costs,
    or shortest paths, and creates a full mesh in which all bandwidth is usable (2:1)
    using very minimal cabling. Very interesting technology<br />http://www.plexxi.com/
  id: '6083052098879313635'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Colin
  profile: https://www.blogger.com/profile/07555981356671239778
  pub: '2013-02-10T17:44:01.123+01:00'
  ref: '1438486463259618950'
  type: comment
- comments:
  - date: 19 May 2018 11:55
    html: Thank you. Removed. That web site was set up to promote a Juniper webinar
      and no longer works.
    id: '1074993703046084918'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2018-05-19T11:55:11.695+02:00'
    ref: '1413647871673744507'
    type: comment
  date: 19 May 2018 11:44
  html: Just to note the &#39;Guide to Network Fabrics&#39; results in a 404.
  id: '1413647871673744507'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/12167799894831585282
  pub: '2018-05-19T11:44:06.643+02:00'
  ref: '1438486463259618950'
  type: comment
count: 12
id: '1438486463259618950'
type: post
url: 2012/04/full-mesh-is-worst-possible-fabric.html
