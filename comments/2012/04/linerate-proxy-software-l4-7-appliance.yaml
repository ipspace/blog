comments:
- date: 17 April 2012 14:40
  html: Would be really cool if they ran the open vswitch on the southbound interfaces,
    and partnered with Nicira and/or Big Switch, so that the appliance could be used
    as a gateway in Overlay based clouds such as, um, Rackspace.
  id: '1041307680405047697'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2012-04-17T14:40:13.611+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 17 April 2012 14:47
  html: Hear, hear!
  id: '7969063468927118039'
  image: https://resources.blogblog.com/img/blank.gif
  name: ccie15672
  profile: null
  pub: '2012-04-17T14:47:12.322+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 17 April 2012 14:52
  html: Yeah, that would be cool, but I wonder what their TCP stack performance would
    be with extra MAC-over-IP encapsulation not supported by NIC HW, so it would be
    either STT or lower performance (assuming they rely on TCP offload to get their
    numbers, which remained a gray area during my briefing).
  id: '8855111238263575624'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-04-17T14:52:28.307+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 17 April 2012 18:33
  html: I&#39;m very wary about the SSL performance as that&#39;s been one aspect
    that we&#39;ve relied heavily on SSL ASICs for many years. With the migration
    to 2048 bit certs, which eats up about 5x as many CPU cycles as 1024 bit keys,
    I&#39;m going to need proof that generic x86 CPU cycles can scale out with 2048-bit
    SSL keys.<br /><br />Each new SSL connection will require an asymmetric operating
    to occur, and that&#39;s the part that has blasted CPUs before. Putting that onto
    an ASIC has helped greatly over the years. Most load balancing vendors have that,
    usually as a PCI card from Cavium. <br /><br />Admittedly though, it&#39;s been
    a while since I know anyone that&#39;s tested SSL performance with modern hard
    are. I&#39;m willing to challenge the &quot;need ASIC&quot; assumption, but I&#39;m
    going to want proof.
  id: '2941513697533320173'
  image: https://resources.blogblog.com/img/blank.gif
  name: tbourke
  profile: null
  pub: '2012-04-17T18:33:42.520+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 17 April 2012 20:20
  html: It&#39;d be cool if SSL is using AES-NI engine which can offload AES instruction
    to host CPU (even from VM) and configure the load balance to favor AES encryption.
  id: '7728221217283881283'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2012-04-17T20:20:59.187+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 17 April 2012 22:47
  html: I wonder if a GPU could do this job...
  id: '8560987688118677908'
  image: https://resources.blogblog.com/img/blank.gif
  name: ccie15672
  profile: null
  pub: '2012-04-17T22:47:28.127+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 17 April 2012 22:53
  html: This is Manish at LineRate.  We don&#39;t use any special TCP offload hardware.  We
    typically measure peak performance using Intel&#39;s 82599 NICs, so pretty standard
    stuff.
  id: '4403334818494089331'
  image: https://resources.blogblog.com/img/blank.gif
  name: Manish Vachharajani
  profile: null
  pub: '2012-04-17T22:53:29.279+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 04:48
  html: Brad, I spoke with Radware earlier at their booth at ONS and think they are
    sort of accomplishing what you are referring to here.  They have and always had
    their Service Delivery Controller, managing their own ADCs, but now are integrating
    it with an OpenFlow controller that is then in turn controlling the OF switches
    in the environment.  With this integration between the OF and Radware controllers,
    they are able to map tenants to specific virtual or physical appliances.  Nice
    way for L4-7 insertion.<br /><br />This kind of controller to controller integration
    will be paramount for dynamic gateway and L4-7 insertion.  No reason to openflow
    enable everything, just integrate the head ends on each side.
  id: '7161241714911513030'
  image: https://resources.blogblog.com/img/blank.gif
  name: '@jedelman8'
  profile: null
  pub: '2012-04-18T04:48:06.517+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 07:01
  html: 20 - 40Gbps throughput on a single CPU? That is just not possible. The highest
    performance single socket Sandy Bridge can do only about 25Gbps with just packet
    in/out without any thing useful running, on bare metal. If this throughput is
    on multiple CPUs, why stop at 20 -40Gbps, shouldn&#39;t this be able to scale
    out -- like indefintely (at least for a while)?
  id: '122820706594874115'
  image: https://resources.blogblog.com/img/blank.gif
  name: old timer
  profile: null
  pub: '2012-04-18T07:01:49.047+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 07:47
  html: Obviously they need multiple cores to reach that throughput. As for &quot;indefinite
    scaling&quot;, you hit a number of other limitations (listen to one of the recent
    Packet Pushers podcasts on server architectures). At a certain point, it makes
    more sense to deploy a second box than to increase the performance of a single
    server.
  id: '4176009441934598856'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-04-18T07:47:32.960+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 07:53
  html: 82599 can do TCP segmentation (including Receive Side Coalescing) and checksum
    offload. Not a full TCP offload, but the time-consuming functionality is implemented
    in hardware.
  id: '7357316650763422391'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-04-18T07:53:02.954+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 08:15
  html: 'Service insertion: we need a shiny new protocol because the old ones (like
    WCCP) wouldn&#39;t ever work and because things like MPLS warp space-time continuum.
    Makes me sick.'
  id: '5328601688855037244'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2012-04-18T08:15:18.909+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 13:16
  html: Unfortunately, AES-NI can only help with the symmetric encryption. The hard
    part (and CPU-blasting part) of an SSL connection is the asymmetric part, which
    is required for each new SSL connection, and AES-NI doesn&#39;t help with asymmetric
    encryption.
  id: '5291208025271976895'
  image: https://resources.blogblog.com/img/blank.gif
  name: tbourke
  profile: null
  pub: '2012-04-18T13:16:25.773+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 18 April 2012 15:46
  html: Maybe the real money is simply in a clean and easy UI and use &quot;old&quot;
    protocols such as MPLS. :)
  id: '3323801680507013059'
  image: https://resources.blogblog.com/img/blank.gif
  name: '@jedelman8'
  profile: null
  pub: '2012-04-18T15:46:01.673+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 19 April 2012 05:38
  html: I probably didn&#39;t make myself clear. Single CPU implies the multi-core
    CPU but single socket. To achieve a 20-40Gbps throughput on a sinlge socket with
    service is not possible. If you run on multiple sockets, these multiple sockets
    don&#39;t have to reside on the same physical machine, they are mostly likely
    independant machines connected by the network. If these software only solutions
    were to have real value and beauty, they should be able to run on multiple machines,
    to truly scale out -- like indefintely :-)<br /><br />BTW, How many companies
    are doing this kind of stuff? There is Zeus from Riverbed, Embrane (spell it right?)
    and this LineRate. Any more?
  id: '4922847123569442311'
  image: https://resources.blogblog.com/img/blank.gif
  name: old timer
  profile: null
  pub: '2012-04-19T05:38:32.184+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 20 April 2012 04:03
  html: Those throughputs are very much possible on a single CPU.  New software architectures
    coupled with Sandy Bridge EN hardware allow for orders of magnitude increases
    in performance.  On those systems we are seeing 11+Mpps per core (not CPU) for
    L3 forwarding even with a full LPM on every packet.  Linear scalability is limited
    by other factors in the system, but it&#39;s much higher than 40Gbps.  Packets
    rates decrease as complexity of the services increases, however it is still in
    the Mpps/Core range.  Even more interesting is packet latency.  Compared to a
    standard Linux or BSD stack, we see min/avg/max something like 6/12/200 microseconds.  Equivalent
    max times in Linux are sometimes 500ms under load.
  id: '4795767701926794223'
  image: https://resources.blogblog.com/img/blank.gif
  name: Robert Bays
  profile: null
  pub: '2012-04-20T04:03:49.515+02:00'
  ref: '8850714851314952199'
  type: comment
- date: 07 November 2012 01:05
  html: I confirm the values and a single CPU can do much more. With our networking
    acceleration platform, we get 11Mpps/ core wih linear scalability when you add
    cores.<br />On a dual sandy bridge CPU, we demonstrate 162Mpps. Performance independent
    from packet size.<br />
  id: '7739983206967490989'
  image: https://resources.blogblog.com/img/blank.gif
  name: David Le Goff
  profile: http://www.6wind.com
  pub: '2012-11-07T01:05:30.491+01:00'
  ref: '8850714851314952199'
  type: comment
count: 17
id: '8850714851314952199'
type: post
url: 2012/04/linerate-proxy-software-l4-7-appliance.html
