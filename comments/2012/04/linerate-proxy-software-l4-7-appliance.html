<div class="comments post" id="comments">
  <h4>17 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1041307680405047697">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Brad Hedlund</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1041307680405047697" href="#1041307680405047697">17 April 2012 14:40</a>
              </span>
            </div>
            <div class="comment-content">Would be really cool if they ran the open vswitch on the southbound interfaces, and partnered with Nicira and/or Big Switch, so that the appliance could be used as a gateway in Overlay based clouds such as, um, Rackspace.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7969063468927118039">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">ccie15672</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7969063468927118039" href="#7969063468927118039">17 April 2012 14:47</a>
              </span>
            </div>
            <div class="comment-content">Hear, hear!</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8855111238263575624">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8855111238263575624" href="#8855111238263575624">17 April 2012 14:52</a>
              </span>
            </div>
            <div class="comment-content">Yeah, that would be cool, but I wonder what their TCP stack performance would be with extra MAC-over-IP encapsulation not supported by NIC HW, so it would be either STT or lower performance (assuming they rely on TCP offload to get their numbers, which remained a gray area during my briefing).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2941513697533320173">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">tbourke</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2941513697533320173" href="#2941513697533320173">17 April 2012 18:33</a>
              </span>
            </div>
            <div class="comment-content">I&#39;m very wary about the SSL performance as that&#39;s been one aspect that we&#39;ve relied heavily on SSL ASICs for many years. With the migration to 2048 bit certs, which eats up about 5x as many CPU cycles as 1024 bit keys, I&#39;m going to need proof that generic x86 CPU cycles can scale out with 2048-bit SSL keys.<br /><br />Each new SSL connection will require an asymmetric operating to occur, and that&#39;s the part that has blasted CPUs before. Putting that onto an ASIC has helped greatly over the years. Most load balancing vendors have that, usually as a PCI card from Cavium. <br /><br />Admittedly though, it&#39;s been a while since I know anyone that&#39;s tested SSL performance with modern hard are. I&#39;m willing to challenge the &quot;need ASIC&quot; assumption, but I&#39;m going to want proof.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7728221217283881283">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7728221217283881283" href="#7728221217283881283">17 April 2012 20:20</a>
              </span>
            </div>
            <div class="comment-content">It&#39;d be cool if SSL is using AES-NI engine which can offload AES instruction to host CPU (even from VM) and configure the load balance to favor AES encryption.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8560987688118677908">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">ccie15672</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8560987688118677908" href="#8560987688118677908">17 April 2012 22:47</a>
              </span>
            </div>
            <div class="comment-content">I wonder if a GPU could do this job...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4403334818494089331">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Manish Vachharajani</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4403334818494089331" href="#4403334818494089331">17 April 2012 22:53</a>
              </span>
            </div>
            <div class="comment-content">This is Manish at LineRate.  We don&#39;t use any special TCP offload hardware.  We typically measure peak performance using Intel&#39;s 82599 NICs, so pretty standard stuff.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7161241714911513030">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">@jedelman8</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7161241714911513030" href="#7161241714911513030">18 April 2012 04:48</a>
              </span>
            </div>
            <div class="comment-content">Brad, I spoke with Radware earlier at their booth at ONS and think they are sort of accomplishing what you are referring to here.  They have and always had their Service Delivery Controller, managing their own ADCs, but now are integrating it with an OpenFlow controller that is then in turn controlling the OF switches in the environment.  With this integration between the OF and Radware controllers, they are able to map tenants to specific virtual or physical appliances.  Nice way for L4-7 insertion.<br /><br />This kind of controller to controller integration will be paramount for dynamic gateway and L4-7 insertion.  No reason to openflow enable everything, just integrate the head ends on each side.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="122820706594874115">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">old timer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c122820706594874115" href="#122820706594874115">18 April 2012 07:01</a>
              </span>
            </div>
            <div class="comment-content">20 - 40Gbps throughput on a single CPU? That is just not possible. The highest performance single socket Sandy Bridge can do only about 25Gbps with just packet in/out without any thing useful running, on bare metal. If this throughput is on multiple CPUs, why stop at 20 -40Gbps, shouldn&#39;t this be able to scale out -- like indefintely (at least for a while)?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4176009441934598856">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4176009441934598856" href="#4176009441934598856">18 April 2012 07:47</a>
              </span>
            </div>
            <div class="comment-content">Obviously they need multiple cores to reach that throughput. As for &quot;indefinite scaling&quot;, you hit a number of other limitations (listen to one of the recent Packet Pushers podcasts on server architectures). At a certain point, it makes more sense to deploy a second box than to increase the performance of a single server.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7357316650763422391">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7357316650763422391" href="#7357316650763422391">18 April 2012 07:53</a>
              </span>
            </div>
            <div class="comment-content">82599 can do TCP segmentation (including Receive Side Coalescing) and checksum offload. Not a full TCP offload, but the time-consuming functionality is implemented in hardware.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5328601688855037244">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5328601688855037244" href="#5328601688855037244">18 April 2012 08:15</a>
              </span>
            </div>
            <div class="comment-content">Service insertion: we need a shiny new protocol because the old ones (like WCCP) wouldn&#39;t ever work and because things like MPLS warp space-time continuum. Makes me sick.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5291208025271976895">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">tbourke</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5291208025271976895" href="#5291208025271976895">18 April 2012 13:16</a>
              </span>
            </div>
            <div class="comment-content">Unfortunately, AES-NI can only help with the symmetric encryption. The hard part (and CPU-blasting part) of an SSL connection is the asymmetric part, which is required for each new SSL connection, and AES-NI doesn&#39;t help with asymmetric encryption.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3323801680507013059">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">@jedelman8</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3323801680507013059" href="#3323801680507013059">18 April 2012 15:46</a>
              </span>
            </div>
            <div class="comment-content">Maybe the real money is simply in a clean and easy UI and use &quot;old&quot; protocols such as MPLS. :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4922847123569442311">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">old timer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4922847123569442311" href="#4922847123569442311">19 April 2012 05:38</a>
              </span>
            </div>
            <div class="comment-content">I probably didn&#39;t make myself clear. Single CPU implies the multi-core CPU but single socket. To achieve a 20-40Gbps throughput on a sinlge socket with service is not possible. If you run on multiple sockets, these multiple sockets don&#39;t have to reside on the same physical machine, they are mostly likely independant machines connected by the network. If these software only solutions were to have real value and beauty, they should be able to run on multiple machines, to truly scale out -- like indefintely :-)<br /><br />BTW, How many companies are doing this kind of stuff? There is Zeus from Riverbed, Embrane (spell it right?) and this LineRate. Any more?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4795767701926794223">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Robert Bays</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4795767701926794223" href="#4795767701926794223">20 April 2012 04:03</a>
              </span>
            </div>
            <div class="comment-content">Those throughputs are very much possible on a single CPU.  New software architectures coupled with Sandy Bridge EN hardware allow for orders of magnitude increases in performance.  On those systems we are seeing 11+Mpps per core (not CPU) for L3 forwarding even with a full LPM on every packet.  Linear scalability is limited by other factors in the system, but it&#39;s much higher than 40Gbps.  Packets rates decrease as complexity of the services increases, however it is still in the Mpps/Core range.  Even more interesting is packet latency.  Compared to a standard Linux or BSD stack, we see min/avg/max something like 6/12/200 microseconds.  Equivalent max times in Linux are sometimes 500ms under load.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7739983206967490989">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://www.6wind.com" rel="nofollow">David Le Goff</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7739983206967490989" href="#7739983206967490989">07 November 2012 01:05</a>
              </span>
            </div>
            <div class="comment-content">I confirm the values and a single CPU can do much more. With our networking acceleration platform, we get 11Mpps/ core wih linear scalability when you add cores.<br />On a dual sandy bridge CPU, we demonstrate 162Mpps. Performance independent from packet size.<br /></div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
