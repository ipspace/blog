{
   "comments": [
      {
         "date": "07 May 2012 14:03",
         "html": "In a previous company, we went with TOR routing, even with a L2 extended everywhere: because of the many vlans used.<br />For firewall: iptables on every host, acls and monitoring.<br /><br />You should go with ToR from the beginning, if you can afford it.",
         "id": "2745335930893253430",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Xavier Nicollet",
         "profile": null,
         "pub": "2012-05-07T14:03:43.932+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 15:04",
         "html": "&quot;Does it Matter?&quot; Not really. In a real cloud L3 hops are largely in appliances and x86 machines.<br />Nice post Ivan.",
         "id": "6726479723303281572",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Brad Hedlund",
         "profile": null,
         "pub": "2012-05-07T15:04:40.026+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 18:01",
         "html": "I think I&#39;ve here a typical datacenter, with:<br /><br />DataCenter:<br /><br />9 x ToR L2 stackable-switch, with 2x10Gbps aggregation to the 2 Core Switches and each ToR has 2 units with 48x1Gbps ports;<br /><br />2 x L3 Core DataCenter Switchs, with 12 VLANs, and L3 between &quot;Core DataCenter Switchs&quot; and &quot;Core Users Switchs&quot;<br /><br />On Core Users Switchs, we have 95 VLANs.<br /><br />Don&#39;t think in L3 on ToR, yet.",
         "id": "4066994000248863481",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Forrequi",
         "profile": null,
         "pub": "2012-05-07T18:01:52.639+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 19:45",
         "html": "IMHO, it is less about optimal layer 3 forwarding and more about simplifying your network. I truly large network needs to be automated, and not with points and clicks. I feel bad for all the people that need to support vMotion on their network. As the South Park meme states, &quot;You&#39;re gonna have a bad time&quot; if you try to scale it out. http://qkme.me/3p631g<br /><br />L3 TOR will typically provide much better flow hashing than glbp/hsrp/vrrp. You get per flow hashing and not per host hashing. It is also easier to go more than 2 wide above your TORs with L3 vs L2. Troubleshooting L3 is much saner than trying to troubleshoot evil protocols like virtual port-channels.<br /><br />Optimal forwarding is nice. I see L3 as orthogonal  to firewalls and load balancers, at least at scale.<br /><br />If you are talking about a &quot;datacenter&quot; with a few dozen racks where all the config is hand managed, it probably doesn&#39;t matter. If you&#39;re network is small enough that a single engineer can hand manage it, stunt networking with vPC and vlans everywhere won&#39;t be an issue. Until you try grow it out by an order of magnitude. Then you&#39;ll be screwed.<br /><br />IMHO",
         "id": "1188557878640954733",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Peter",
         "profile": null,
         "pub": "2012-05-07T19:45:49.499+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 20:15",
         "html": "I think that my network is small, compared with some large scales datacenter in Google Apple era. My ARP table on Core DataCenter switch has 1100 lines. My ARP table on Core Users switch has 4838 lines.",
         "id": "4869439520051898210",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Forrequi",
         "profile": null,
         "pub": "2012-05-07T20:15:39.203+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 21:14",
         "html": "Sure it does.<br /><br />Strictly M1 F1 card speaking - Nexus 7000 switches require an M1 card to route traffic.  Each M1 card can route 80Gb throughput.  If i have one M1 card and 9 F1 cards and all servers are on their own subnet (hello VSheild Edge or VSG) then they need to get routed.  Routing over-subscription can reach 1 to 36.<br /><br />M2/F2 series run into same issues with lower OSR.<br /><br />If you want your 7K pair to never be oversubscribed then you need to plan around routing.<br /><br />That&#39;s why I try to design L2 domains that will rarely route to other L2 domains in a data center if i can prevent it.",
         "id": "7631016462639242277",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Rob",
         "profile": null,
         "pub": "2012-05-07T21:14:40.718+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 23:06",
         "html": "If you want your 7k pair to never be oversubscribed, only use M108s. Done. I am spoiled in that I don&#39;t need to support bridging a broadcast domain across multiple racks. If I need multiple security domains, I prefer having different physical routers. It is way easier to scale wide with simple blocks than trying to support stunt networking.<br /><br />If you can connect all your racks to a single pair of routers, it probably isn&#39;t a big deal.<br /><br />IMHO",
         "id": "4072708181682796740",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Peter",
         "profile": null,
         "pub": "2012-05-07T23:06:58.192+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 23:07",
         "html": "If you want your 7k pair to never be oversubscribed, only use M108s. Done. I am spoiled in that I don&#39;t need to support bridging a broadcast domain across multiple racks. If I need multiple security domains, I prefer having different physical routers. It is way easier to scale wide with simple blocks than trying to support stunt networking.<br /><br />If you can connect all your racks to a single pair of routers, it probably isn&#39;t a big deal.<br /><br />IMHO",
         "id": "7478826087827808556",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Peter",
         "profile": null,
         "pub": "2012-05-07T23:07:18.437+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "07 May 2012 23:10",
         "html": "This is an interesting discussion that we have thought  about recently when we decided to build a few data centers to consolidate many DCs handling 15K apps. VM and HP&#39;s equivalent were to support  80% of these apps. <br /><br />2 DCs are paired by DWDM and short distances to be able to accommodate capacities in excess of 100 Gbps and low latencies for whatever FC or Ethernet based replications people want to do. <br /><br />At each DC we have a lan core of 2xN7Ks, end of row of 2xN5Ks for every row, and the N2Ks FECs for the top of rack which connect to blade/chassis based switches. So, many port channels and many trunks everywhere.<br /><br />Naturally, the biggest point of disagreement between the server/application teams and the network team was the border between L2 and L3. Ultimately, we brought L2 back to the lan core. To the other DC, rest of the Wan, load balancing infrastructure, or services on the other side of firewalls, we route at L3. <br /><br />Suboptimal forwarding performance, within the L2/L3 switched lan environment, is less a concern to us, than the performance limitations we have when traversing our firewalls, IPSs, or Wan. Even when packets or frames need to come back to the core, we are probably looking microseconds of additional delay versus milliseconds of delay. <br /><br />Another important consideration for us, is routing into and out of the pair of data centers requiring symmetry for wan optimization and security. This forced a certain level of suboptimal routing into the design.<br /><br />Another reader mentioned simplicity of design. In my opinion, this is even more critical than sub-optimal routing to the performance of a data center within the data center. If only high level engineers can troubleshoot and resolve issues, due to the complexities of the data center, then the time to resolution increases substantially as there are generally fewer of these engineers around. <br /><br />The only caveat is that the uplinks and hardware has the capacity to be able to forward the traffic.",
         "id": "8044632969380060237",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Tony S",
         "profile": null,
         "pub": "2012-05-07T23:10:26.061+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "15 May 2012 06:53",
               "html": "Yeah, I suppose you can do vMotion across different subnets if you can make DNS act in milli-second level and figure out a way to flush client DNS entry also in milli-seconds.",
               "id": "2723480841969435524",
               "image": "https://resources.blogblog.com/img/blank.gif",
               "name": "Anonymous",
               "profile": null,
               "pub": "2012-05-15T06:53:44.763+02:00",
               "ref": "5682660892037012750",
               "type": "comment"
            },
            {
               "date": "15 May 2012 06:56",
               "html": "Even that won&#39;t work, TCP connection will break when IP address changes.",
               "id": "4455848832975601716",
               "image": "https://resources.blogblog.com/img/blank.gif",
               "name": "Anonymous",
               "profile": null,
               "pub": "2012-05-15T06:56:26.536+02:00",
               "ref": "5682660892037012750",
               "type": "comment"
            },
            {
               "date": "15 May 2012 08:08",
               "html": "There&#39;s a mechanism that works - host routes. There&#39;s not much difference between having /32 prefix in IP table and MAC address in MAC table ... apart from the fact that one of them is predictable (based on routing protocols) and the other is guesswork (based on traffic gleaning).<br /><br />However, routing protocols require actual configuration, some knowledge, and might take a few seconds to converge.",
               "id": "1961522221337213645",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Ivan Pepelnjak",
               "profile": "https://www.blogger.com/profile/13457151406311272386",
               "pub": "2012-05-15T08:08:54.880+02:00",
               "ref": "5682660892037012750",
               "type": "comment"
            },
            {
               "date": "16 May 2012 03:58",
               "html": "I don&#39;t understand your argument, how host route is related to vMotion at all? could you please elaborate?",
               "id": "3343743829913554125",
               "image": "https://resources.blogblog.com/img/blank.gif",
               "name": "Anonymous",
               "profile": null,
               "pub": "2012-05-16T03:58:26.346+02:00",
               "ref": "5682660892037012750",
               "type": "comment"
            },
            {
               "date": "05 July 2012 20:44",
               "html": "The problem with vMotion between subnets is that the IP changes (moving from one /24 to another, your IP *must* change in order for the routed infrastructure to find the new location).  However, if the IP of the VM guest is announced to the network (maybe by the hypervisor) as a /32, then the guest can migrate to virtually anywhere on the network and the new hypervisor simply announces the /32, the old one stops announcing, and the guest has been migrated (ok, so it&#39;s maybe not all that &quot;simple&quot; but it&#39;s possible).",
               "id": "2291226955108566593",
               "image": "https://resources.blogblog.com/img/blank.gif",
               "name": "Anonymous",
               "profile": null,
               "pub": "2012-07-05T20:44:10.578+02:00",
               "ref": "5682660892037012750",
               "type": "comment"
            }
         ],
         "date": "07 May 2012 23:50",
         "html": "It seems people go with large L2 domain because vMotion requires the same VLAN domain for moved to host and moved from host. Is there something fundamental that prevents vMotion to work across different but reachable subnets?",
         "id": "5682660892037012750",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Sherry Wei",
         "profile": null,
         "pub": "2012-05-07T23:50:57.255+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "09 May 2012 05:33",
               "html": "There&#39;s a few protocols that don&#39;t like a host to switch IPs and subnets. Namely all of them. Remember vmotion moves a live running server between hosts.",
               "id": "7171215554019569627",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "RPM",
               "profile": "https://www.blogger.com/profile/12675510409950425811",
               "pub": "2012-05-09T05:33:28.203+02:00",
               "ref": "3521775083751406468",
               "type": "comment"
            }
         ],
         "date": "07 May 2012 23:51",
         "html": "It seems people go with large L2 domain because vMotion requires the same VLAN domain for moved to host and moved from host. Is there something fundamental that prevents vMotion to work across different but reachable subnets?",
         "id": "3521775083751406468",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Sherry Wei",
         "profile": null,
         "pub": "2012-05-07T23:51:08.427+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "16 May 2012 23:41",
         "html": "I hate to give this answer, but: It depends.<br /><br />Given the typical tools of VLAN and VRF, many datacenters would find that VRF support is lacking and/or burdensome to support in access and aggregation tiers. So VLANs make more sense, bridging at the access/agg with routing happening in the &quot;core&quot; or WAN edge. Of course, this also gives a larger broadcast domain and might facilitate mobility, plug and play behaviors, etc.<br /><br />As to traffic patterns: If we&#39;re using VRFs for  the routing edge, then maybe it doesn&#39;t matter - if the VRFs are supporting different VPNs then inter-VLAN traffic isn&#39;t likely anyway. On the other hand, if each VRF is routing (e.g. locally) between multiple VLANs then you get a traffic trombone. It&#39;s not very efficient of course, in terms of capacity etc, but even a &quot;traffic trombone&quot; is possibly a good tradeoff in terms of centralizing the VRF capability and configuration. And as you point out, we may have this traffic pattern anyway because of e.g. the location of the security edge (firewall) or load-balancer.<br /><br />In other environments, VRF support may not be necessary for L3, e.g. because we&#39;re attaching everything to &quot;the Internet&quot; or some other common network. In that case, it might make more sense to route in the access and/or aggregation layer. Plain L3 routing is more likely to be supported in various equipment, and it&#39;s somewhat easier to manage.<br /><br />And we can always run both L2 and L3 throughout the datacenter - L3 on the access node for routing to the Internet and L2 for carrying traffic to centralized VRF PEs, or something like that. You can probably imagine other ways to combine these; the point is that each design will be judged based on the purpose / goal of the datacenter, so there&#39;s probably no &quot;one size fits all&quot; answer here.<br /><br />Of course, all of this might become moot with overlays and SDN architectures. At some point, the underlying network can be all-L3 and the &quot;service&quot; can be a mix of L2 and L3. Depending on how much control is built into the control plane, we might be able to do interesting policy and non-traditional forwarding... But that&#39;s a topic for another day. :)",
         "id": "7927818369699456218",
         "image": "https://resources.blogblog.com/img/blank.gif",
         "name": "Benson Schliesser",
         "profile": "http://www.queuefull.net/~bensons/",
         "pub": "2012-05-16T23:41:34.787+02:00",
         "ref": "7618880673252151744",
         "type": "comment"
      },
      {
         "date": "15 February 2025 05:03",
         "html": "<p>13 years later, some things haven&#39;t changed much</p>\n\n<p>I recently joined a new company, and I&#39;m told that while designs started out with L3 at ToRs, things have standardized on L2-only ToRs with routing at the core switches (VRRP pair per VLAN). The reason is because of what invariably happened: Applications would grow out of their initial /24 subnet, and the team would have to stretch the original /24 VLAN - which had now gotten too small - to a new pair of Tor switches.</p>\n\n<p>It&#39;s not about &quot;optimal L3 forwarding&quot; - it&#39;s about configurations that don&#39;t need to change</p>\n",
         "id": "2528",
         "name": " Jeroen van Bemmel",
         "pub": "2025-02-15T17:03:11",
         "type": "comment"
      }
   ],
   "count": 13,
   "id": "7618880673252151744",
   "type": "post",
   "url": "2012/05/does-optimal-l3-forwarding-matter-in.html"
}
