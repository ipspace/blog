comments:
- comments:
  - date: 28 May 2012 08:22
    html: Sure they do and a lot of people bet such features always work flawlessly
      ;)
    id: '6635134934681662641'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-05-28T08:22:59.748+02:00'
    ref: '1193851848527917209'
    type: comment
  - date: 28 May 2012 12:49
    html: Everything is a compromise - you don&#39;t have to solve the problem to
      solve the problem. You only need to get it to where the risk is acceptable.<br
      /><br />Sometimes, when your faith in your Vendor&#39;s new features is somewhat
      shaky, that can mean &quot;just run spanning tree and be cautious&quot;. ;)
    id: '3748241874493946305'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2012-05-28T12:49:58.056+02:00'
    ref: '1193851848527917209'
    type: comment
  date: 28 May 2012 07:59
  html: '...and, because it&#39;s a widely recognized problem, work is being done
    on putting up some kludges to make it better. Some examples from ALU land:<br
    /><br />STP Loop guard: http://www.alcatelunleashed.com/viewtopic.php?f=190&amp;t=18462&amp;start=20#p66200<br
    /><br />MAC Move: http://lucent-info.com/html/93-0107-08-04-H/7450%20Services%20Guide/wwhelp/wwhimpl/common/html/wwhelp.htm#href=services_con_vpls.12.20.html&amp;single=true<br
    /><br />I bet other Vendors have something similar. ;)'
  id: '1193851848527917209'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2012-05-28T07:59:03.935+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 28 May 2012 15:22
    html: IGMP snooping (actually, MLD) reduces the flooding scope for multicast destination
      MAC addresses. Broadcast and unknown unicast flooding is not affected.
    id: '283886526857555668'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-05-28T15:22:13.238+02:00'
    ref: '1654008037151910281'
    type: comment
  - date: 11 June 2012 12:00
    html: Yes, but my point was does an IPv6 only network with MLD need broadcast
      and unknown unicast flooding enabled at all? Or could those functions be disabled?
    id: '6482434627160671206'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Sam Stickland
    profile: https://www.blogger.com/profile/16576239659380574562
    pub: '2012-06-11T12:00:03.257+02:00'
    ref: '1654008037151910281'
    type: comment
  - date: 11 June 2012 12:26
    html: There might still be applications using broadcast. Assuming those eventually
      become extinct and you&#39;re running IPv6-only network, you could still experience
      TCAM overflows that would then require unicast flooding to ensure end-station
      reachability. Also, you&#39;d have to ensure the IPv6 ND timeouts are lower
      than the MAC aging timeouts ... but in theory, it could be done.
    id: '7649848775288230625'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-06-11T12:26:06.234+02:00'
    ref: '1654008037151910281'
    type: comment
  date: 28 May 2012 10:21
  html: I wonder if we&#39;ll ever be able to modify Ethernet to make hosts register
    their MAC/IP entries and just remove broadcast.<br /><br />http://www.cs.cmu.edu/~acm/papers/myers-hotnetsIII.pdf<br
    /><br />Does IPv6 with IGMP snooping switches require flooding or could it be
    removed?
  id: '1654008037151910281'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Sam Stickland
  profile: https://www.blogger.com/profile/16576239659380574562
  pub: '2012-05-28T10:21:37.147+02:00'
  ref: '1616532859803080648'
  type: comment
- date: 28 May 2012 17:27
  html: Sounds familiar ;-) <br /><br />That picture is right up there with the swiss
    army knife!
  id: '7357087412911131530'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Ian Castleman
  profile: https://www.blogger.com/profile/04661928787006765039
  pub: '2012-05-28T17:27:02.839+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 29 May 2012 19:19
    html: The answer is there is no way to overcome the hurdle.  Create several interfaces
      on the server each interface on its own vlan.
    id: '2166101525791839740'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2012-05-29T19:19:24.534+02:00'
    ref: '6748464688103977494'
    type: comment
  date: 28 May 2012 17:57
  html: Ivan,<br /><br />As usual, quality post but regarding your statement, &quot;...then
    you simple have to split it up into multiple layer-2 domains connected through
    layer-3 switches&quot;, would you care to elaborate?  I work in datacenters but
    on the facilities side and I have been burned many times by industrial devices
    that have poor/limited tcp/ip stack or in some cases, devices not able to route
    back to their server leaving me with having to span layer two across a couple
    switches.  I have implemented storm control but as you mentioned, that may not
    be enough to stop a meltdown.  I&#39;m curious how I can overcome that hurdle
    while maintaining your recommendation about splitting the layer-2 domain through
    layer-3 switches.
  id: '6748464688103977494'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2012-05-28T17:57:04.766+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 28 May 2012 22:25
    html: My thoughts exactly ... :)
    id: '1574875269892286373'
    image: https://resources.blogblog.com/img/blank.gif
    name: Ofer
    profile: null
    pub: '2012-05-28T22:25:52.982+02:00'
    ref: '1957668514226599600'
    type: comment
  - date: 29 May 2012 08:12
    html: L2 network is still a single failure domain, even if it&#39;s wrapped in
      IP (that&#39;s why using VXLAN or NVGRE for long-distance stretched clusters
      makes no sense), but at least the underlying transport is not.<br /><br />As
      for &quot;fixing the flooding behavior&quot;, Nicira got pretty far (VXLAN and
      NVGRE have just inserted another layer of abstraction and resurrected IP MC)
      and can do either headend replication or replication in dedicated nodes. <br
      /><br />The only one that decided to go all the way and kill flooding was Amazon;
      everyone else is too concerned about precious enterprise craplications that
      rely on L2 flooding in one stupid way or another.
    id: '9089731848125618367'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-05-29T08:12:57.532+02:00'
    ref: '1957668514226599600'
    type: comment
  - date: 29 May 2012 09:27
    html: I can see how VXLAN/NVGRE may *narrow* flooding, but can you really kill
      it? e.g. VM instantiation still involves G-ARP AFAIK...
    id: '990911636418079558'
    image: https://resources.blogblog.com/img/blank.gif
    name: Ofer
    profile: null
    pub: '2012-05-29T09:27:04.117+02:00'
    ref: '1957668514226599600'
    type: comment
  - date: 29 May 2012 14:15
    html: VXLAN or NVGRE cannot kill flooding because they have no control plane (although
      Dell did announce an ARP helper appliance @ Interop, so who knows what will
      happen to NVGRE).<br /><br />Nicira&#39;s NVP is a totally different story.
      They might not be totally there yet, but the architecture does allow that.
    id: '5646580510760879723'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-05-29T14:15:58.009+02:00'
    ref: '1957668514226599600'
    type: comment
  date: 28 May 2012 19:50
  html: Well, I don&#39;t quite get something. So we state that L2 network is a single
    failure domain. Alright. But now we get the same L2 network wrapped and tunneled
    over IP, and it&#39;s no longer a single failure domain? :) Have we magically
    agreed on fixing the flooding behavior, which is the actual root cause of L2 scalability
    limitation, in any of these standards?
  id: '1957668514226599600'
  image: https://resources.blogblog.com/img/blank.gif
  name: Petr Lapukhov
  profile: http://bing.com
  pub: '2012-05-28T19:50:33.440+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 28 May 2012 20:44
    html: The difference is that routing control plane is, well, more controlled with
      regards to information flooding :)  So in theory, you could reduce the disruption
      risks, if designed and operated properly. <br /><br />This being said, control
      plain failure examples are always epic. Especially at the Internet scale :)
    id: '5332980133630756741'
    image: https://resources.blogblog.com/img/blank.gif
    name: Petr Lapukhov
    profile: http://bing.com
    pub: '2012-05-28T20:44:17.751+02:00'
    ref: '6872176489045036587'
    type: comment
  date: 28 May 2012 19:58
  html: As a reminder, routing is not a magic solution. IGP is also single fault domain,
    just wait until you hit bug in IS-IS implementation or have bad luck with packet
    corruption. Been there, done that.
  id: '6872176489045036587'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2012-05-28T19:58:26.605+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 30 May 2012 15:04
    html: Would A cheat to win solution to the cable guy problem be to leave unused
      ports disabled?
    id: '8902480159303701581'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2012-05-30T15:04:00.026+02:00'
    ref: '2273933611413130659'
    type: comment
  - date: 07 June 2012 16:30
    html: Will,<br /><br />Perhaps if you use different core switch pairs for each
      L2 domain, you&#39;ll be able to avoid fate-sharing in this case. It&#39;s more
      expensive, and not as fancy as VDCs, but it should meet your requirements.<br
      /><br />I personally don&#39;t have the budget to implement multiple N7K pairs
      in my DC, so I can&#39;t take this advice either :(<br /><br />Jeremy
    id: '5471141425059842562'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Jeremy Filliben
    profile: https://www.blogger.com/profile/07558728700926286196
    pub: '2012-06-07T16:30:22.703+02:00'
    ref: '2273933611413130659'
    type: comment
  date: 29 May 2012 06:57
  html: I&#39;m not sure I agree anymore.  I run a single tenant data center for a
    huge company (60K plus users)<br /><br />I can have a huge network that is broken
    up into a huge amount of small L2 broadcast domains all connected to the same
    core switch pair (or aggregate if you&#39;re a Cisco guy)<br /><br />If one of
    those tiny L2 broadcast domains loop, then your core switches lock up, and your
    whole network goes down.  &quot;single circulating broadcast packet (and its infinite
    copies) will trigger storm control on ALL SWITCHES, and prevent other valid broadcasts&quot;<br
    /><br />I&#39;ve tested various loops scenarios in a large scale network (300+
    TOR switches and a pair of Cisco 7Ks).  I&#39;ve found storm control doesn&#39;t
    work in 10Gb networks with Cisco FEX.  I&#39;ve found port-security does work
    well although increases trouble ticets(opex).  I&#39;ve found that default COPP
    works awesomely in keeping your Nexus 7Ks alive so you can find the loop. I&#39;ve
    found my best bet is to configure the network to prevent loops and not try and
    configure around loops.  And screw the ideals of preventing loops by telling your
    cabling crew to cable properly!!  That will work for 6 months or a year until
    they forget again.<br /><br />So who cares how big your L2 domains are?  And if
    you have the same aggregate switch pair (everyone does) then it doesnt matter
    how many load balanceers or firewall instances you have. I&#39;d say your chances
    are equally the same taking out your data center.  In fact....if you cable differently
    for smaller L2 domains then I&#39;d say your chances go up!  But You do lose mobility
    and scalability the smaller you make your L2 domains.<br /><br />I dont even want
    to talk about mac-in-mac or mac-in-multicast.  No one is there yet.<br /><br />I
    dont even want to talk about STP replacement.  No one is there yet.<br /><br />I&#39;d
    perhaps consider multiple aggregate layer switches (if the 7Ks had the capacity
    for more than four VDC).  That limitation makes VDC useless except for your Development
    instance.<br /><br />Ivan, btw, will you be in San Diego this month?
  id: '2273933611413130659'
  image: https://resources.blogblog.com/img/blank.gif
  name: Will
  profile: null
  pub: '2012-05-29T06:57:21.807+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 19 June 2012 01:19
    html: If I understood this right, it is not about Broadcast alone.<br /><br />Its
      about errors which may occur (software or manual or just race conditions), that
      can cause loops.<br /><br />-Vishwas
    id: '4490554774536154513'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Unknown
    profile: https://www.blogger.com/profile/06975038984318901369
    pub: '2012-06-19T01:19:07.616+02:00'
    ref: '7392665028352334953'
    type: comment
  date: 30 May 2012 14:58
  html: pbb-te / pbt avoid these problems by turning off broadcasts (all, including
    unknown mac flooding) and forcing the switches to use an outboard control plane
    to provision all mac forwarding tables.<br /><br />I&#39;m sure openflow has a
    similar magic unicorn fart type answer to this problem....<br /><br />Now if only
    someone could solve the problem in practice as well as theory.
  id: '7392665028352334953'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2012-05-30T14:58:22.902+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 06 June 2012 06:14
    html: 2) when they take out a data center
    id: '7098900407026887003'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2012-06-06T06:14:45.099+02:00'
    ref: '421886107361628056'
    type: comment
  date: 03 June 2012 01:37
  html: Ivan,<br /><br />Thanks for calling out storm control as the preemptive DoS
    attack that it is.<br /><br />That feature is just a turd in my opinion:<br /><br
    />1) The granularity is terrible - it&#39;s not a throttle, it just counts bytes
    over a 1 second (I think?) interval, and then throws data away for the remainder
    of the second once the threshold has been hit. Awful, unless you set it to 10%
    with the expectation that your server ports will be deaf to some protocols for
    90% of every second...<br /><br />2) Who am I (as the network admin) to decide
    when a server has sent &quot;too many&quot; broadcast or multicast frames? I&#39;ve
    never seen anything along these lines codified in an SLA given to the user/server/application
    community. Accordingly, if a craplousy business application is built to work exclusively
    with broadcast frames, and it runs into storm control, guess what feature is going
    to be switched off?
  id: '421886107361628056'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: chris marget
  profile: https://www.blogger.com/profile/06646973209424821070
  pub: '2012-06-03T01:37:03.434+02:00'
  ref: '1616532859803080648'
  type: comment
- comments:
  - date: 17 July 2012 10:43
    html: Exactly my thinking.<br /><br />TRILL is actually closer to routing than
      to bridging, therefore it&#39;s a L2 protocol that incorporates most of the
      benefits of L3 protocols. TTLs prevent infinite loops, and in Cisco&#39;s FabricPath
      you get conversational MAC learning, which eases the burden of having to learn
      all addresses in your L2 domain.<br /><br />Still, I wonder how is it that SAN
      admins got away with deploying two independent and separate networks, and LAN
      admins did not. It is evident that the first hop L2 connection is a single failure
      domain, no matter how resilient it is. <br /><br />It&#39;s time Operating Systems
      and applications start supporting dual LAN designs so we can effectively protect
      this single failure domain.
    id: '3776314869924927243'
    image: https://resources.blogblog.com/img/blank.gif
    name: Pablo Carlier
    profile: http://stormcontrol.tumblr.com
    pub: '2012-07-17T10:43:36.584+02:00'
    ref: '2182244395018940205'
    type: comment
  - date: 18 July 2012 08:11
    html: '@Pablo: You can have dual LAN design any time you want - just configure
      a loopback interface on the server and run a routing protocol with the network
      ... or configure the load balancer with both server addresses in the server
      pool. <br /><br />There&#39;s no technological reason you can&#39;t do it ...
      apart from the limitations of broken socket API and missing session layer in
      TCP stack ;)'
    id: '7451885924019137893'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-07-18T08:11:01.355+02:00'
    ref: '2182244395018940205'
    type: comment
  date: 19 June 2012 00:52
  html: Ivan, <br /><br />I agree with most of what you say.<br /><br />TRILL does
    have loop prevention by adding a Hop Count, as the last measure of breaking loops.<br
    /><br />Tunneling can cause loops too BTW. This is something that has been raised
    with IPv6. Have a look at:<br />http://tools.ietf.org/html/rfc6324<br /><br />-Vishwas
  id: '2182244395018940205'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/06975038984318901369
  pub: '2012-06-19T00:52:35.902+02:00'
  ref: '1616532859803080648'
  type: comment
count: 27
id: '1616532859803080648'
type: post
url: 2012/05/layer-2-network-is-single-failure.html
