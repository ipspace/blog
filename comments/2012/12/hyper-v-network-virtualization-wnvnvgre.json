{
  "comments": [
    {
      "comments": [
        {
          "date": "14 December 2012 18:53",
          "html": "You&#39;ll have to wait for System Center 2012 SP1 to get the details of the default control plane/orchestration system.<br /><br />However, since Hyper-V Network Virtualization uses PowerShell-based configuration, you can always write your own orchestration system which can be as redundant as you wish.",
          "id": "715557809145026207",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-14T18:53:57.835+01:00",
          "ref": "4132157195863390221",
          "type": "comment"
        },
        {
          "date": "17 December 2012 02:10",
          "html": "High availability is by clustering the control plane (VMM), which is a traditional active/passive configuration.  It&#39;s worth noting that the configuration is partially cached at each host, so you could sustain the loss of the control plane for a while - until something changes I suppose.  At least that&#39;s my understanding of it.<br /><br />Or, as Ivan notes you could build your own...",
          "id": "9013767985588724326",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Stu Fox",
          "profile": "http://stufox.wordpress.com",
          "pub": "2012-12-17T02:10:32.570+01:00",
          "ref": "4132157195863390221",
          "type": "comment"
        }
      ],
      "date": "14 December 2012 18:38",
      "html": "How do we get high availability from the centralized control plane? Is it a traditional active/passive setup, or is the control plane fancier (3+ nodes, paxos fault tolerant, etc.)",
      "id": "4132157195863390221",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "RPM",
      "profile": "https://www.blogger.com/profile/12675510409950425811",
      "pub": "2012-12-14T18:38:53.117+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "14 December 2012 18:52",
          "html": "Could you, please, show me the VXLAN control plane in either Nexus 1000V or vSphere 5.1 vDS? They both rely on MAC flooding  and learning.",
          "id": "9214818491678040298",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-14T18:52:00.612+01:00",
          "ref": "6848425083929897142",
          "type": "comment"
        },
        {
          "date": "15 December 2012 18:06",
          "html": "Totally right Yvan, but everybody know it&#39;s just a question of time for them to integrate a new Nicira NVP like control plane associated with a VxLAN encapsulation ...<br /><br />So SDN VxLAN will become a reality very soon. Sorry Mr John Chambers, looks like a very bad news ! If Cisco is loosing control on virtual switch, they will loose associated control with their ToR switches ... snif ;)",
          "id": "570750753075551135",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Romain Jacotin",
          "profile": "http://fr.linkedin.com/in/romainjacotin",
          "pub": "2012-12-15T18:06:14.491+01:00",
          "ref": "6848425083929897142",
          "type": "comment"
        },
        {
          "date": "15 December 2012 19:03",
          "html": "Let&#39;s wait and see when VXLAN with control plane happens.<br /><br />The last time I was briefed on Nicira&#39;s NVP, they still relied exclusively on L2 forwarding and although they managed to reduce the impact of flooding, they still had to do it.",
          "id": "8536201831084623612",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-15T19:03:34.595+01:00",
          "ref": "6848425083929897142",
          "type": "comment"
        },
        {
          "date": "18 December 2012 16:47",
          "html": "I stand by my previous statement. However, my perspective is limited to multi-tenant cloud environments. But isn&#39;t that where VXLAN is targeted?? Conversely, outside of a multi-tenant cloud environments (where flooding might be a problem) why would you even need or consider VXLAN? What problem does it solve there??<br />  ",
          "id": "2140692663119265929",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Chris Marino",
          "profile": "https://twitter.com/chris_marino",
          "pub": "2012-12-18T16:47:43.089+01:00",
          "ref": "6848425083929897142",
          "type": "comment"
        },
        {
          "date": "01 February 2013 02:06",
          "html": "Ivan<br />With VMware/Nicira NVP, the goal is to recreate the properties of the physical network into a logical, virtual network.  This means a &quot;logical switch&quot; provides the same L2 service model to the virtual machines attached to it, as would a physical switch.  So, yes, this means broadcasts and multicasts are forwarded to where they need to go.  No new caveats and restrictions placed on the application architect.",
          "id": "8842372493160704494",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Brad Hedlund",
          "profile": "http://bradhedlund.com",
          "pub": "2013-02-01T02:06:54.951+01:00",
          "ref": "6848425083929897142",
          "type": "comment"
        },
        {
          "date": "01 February 2013 08:10",
          "html": "Yeah, I know what you&#39;re doing and I know why you have to do it. Still, it&#39;s interesting that a major vendor finally had the guts to say &quot;this is not how things should be done&quot; ;)",
          "id": "6097453414729532662",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2013-02-01T08:10:43.672+01:00",
          "ref": "6848425083929897142",
          "type": "comment"
        }
      ],
      "date": "14 December 2012 18:49",
      "html": "As always, an interesting read. Minor quibble with your scalability ranking, though. I have never once encountered anyone thinking about using VXLAN without a control plane.",
      "id": "6848425083929897142",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Chris Marino",
      "profile": "https://twitter.com/chris_marino",
      "pub": "2012-12-14T18:49:45.019+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "15 December 2012 19:02",
          "html": "Anything can be misused as a DCI technology. For more details, see http://blog.ioshints.info/2012/03/stretched-layer-2-subnets-server.html",
          "id": "6303832786369884084",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-15T19:02:11.128+01:00",
          "ref": "2700871057465430196",
          "type": "comment"
        }
      ],
      "date": "15 December 2012 15:55",
      "html": "Could u say this technology can be used as DCI?",
      "id": "2700871057465430196",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2012-12-15T15:55:57.716+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "16 December 2012 20:39",
          "html": "I wouldn&#39;t, till I see a hypervisor implementation. Right now, you can start LISP in data center core switch, which is &quot;a bit&quot; removed from the hypervisor, and I&#39;m not aware of any orchestration tool that would sync the hypervisor VLANs with LISP in VRFs. Am I missing something?",
          "id": "5597612799463793737",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-16T20:39:22.143+01:00",
          "ref": "7119691356764583317",
          "type": "comment"
        }
      ],
      "date": "16 December 2012 20:07",
      "html": "I would add LISP as well to the list of the major virtual network solutions, which is a layer-3 over UDP solution, using a centralized control plane and 24 bit &quot;Instance ID&quot; as a network identifier. You said &quot;LISP is... whatever you want it to be&quot; :) here: http://blog.ioshints.info/2011/09/vxlan-otv-and-lisp.html<br /><br />Where would you rank it in the scalability toplist?",
      "id": "7119691356764583317",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Lori Jakab",
      "profile": "https://www.blogger.com/profile/03137097658670524449",
      "pub": "2012-12-16T20:07:31.598+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "17 December 2012 12:29",
          "html": "Still missing: hypervisor implementation. Today LISP scales no better than VLANs, because the only way you can get to a LISP VRF is through a VLAN.",
          "id": "2048367766180720895",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-17T12:29:19.433+01:00",
          "ref": "8071653294512460028",
          "type": "comment"
        }
      ],
      "date": "17 December 2012 10:30",
      "html": "I left out the orchestration tool from my definition of virtual networking, focusing just on the technology itself. But I&#39;m still curios, assuming such tool existed, where would you rank the solution?",
      "id": "8071653294512460028",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Lori Jakab",
      "profile": "https://www.blogger.com/profile/03137097658670524449",
      "pub": "2012-12-17T10:30:25.680+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "17 December 2012 15:53",
      "html": "Hey Ivan,<br /><br />I understand that LISP is currently not in any Hypervisor or vSwitch (nor is BGP for that matter, and that is currently the controlplane that most of the NVO3 mailing list is cluttered with ), but i do think it could be usefull to consider it: take a look at http://blogs.cisco.com/getyourbuildon/a-unified-l2l3-ip-based-overlay-for-data-centres-another-use-case-for-the-location-identity-separation-protocol/ for some considerations.<br /><br />Another observation is that todays virtualization vendors often nothing but a centralized SDN style controlplane, touching all Virtual Switches simultaneously.",
      "id": "6031624035529110270",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/08970761764257948861",
      "pub": "2012-12-17T15:53:06.160+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "17 December 2012 18:48",
          "html": "John, I wrote extensively about TRILL (or other large-scale bridging technologies) in IaaS environments. They have the same bright future DLSw+ had a decade ago.<br /><br />Also, we&#39;ve solved the network infrastructure problem in the meantime - every single vendor knows how to build Clos fabrics, and with something-over-IP virtual networking we no longer need fancy core forwarding technologies. ",
          "id": "1415202092497782043",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-17T18:48:58.822+01:00",
          "ref": "3102936288886592349",
          "type": "comment"
        },
        {
          "date": "17 December 2012 20:33",
          "html": "You heard it here from Ivan&#39;s mouth. Advanced Ethernet switching in the core for cloud is dead. We&#39;ll tell all the sales guys to cancel their meetings. <br /><br />Arise the world of the new networking overloads VMware and Microsoft. It&#39;s a bit of a scary thought. <br />",
          "id": "3987776946873141923",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/04982620456797894645",
          "pub": "2012-12-17T20:33:17.932+01:00",
          "ref": "3102936288886592349",
          "type": "comment"
        }
      ],
      "date": "17 December 2012 17:30",
      "html": "So your premise is that Amazon&#39;s solution would scale better because it does not do the MAC in GRE encapsulation, but is strictly IP routing leaving the compute node? What does that say about TRILL and PBB in enterprise who are demanding cloud scale?<br /><br />I found it interesting that the VL2 whitepaper from 2009 (Microsoft + Amazon.. Notice James Hamilton&#39;s name at the top), which was all IP basically told us all what direction they were going 3 years ago.<br /><br />http://research.microsoft.com/apps/pubs/default.aspx?id=80693<br /><br />The trick was the integration into the hypervisor&#39;s networking stack (the proxy ARP and security) needed to be supported. This was doable enough for a single company, Amazon, in their own contained environment.  A Microsoft environment, which is not nearly as contained, but closer than the wild west of roll-your-own cloud (or the larger eco-system of VMware based networking vendors), to me also has a fair shot. Your thoughts about &#39;less deployed&#39; means faster support?<br /><br />Notice the whitepaper&#39;s focus on making the core network CHEAP and easy. I assume this will not be great news for core big-iron switching and routing feature road maps.Would you agree? Broadcom and Intel are looking better and better in this world. Everyone has relatively cheap ECMP flow routing these days.<br /><br />I would think another loser here are the early adopters who bought the solid work product of deep hypervisor network integration with last year&#39;s API. They are likely going to get edged out by &#39;hypervisor vendor lockin features&#39;, like &#39;behind the curtain SDN&#39;.  There will always be nuances outside vendors will have to come to partner with the hypervisor vendor to support. This is step one in a long road, and the owner of the VM guest MAC address will dictate the edge, and the technologies deployed at the edge. Do you agree?",
      "id": "3102936288886592349",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/04982620456797894645",
      "pub": "2012-12-17T17:30:12.463+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "19 December 2012 19:50",
      "html": "Not Vmware in the long run...just MS....heh",
      "id": "7727398296341931466",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2012-12-19T19:50:19.374+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "19 December 2012 22:10",
      "html": "Chanced on this virtual switching blog - if you are in SF bay area, you may want to check out the meetup group - <br /><br />http://www.meetup.com/openvswitch/<br /><br />they seem to have regular network virtualization tech talks by industry leaders as well as technical workshops",
      "id": "6103548129758149960",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2012-12-19T22:10:15.028+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "22 January 2013 17:12",
          "html": "#1 - Calm down.<br /><br />#2 - You might want to be polite and use your real name<br /><br />#3 - Just because people use L2-based HA tricks doesn&#39;t mean those tricks are OK. Anyhow, if you so badly need them, you could still implement them in NVGRE with orchestration tool assistance.<br /><br />#4 - NVGRE kernel module provides IP routing functionality with static routes (including default route), so you can push traffic toward IP next hop.<br /><br />#5 - The only way to get traffic out of a virtualized network today is through a VM-based L3-7 appliance ... like with most other overlay network solutions.<br /><br />#6 - HW vendors are promising HW- or appliance-based NVGRE gateways (in this case, Dell/Force10 and F5, not sure about Arista).",
          "id": "1365896937309559750",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2013-01-22T17:12:10.877+01:00",
          "ref": "6740627190806906357",
          "type": "comment"
        },
        {
          "date": "02 February 2013 00:32",
          "html": "Dear Ivan,<br /><br />Thanks for the reply. I didn&#39;t mean to come off as angry or impolite. I do take issue with your dismissal of industry standard practices and endorsement of a not-fully-functional implementation, so maybe that is the source of my apparent frustration.<br /><br />The issue is that in a VMM 2012SP1 environment, we are forced to only use NV policies VMM allows so #4 is kind of moot. So, without a VMM-integrated gateway, there is no way to have a virtual network interface with a non-virtual one via static routes in the NVGRE rule-set.  No gateways are available as of right now. Some are coming in a few months, others later this year (re: #6). <br /><br />For #3, do you mean that VRRP is not OK? (http://tools.ietf.org/html/rfc5798) <br />I agree that there are some &#39;dirty&#39; L2 tricks that some HA software does (looking at you, LVS&#39;s DR), but there are plenty of legitimate and RFC supported reasons to have flexible MAC-IP mappings. This is why ARP is a critical protocol in IPv4. It would be nice if we could have long ARP timeouts due to highly static IP-MAC mappings, but there are a lot of advantages to be had doing it the current way too. Network Virtualization does not really nullify them, IMO, so I feel that it should have the necessary flexibility without having a 3rd party update policy sets.<br /><br />#5, 100% agree. So, why was the technology released before such things are available? :P<br /><br />The last problem I&#39;ve had with the specific way NVGRE was implemented in VMM is that the VM that has a NIC on a virtual NVGRE network has to get DHCP from the HV switch for that NIC, and that DHCP has a default gateway. That means that I can&#39;t have a single-subnet &#39;private&#39; virtualized network between VMs combined with a traditional network for Internet traffic (such as a web server VM that would use NVGRE to talk to it&#39;s storage and DB and a traditional network on which requests come in and replies go out). Is there a way to get around this? (if only DHCP clients respected DHCP route options, but alas). The only way I can think of is a proxy/LB in front like F5 or HAProxy.<br /><br />My point is that, NVGRE is a good concept, and works well as a core technology. I agree that it seems more scale-able that VxLAN. I think the issue I have is more in how it was released: lacking important supporting functionality.<br /><br />Thanks,<br />Alex",
          "id": "5943107492497738506",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Anonymous",
          "profile": null,
          "pub": "2013-02-02T00:32:43.740+01:00",
          "ref": "6740627190806906357",
          "type": "comment"
        },
        {
          "date": "05 February 2013 14:36",
          "html": "Dear Anonymous Alex,<br /><br />We&#39;re in total agreement regarding the out-of-sync release of NVGRE Hyper-V module (which I like) and orchestration software (which I haven&#39;t seen yet but might not like once I do ;) Same thing happened with the first release of vCloud Director and vShield Edge ... but vShield Edge  got infinitely better in the next release, and I hope VMM will go down the same path (hope never dies, does it).<br /><br />As for &quot;industry standard practices&quot; I couldn&#39;t disagree more. Just because everyone misuses a technology in ways it was never intended to be used to fix layer 8-10 problems doesn&#39;t make it right. <br /><br />DNS and SRV records were invented for a reason, and are the right tool to solve service migration issues. I know everyone uses ARP spoofing (oops, dynamic IP-to-MAC mapping ;) to be able to pretend the problem doesn&#39;t exist, but that still doesn&#39;t make it right. The &quot;R&quot; in VRRP stands for &quot;Router&quot;, not &quot;Cluster member&quot; for a good reason ;))<br /><br />As for your last problem, I have to think about it. Could you send me more details?<br /><br />Thank you!<br />Ivan",
          "id": "8711547136928584605",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2013-02-05T14:36:14.850+01:00",
          "ref": "6740627190806906357",
          "type": "comment"
        },
        {
          "date": "05 February 2013 22:16",
          "html": "Oh, that whole MS clustering mess... I couldn&#39;t agree more. MS&#39;s NLB is re-defines the concept of the layer 2-to-3 mess. My comments were always talking about VRRP in it&#39;s correct sense of router redundancy by allow an IP to move by a standby router. I&#39;ll send you an email detailing my experiences with how NVGRE is implemented, especially with VMM in the picture.<br /><br />Thanks,<br /><br />Anonymous Alex (I may have to use this henceforth)",
          "id": "5840912304811112895",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Anonymous",
          "profile": null,
          "pub": "2013-02-05T22:16:19.639+01:00",
          "ref": "6740627190806906357",
          "type": "comment"
        },
        {
          "date": "22 July 2013 07:16",
          "html": "Windows 2012 R2 &amp; System Center 2012 R2 have natively solved the gateway problem in NVGRE:  <br /><br />(http://channel9.msdn.com/Events/TechEd/Europe/2013/MDC-B380)<br /><br />1. In WS2012.R2, the HNV module was moved up into the vSwitch itself from where it previously resided on top of the LBFO module.  This allows L3/GRE encap/decap activities within the switch itself, subsequently allowing all extension modules to act on both physical/virtual (fabric/tenant) addresses.  This provides the hypervisor/switch extensions full visibility into both networks.<br /><br />2. In VMM2012.R2, a specific, multi-tenant gateway role was designed.  this allows HA VMs to act as a routing gateway for up to 100 tenant networks (at least in preview).  This does mean that access to external/shared resources is now flowing through an additional hop/server before hitting the network unencap, but this also allows multisite bridging between DCs, tenant sites, networks, etc.<br /><br />3. as to offloads: several NIC vendors (mostly merchant silicon: intel &amp; broadcom) are integrating NVGRE endpoints into their NICs.  This will allow all existing offloads to function as expected without any additional change to hardware.<br /><br /><br />Matthew Paul Arnold",
          "id": "7008047785291608698",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Anonymous",
          "profile": null,
          "pub": "2013-07-22T07:16:02.174+02:00",
          "ref": "6740627190806906357",
          "type": "comment"
        },
        {
          "date": "22 July 2013 07:56",
          "html": "addendum: I forgot to mention that VMM2012.R2 uses BGP on the control plane.<br /><br />mpa",
          "id": "8688314176968917226",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Anonymous",
          "profile": null,
          "pub": "2013-07-22T07:56:36.280+02:00",
          "ref": "6740627190806906357",
          "type": "comment"
        }
      ],
      "date": "22 January 2013 17:02",
      "html": "Those &#39;dirty&#39; layer 2 trick are part of the spec and are not &#39;tricks&#39; at all. An IP-MAC mapping is supposed to be flexible, and a IP address is supposed to be able to move between devices. The way NVGRE is implemented breaks most common HA mechanisms such as VRRP based virtual IPs. The fact that I can&#39;t set the virtualized IP of VM &#39;A&#39; as the default gateway of VM &#39;B&#39; and have NVGRE forward packets to it based on the MAC (not IP) means that basic layer 3 functionality is also broken in NVGRE. This is why a separate layer 3 gateway is required in VMM. It is astounding that Microsoft thought that it was OK to release such a thing without providing basic gateway functionality themselves. Currently, there are no big names providing gateway functionality, so there is no real way to get traffic out of a virtualized network to the rest of the world.",
      "id": "6740627190806906357",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2013-01-22T17:02:23.378+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "04 February 2013 10:24",
      "html": "Ivan,<br /><br />Any thoughts on  creating multipoint NVGRE (mNVGRE)?<br /><br />It seems like this would be NHRP + NVGRE implemented in a switch or router.  <br /><br />If you then had some mechanism for mapping a NVGRE TNI into a VRF, it seems like you would have the basis for a non-MPLS VPN-based way to extend virtual networks over an arbitrary IP WAN, encapsulated in an overlay, but without resorting to VRF-Lite complexity. <br /><br />If you could get it in a switch, from say, Cisco, Juniper, or Arista, it seems to me you have a big portion of the things you need to directly extend NV over an IP-only network without resorting to strapping expensive routers into your design just for the encapsulation function.  <br /><br />  ",
      "id": "2505312042637049587",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Sam Crooks",
      "profile": "http://the8thlayerof.net",
      "pub": "2013-02-04T10:24:54.680+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "19 March 2013 06:19",
      "html": "Ivan,<br /><br /> there seems to be another version of NVGRE draft published by murari et al as of feb 24, 2013<br /><br /> is it possible to compare the two drafts and comment on any changes or enlighten us on the new publication ?<br /><br />Kind Regards,<br />Anand Shah",
      "id": "2916658092632973585",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Anand Shah",
      "profile": "https://www.blogger.com/profile/14953814454666600401",
      "pub": "2013-03-19T06:19:15.818+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "02 April 2013 14:08",
          "html": "It&#39;s pretty easy to see what has changed, the page you quote has &quot;History&quot; tab with diffs.<br /><br />http://www.ietf.org/rfcdiff?url2=draft-sridharan-virtualization-nvgre-01<br /><br />http://www.ietf.org/rfcdiff?url2=draft-sridharan-virtualization-nvgre-02<br /><br />As you can see from the diff results, nothing substantial has been changed.",
          "id": "3129171939876091201",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2013-04-02T14:08:28.874+02:00",
          "ref": "9062924517217946323",
          "type": "comment"
        }
      ],
      "date": "19 March 2013 06:19",
      "html": "sorry... forgot to provide a link<br /><br />http://datatracker.ietf.org/doc/draft-sridharan-virtualization-nvgre/?include_text=1",
      "id": "9062924517217946323",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Anand Shah",
      "profile": "https://www.blogger.com/profile/14953814454666600401",
      "pub": "2013-03-19T06:19:49.167+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "23 March 2013 08:23",
      "html": "Im really lost with testing of NVGRE. I was able to test it with host (Provider Addresses) connected on access ports on the switch, but when im using teamed adapters connected on 802.1q trunk ports NVGRE seems not working. I really believe that Microsoft does not develop it in the way NVGRE to work only on access ports, but how? What is different in the configuration? What if im using 2 teamed converged adapters for cluster, management and tenant network? ",
      "id": "1348120841196031460",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2013-03-23T08:23:47.057+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "20 November 2013 14:03",
      "html": "I&#39;ve tested HNV with Windows 2012 R2 and SCVMM 2012 R2 and am very happy with what it offers. Will work perfectly for a public hyper-v based cloud project that I am involved in. What worries me currently is the gateway part. Microsoft gw is very undocumented especially when it comes to real world scalability. So far I understood that each tenant needs ITS OWN VM running win2012 rras. I have not found any clues on multitenancy features WITHIN a single windows server inatance. Anyone knows something better than that?<br /><br />Ofcourse we have the option to use any router/firewall vm appliance with 1 vnic in the virtual network and another vnic in the physical network. There are plenty of them that can do the job with much much less resources compared to the Windows RRAS, but still does not scale very well for public cloud. Especially for clients with a single VM it becomes too much overhead.<br /><br />On the hardware side i&#39;ve found only F5 and iron networks to offer something. <br /><br />In F5 what I understand is that they just add NVGRE functionality in their existing load balancers, so it&#39;s not a pure virtual to physical network gateway. Especially when the clients are not requiring load balancer, but rather NAT (static and/or overload), RA VPN and Site-2-Site VPN.<br /><br />Iron networks are also quite undocumented on the &quot;multi-tenancy&quot; topic. Nobody says how many tenants and what a cloud provider needs to use it.<br /><br />I will be very happy if anyone shares some additional knowledge and experience.",
      "id": "448846645780257018",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Borislav Lazarov",
      "profile": "http://www.efellows.bg/",
      "pub": "2013-11-20T14:03:33.874+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "06 January 2014 08:11",
          "html": "I think you should rethink all the different meanings of &quot;next hop&quot; in the overlay virtual network environment. I know, it&#39;s a bit confusing sometimes ;)",
          "id": "9201063809288516895",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2014-01-06T08:11:14.946+01:00",
          "ref": "7111554721955627841",
          "type": "comment"
        }
      ],
      "date": "06 January 2014 07:12",
      "html": "RE: #4 - NVGRE kernel module provides IP routing functionality with static routes (including default route), so you can push traffic toward IP next hop.<br /><br />I completely disagree with that. Even though you have -NextHop switch but that can only work for a PA Address (provided you have routing rules created) or a VMM gateway implemented on the network. <br /><br />-NextHop is available with CustomerRoute and ProviderRoute cmdlets but none of them work to forward non-NVGRE traffic to an external router.<br /><br />Thanks!",
      "id": "7111554721955627841",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2014-01-06T07:12:38.282+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    },
    {
      "date": "06 January 2014 08:28",
      "html": "Oops..sorry about that - I thought you&#39;re talking about -NextHop switch. My mistake!<br /><br />Thanks!",
      "id": "4105772435315895612",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2014-01-06T08:28:58.213+01:00",
      "ref": "3232527671910418905",
      "type": "comment"
    }
  ],
  "count": 38,
  "id": "3232527671910418905",
  "type": "post",
  "url": "2012/12/hyper-v-network-virtualization-wnvnvgre.html"
}