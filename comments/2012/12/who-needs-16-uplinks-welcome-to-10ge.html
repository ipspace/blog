<div class="comments post" id="comments">
  <h4>10 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1904024917800767975">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://blog.garraux.net" rel="nofollow">Oliver</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1904024917800767975" href="#1904024917800767975">06 December 2012 07:39</a>
              </span>
            </div>
            <div class="comment-content">I think people definitely should reconsider their oversubscription ratios for 10G server connectivity.  At least in our environment(enterprise), we&#39;re finding that we can have a much higher over-subscription ratio for 10G server access.  Most of our internal customers that are using 10G on their servers really don&#39;t &quot;need&quot; it based on the amount of traffic they generate.  (Granted - we have separate uplinks to connect our access switches together for vMotion, and not much in the way of IP storage).  Hopefully by the time they start pushing the 10G connections more, 40 / 100 G connectivity will be more commonly available on access switches.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5627412420406308835">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5627412420406308835" href="#5627412420406308835">06 December 2012 08:04</a>
              </span>
            </div>
            <div class="comment-content">As I wrote, &quot;it depends&quot;. However, having 10GE server links and not fully utilizing them feels like throwing money away to me ... and the number of multimode fiber pairs won&#39;t change regardless of whether you have 10GE, 40GE or 100GE ports.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4919316146352511634">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12675510409950425811" rel="nofollow">RPM</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4919316146352511634" href="#4919316146352511634">06 December 2012 15:05</a>
              </span>
            </div>
            <div class="comment-content">A significant fraction (perhaps a majority?) of new &quot;private cloud&quot; virtualization clusters I&#39;ve seen are being implemented with some form of IP-based storage (iSCSI, NFS, or now even SMB3 I suppose). This is usually done with just two 10G links to each virtualization host, with VLANs and perhaps QoS policies differentiating the storage traffic from the application and VMotion traffic.<br /><br />In some cases, the clustered storage is actually inside the virtualization hosts themselves (see Gluster or HP StoreVirtual VSA).<br /><br />The capex and opex savings that come with eliminating FC are compelling, but the use of IP storage does require lower oversubscrition ratios. Fortunately Ethernet ports and links are (comparatively) cheap, especially when the outrageous costs of &quot;vendor-blessed&quot; HBAs, FC optics, FC switches, and monolithic FC SANs are included in a TCO model.<br /><br />To my knowledge, approximately zero public/hybrid cloud service providers are using FC storage. Some might still offer FC with their managed/dedicated hosting offerings, but everything new is cloudy and IP-based.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3863109803777613441">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03014282355010119539" rel="nofollow">Michael67</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3863109803777613441" href="#3863109803777613441">09 December 2012 18:06</a>
              </span>
            </div>
            <div class="comment-content">//outrageous costs<br /><br />I know what you mean: Two fully equipped, plus SFPed, plus licensed, plus carepacked Brocade port fibrechannel switches for 80000,-â‚¬ is simply too much nowadays!</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3037925131502464599">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Will</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3037925131502464599" href="#3037925131502464599">06 December 2012 20:36</a>
              </span>
            </div>
            <div class="comment-content">Thanks for that info.  When I first installed the 5020s/5010s I was concerned about the number of uplinks to use but was swayed by firefly that it wouldnt be a big deal (mainly because Windows in combination with lower tiered storage have trouble pushing more than 2Gbs much less 10Gbs).  Two years later I&#39;m still running ok on 12:1 oversubcription on non-FCOE and NFS networks although I&#39;d say I&#39;m half 1Gbs and half 10Gb servers.<br /><br />I&#39;m glad I have read this prior to a large 5548 and UCS implementation.  Thanks a lot!</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="190996669120682615">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/15818883829738651247" rel="nofollow">shah</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c190996669120682615" href="#190996669120682615">12 December 2012 14:57</a>
              </span>
            </div>
            <div class="comment-content">For Fabric extender 1:1.2 is not considered oversubscription because port to port trafic is also going thru uplinks which is not the case for other switches. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4836930168356799844">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">LT</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4836930168356799844" href="#4836930168356799844">13 December 2012 14:43</a>
              </span>
            </div>
            <div class="comment-content">&#39;the number of fiber strands you need remains the same&#39; - this is entirely true in the case of multimode fibre and 40GbaseSR4. However if you use singlemode fibre you can do 40GbaseLR4 over a pair. Not suggesting this is a particularly good idea at present - LR4 optics are expensive and running more fibre if needed is probably more cost effective. Perhaps in the future though the economics will change.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="962513871691350166">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01607373488536972094" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c962513871691350166" href="#962513871691350166">23 January 2013 18:50</a>
              </span>
            </div>
            <div class="comment-content">What&#39;s powerful is if you don&#39;t have to think about OSR as being fixed the day you wire your network, or as being uniform across the fabric. It should be variable across the network and dynamic - higher in pockets that don&#39;t need it, and lower where it is needed, and continuously updated based on current conditions. But to do this, you have to move beyond traditional leaf/spine designs.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="8858711940534788652">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8858711940534788652" href="#8858711940534788652">23 January 2013 19:08</a>
              </span>
            </div>
            <div class="comment-content">Would this be an undercover Plexxi plug ;)) In theory I agree with you, in practice I&#39;d prefer building smaller single-purpose fabrics.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4232195021558812611">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01607373488536972094" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4232195021558812611" href="#4232195021558812611">23 January 2013 19:38</a>
              </span>
            </div>
            <div class="comment-content">Not meant to be undercover, just not overtly commercial :)<br /><br />Yes, another way is smaller single-purpose fabrics if that&#39;s what you prefer, but not necessarily practical for converged private cloud infrastructure where the goal is ultimately &quot;any workload&quot; on a single infrastructure, but most problems have multiple solutions, ours is just one.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
