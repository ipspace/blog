comments:
- comments:
  - date: 06 December 2012 08:04
    html: As I wrote, &quot;it depends&quot;. However, having 10GE server links and
      not fully utilizing them feels like throwing money away to me ... and the number
      of multimode fiber pairs won&#39;t change regardless of whether you have 10GE,
      40GE or 100GE ports.
    id: '5627412420406308835'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-12-06T08:04:13.138+01:00'
    ref: '1904024917800767975'
    type: comment
  - date: 06 December 2012 15:05
    html: A significant fraction (perhaps a majority?) of new &quot;private cloud&quot;
      virtualization clusters I&#39;ve seen are being implemented with some form of
      IP-based storage (iSCSI, NFS, or now even SMB3 I suppose). This is usually done
      with just two 10G links to each virtualization host, with VLANs and perhaps
      QoS policies differentiating the storage traffic from the application and VMotion
      traffic.<br /><br />In some cases, the clustered storage is actually inside
      the virtualization hosts themselves (see Gluster or HP StoreVirtual VSA).<br
      /><br />The capex and opex savings that come with eliminating FC are compelling,
      but the use of IP storage does require lower oversubscrition ratios. Fortunately
      Ethernet ports and links are (comparatively) cheap, especially when the outrageous
      costs of &quot;vendor-blessed&quot; HBAs, FC optics, FC switches, and monolithic
      FC SANs are included in a TCO model.<br /><br />To my knowledge, approximately
      zero public/hybrid cloud service providers are using FC storage. Some might
      still offer FC with their managed/dedicated hosting offerings, but everything
      new is cloudy and IP-based.
    id: '4919316146352511634'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: RPM
    profile: https://www.blogger.com/profile/12675510409950425811
    pub: '2012-12-06T15:05:27.676+01:00'
    ref: '1904024917800767975'
    type: comment
  - date: 09 December 2012 18:06
    html: "//outrageous costs<br /><br />I know what you mean: Two fully equipped,\
      \ plus SFPed, plus licensed, plus carepacked Brocade port fibrechannel switches\
      \ for 80000,-\u20AC is simply too much nowadays!"
    id: '3863109803777613441'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Michael67
    profile: https://www.blogger.com/profile/03014282355010119539
    pub: '2012-12-09T18:06:04.442+01:00'
    ref: '1904024917800767975'
    type: comment
  date: 06 December 2012 07:39
  html: I think people definitely should reconsider their oversubscription ratios
    for 10G server connectivity.  At least in our environment(enterprise), we&#39;re
    finding that we can have a much higher over-subscription ratio for 10G server
    access.  Most of our internal customers that are using 10G on their servers really
    don&#39;t &quot;need&quot; it based on the amount of traffic they generate.  (Granted
    - we have separate uplinks to connect our access switches together for vMotion,
    and not much in the way of IP storage).  Hopefully by the time they start pushing
    the 10G connections more, 40 / 100 G connectivity will be more commonly available
    on access switches.
  id: '1904024917800767975'
  image: https://resources.blogblog.com/img/blank.gif
  name: Oliver
  profile: http://blog.garraux.net
  pub: '2012-12-06T07:39:51.261+01:00'
  ref: '811227712710341998'
  type: comment
- date: 06 December 2012 20:36
  html: Thanks for that info.  When I first installed the 5020s/5010s I was concerned
    about the number of uplinks to use but was swayed by firefly that it wouldnt be
    a big deal (mainly because Windows in combination with lower tiered storage have
    trouble pushing more than 2Gbs much less 10Gbs).  Two years later I&#39;m still
    running ok on 12:1 oversubcription on non-FCOE and NFS networks although I&#39;d
    say I&#39;m half 1Gbs and half 10Gb servers.<br /><br />I&#39;m glad I have read
    this prior to a large 5548 and UCS implementation.  Thanks a lot!
  id: '3037925131502464599'
  image: https://resources.blogblog.com/img/blank.gif
  name: Will
  profile: null
  pub: '2012-12-06T20:36:46.429+01:00'
  ref: '811227712710341998'
  type: comment
- date: 12 December 2012 14:57
  html: 'For Fabric extender 1:1.2 is not considered oversubscription because port
    to port trafic is also going thru uplinks which is not the case for other switches. '
  id: '190996669120682615'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: shah
  profile: https://www.blogger.com/profile/15818883829738651247
  pub: '2012-12-12T14:57:09.662+01:00'
  ref: '811227712710341998'
  type: comment
- date: 13 December 2012 14:43
  html: '&#39;the number of fiber strands you need remains the same&#39; - this is
    entirely true in the case of multimode fibre and 40GbaseSR4. However if you use
    singlemode fibre you can do 40GbaseLR4 over a pair. Not suggesting this is a particularly
    good idea at present - LR4 optics are expensive and running more fibre if needed
    is probably more cost effective. Perhaps in the future though the economics will
    change.'
  id: '4836930168356799844'
  image: https://resources.blogblog.com/img/blank.gif
  name: LT
  profile: null
  pub: '2012-12-13T14:43:05.507+01:00'
  ref: '811227712710341998'
  type: comment
- comments:
  - date: 23 January 2013 19:08
    html: Would this be an undercover Plexxi plug ;)) In theory I agree with you,
      in practice I&#39;d prefer building smaller single-purpose fabrics.
    id: '8858711940534788652'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2013-01-23T19:08:07.688+01:00'
    ref: '962513871691350166'
    type: comment
  - date: 23 January 2013 19:38
    html: Not meant to be undercover, just not overtly commercial :)<br /><br />Yes,
      another way is smaller single-purpose fabrics if that&#39;s what you prefer,
      but not necessarily practical for converged private cloud infrastructure where
      the goal is ultimately &quot;any workload&quot; on a single infrastructure,
      but most problems have multiple solutions, ours is just one.
    id: '4232195021558812611'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Unknown
    profile: https://www.blogger.com/profile/01607373488536972094
    pub: '2013-01-23T19:38:28.458+01:00'
    ref: '962513871691350166'
    type: comment
  date: 23 January 2013 18:50
  html: What&#39;s powerful is if you don&#39;t have to think about OSR as being fixed
    the day you wire your network, or as being uniform across the fabric. It should
    be variable across the network and dynamic - higher in pockets that don&#39;t
    need it, and lower where it is needed, and continuously updated based on current
    conditions. But to do this, you have to move beyond traditional leaf/spine designs.
  id: '962513871691350166'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/01607373488536972094
  pub: '2013-01-23T18:50:01.227+01:00'
  ref: '811227712710341998'
  type: comment
count: 10
id: '811227712710341998'
type: post
url: 2012/12/who-needs-16-uplinks-welcome-to-10ge.html
