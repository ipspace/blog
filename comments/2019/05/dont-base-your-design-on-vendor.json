{
  "comments": [
    {
      "comments": [
        {
          "date": "22 May 2019 10:34",
          "html": "Hi Andrea,<br /><br />As I wrote in the blog post, most people who know how TCP and buffers work agree that deep buffers don&#39;t make sense in environments like data center fabrics... with a few exceptions.<br /><br />Also, never focus on the $vendor-generated buzzwords like &quot;Telco Cloud&quot;. <br /><br />If the so-called &quot;Telco Cloud&quot; runs TCP-based applications like most other data centers in this world, then it just might work with the same equipment that most other people use. <br /><br />OTOH, if all you run in your environment is CBR voice traffic using 64-byte UDP packets then you might need a different solution.<br /><br />Long story short: always understand the problem you&#39;re trying to solve first, and then try to figure out how other people with more experience solved similar problems.<br /><br />Hope this helps,<br />Ivan",
          "id": "7638969550985915395",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2019-05-22T10:34:57.887+02:00",
          "ref": "2685038696414754363",
          "type": "comment"
        }
      ],
      "date": "22 May 2019 09:41",
      "html": "Hello Ivan,<br />In this very respect, what I still do not understand is, for instance, the reason why vendors keep pushing and (thus) customers keep purchasing very (cheap) shallow-buffer Leaves (some have a total 16MB for the whole box ...) as part of the Telco Cloud&#39;s IP Fabric. To me the interfaces&#39; clock differences on the Leaves as well as a much higher RTTs than those typical of DCs&#39;(as the north-south component is dominant over the east-west in a Telco cloud) and the inability of controlling the TCP Congestion Control Algorithm flavour in a telco cloud environment as opposed to a DC environment dramatically increases the chance of TCP-induced traffic bursting on such shallow-buffer Leaves and thus the chance of poor performance. <br />Hope you Ivan/guys can share the same design concern for these days&#39; Telco Clouds&#39; IP Fabrics.<br />Cheers,<br />Andrea",
      "id": "2685038696414754363",
      "image": "https://4.bp.blogspot.com/-OxTJXQLMTP4/XiqiBKKL_vI/AAAAAAAAStc/w6E4R_ZKxB4nZweTCGxNGZAqyW_gkQtNACK4BGAYYCw/s32/BR.jpg",
      "name": "Andrea Di Donato",
      "profile": "https://www.blogger.com/profile/08479860629623524293",
      "pub": "2019-05-22T09:41:04.632+02:00",
      "ref": "4730990514628519984",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "28 May 2019 13:00",
          "html": "Hi Anonymous,<br />Thanks - I know I know ! I was once an R&amp;D on the subject of Router QoS and TCP performance in high bandwidth-delay product environments. The video you posted though holds for canonical DCs only and not also for Telco-clouds&#39; IP-FABRIC environments as 100 microseconds is considered by Tom Edsall in the video a large RTT !!! <br /><br />Coming back to us, I wouldn&#39;t be so sure about the current shallow buffer trend (probably fueled by some declination of the &#39;Stanford Model&#39;) on Fabric Leaves in a telco cloud environment after (re)reading the following Dovrolis&#39; papers:<br />https://www.caida.org/~amogh/papers/buffers-CCR06.pdf <br />https://www.cc.gatech.edu/~dovrolis/Papers/ravi-final-conext07.pdf<br /><br />Ciao <br />Andrea<br />Andrea",
          "id": "323476703171112360",
          "image": "https://4.bp.blogspot.com/-OxTJXQLMTP4/XiqiBKKL_vI/AAAAAAAAStc/w6E4R_ZKxB4nZweTCGxNGZAqyW_gkQtNACK4BGAYYCw/s32/BR.jpg",
          "name": "Andrea Di Donato",
          "profile": "https://www.blogger.com/profile/08479860629623524293",
          "pub": "2019-05-28T13:00:06.156+02:00",
          "ref": "840449128297940215",
          "type": "comment"
        },
        {
          "date": "28 May 2019 19:55",
          "html": "Those 100 microseconds RTT were maybe just an example for having an easy computation. Don&#39;t forget that 100 mircoseconds are 0.1 ms which is pretty fast. According to the bandwidth delay product you&#39;ll get even smaller buffers (product) when your RTT is low. But in a VoIP environment you&#39;re probably more interested in latency. Latency != RTT . UDP behaves differently than TCP (one has to keep the two separate). I can&#39;t imagine that big buffers would be beneficial in a &#39;Telco cloud&#39;.",
          "id": "2087887107487892024",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Anonymous",
          "profile": "https://www.blogger.com/profile/17892204563666956100",
          "pub": "2019-05-28T19:55:57.845+02:00",
          "ref": "840449128297940215",
          "type": "comment"
        },
        {
          "date": "28 May 2019 23:59",
          "html": "No, those 100 microseconds are a typical rtt of a canonical DC environment, not of a Telco cloud (e.g. virtualized sp edge), which is much more of a complex and challenging environment for tcp.  That guy is selling nexus for the DC market segment afterall - it makes sense to me. ",
          "id": "7338833188098130607",
          "image": "https://4.bp.blogspot.com/-OxTJXQLMTP4/XiqiBKKL_vI/AAAAAAAAStc/w6E4R_ZKxB4nZweTCGxNGZAqyW_gkQtNACK4BGAYYCw/s32/BR.jpg",
          "name": "Andrea Di Donato",
          "profile": "https://www.blogger.com/profile/08479860629623524293",
          "pub": "2019-05-28T23:59:22.984+02:00",
          "ref": "840449128297940215",
          "type": "comment"
        }
      ],
      "date": "22 May 2019 11:22",
      "html": "@Andrea: The problem that you have is called the bandwidth delay product. Here&#39;s a not so bad explanation (it&#39;s from Cisco but do connive) on buffers and TCP: https://youtu.be/ETpIp6fSw_4",
      "id": "840449128297940215",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Anonymous",
      "profile": "https://www.blogger.com/profile/17892204563666956100",
      "pub": "2019-05-22T11:22:27.446+02:00",
      "ref": "4730990514628519984",
      "type": "comment"
    },
    {
      "date": "22 May 2019 11:49",
      "html": "Hello Ivan,<br />Of course it helps - it&#39;s yours !!  :)<br />I am actually referring to the &#39;few exceptions&#39; that you mentioned. <br />The thing is, that, as far as I am seeing, the very same IP-Fabric is used in traditional DC environments as well as in Telco-Cloud environments with the latter still being a DC but with the SP&#39;s EDGE boxes&#39; (i.e. Business/Mobile PE and Residential NAS among others) data-plane been virtualised as VNF. I reckon these two environments can be very different in terms of RTTs (especially for the Business and Residential traffic as part of the mobile can and is TCP-proxied) and traffic direction (the dominant north-south component of Telco-Cloud exacerbates the IP Fabric Leaves&#39; interface-speed mismatch&#39;s impact on the traffic). <br />To me, the &quot;other people with more experience&quot;  seem to be the usual suspects  Google/Facebook/Amazon/..and the way they &quot;solved similar problems&quot; seem to be that of not using CUBIC for instance but well-engineered TCP Congestion Control Algorithms when in presence of shallow-buffer boxes. This is something a SP cannot do.<br />On a very similar note, the Google Global Caches solution I am witnessing within my SP for instance sees the deployment of a very-shallow-buffer Cisco box (16MB in total) as its network front-end and my suspect is that they are definitely not using CUBIC as the TCP Congestion Control Algorithm in order to avoid bursting and thus dropping. Will see ...<br /><br />Hope it makes sense<br />Ciao <br />Andrea    <br /><br />p.s. Having said that, at the very end, should there be any performance issues within the Telco Cloud environment due to the Leaves been shallow-buffered then I guess the $vendor could just swap the much cheaper Leaves with more expensive (as deep-buffered with GB of ASIC) boxes (acting now as Spines only) and still winning !!! :)",
      "id": "5614298008088969638",
      "image": "https://4.bp.blogspot.com/-OxTJXQLMTP4/XiqiBKKL_vI/AAAAAAAAStc/w6E4R_ZKxB4nZweTCGxNGZAqyW_gkQtNACK4BGAYYCw/s32/BR.jpg",
      "name": "Andrea Di Donato",
      "profile": "https://www.blogger.com/profile/08479860629623524293",
      "pub": "2019-05-22T11:49:27.787+02:00",
      "ref": "4730990514628519984",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "28 May 2019 13:01",
          "html": "Hi Anonymous,<br /><br />Thanks - I know I know ! I was once an R&amp;D on the subject of Router QoS and TCP performance in high bandwidth-delay product environments. The video you posted though holds for canonical DCs only and not also for Telco-clouds&#39; IP-FABRIC environments as 100 microseconds is considered by Tom Edsall in the video a large RTT !!! <br /><br />Coming back to us, I wouldn&#39;t be so sure about the current shallow buffer trend (probably fueled by some declination of the &#39;Stanford Model&#39;) on Fabric Leaves in a telco cloud environment after (re)reading the following Dovrolis&#39; papers:<br />https://www.caida.org/~amogh/papers/buffers-CCR06.pdf <br />https://www.cc.gatech.edu/~dovrolis/Papers/ravi-final-conext07.pdf<br />Ciao <br />Andrea",
          "id": "3478423689486409096",
          "image": "https://4.bp.blogspot.com/-OxTJXQLMTP4/XiqiBKKL_vI/AAAAAAAAStc/w6E4R_ZKxB4nZweTCGxNGZAqyW_gkQtNACK4BGAYYCw/s32/BR.jpg",
          "name": "Andrea Di Donato",
          "profile": "https://www.blogger.com/profile/08479860629623524293",
          "pub": "2019-05-28T13:01:00.748+02:00",
          "ref": "5127814271240126825",
          "type": "comment"
        }
      ],
      "date": "22 May 2019 17:21",
      "html": "Have a look at my answer above it answers most of your questions/obscurities. Additionally in a Telco environment you may suffer from UDP dominance and TCP starvation. You can solve that problem with not mixing UDP and TCP traffic by using QoS (different queues) or separate them physically.",
      "id": "5127814271240126825",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Anonymous",
      "profile": "https://www.blogger.com/profile/17892204563666956100",
      "pub": "2019-05-22T17:21:01.203+02:00",
      "ref": "4730990514628519984",
      "type": "comment"
    },
    {
      "date": "23 May 2019 12:30",
      "html": "Limit MLAG to within a rack, e.g. servers -&gt; 2*ToR and L3-only Leaf/Spine from there. Unless of course you&#39;re running at real scale and can get away with a single ToR :)<br />",
      "id": "5662490915366618984",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Albert Siersema",
      "profile": "https://www.blogger.com/profile/04847257511165693348",
      "pub": "2019-05-23T12:30:12.748+02:00",
      "ref": "4730990514628519984",
      "type": "comment"
    }
  ],
  "count": 10,
  "id": "4730990514628519984",
  "type": "post",
  "url": "2019/05/dont-base-your-design-on-vendor.html"
}