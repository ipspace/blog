<div class="comments post" id="comments">
  <h4>3 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2151232703311377734">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11529916018516958539" rel="nofollow">Salman Naqvi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2151232703311377734" href="#2151232703311377734">08 May 2019 16:18</a>
              </span>
            </div>
            <div class="comment-content">It&#39;s a moot point, as moral of the story is making good design choices and not having L2 spread wildly all over the place, but, I&#39;ve literally worked on over 200 Catalyst 6500s, with many deployed as core switches in a pair using VSS in Enterprise and Data Center environments and never had any stability issues over several years of software upgrades. In the last 6-7 years, across literally a few hundred Nexus switches, VPC has actually proven to be more unstable, but it&#39;s pretty rock solid if you stick to the Cisco&#39;s official &quot;Recommended Release&quot; which tends to be a couple of major versions behind.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7312229550521482246">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06074343110093815035" rel="nofollow">Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7312229550521482246" href="#7312229550521482246">08 May 2019 22:00</a>
              </span>
            </div>
            <div class="comment-content">If L2 is required then the lowest risk involved with a propagation of L2 issues (loops, cpu meltdowns, floods, etc.) is related to the host based overlays. A larger L2 underlay topology, a higher risk related to L2. Still VSS, vPC, MC-LAG or even just xSTP is good enough (read: easy and cheap) if there is a L3 data center interconnect.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7628233780158155401">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13811532460946121328" rel="nofollow">Ibi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7628233780158155401" href="#7628233780158155401">08 October 2019 12:57</a>
              </span>
            </div>
            <div class="comment-content">I had a customer break their DR site after someone patched in both iLO ports on a server (which are designed to be daisy chained); the switch they were plugged into was part of an EVPN fabric with L2 out to a VMware environment and a management switch, you can probably guess what happened but the broadcast storm propagated through the EVPN fabric, and out the L2 links; killing management and parts of the VMware environment. EVPN does not fix stupidity..</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
