{
  "comments": [
    {
      "comments": [
        {
          "date": "05 June 2019 14:49",
          "html": "Minimal latency and buffering are competing agendas in that case, aren&#39;t they?",
          "id": "920021382625867132",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Anthony M",
          "profile": "https://www.blogger.com/profile/07435790160863517891",
          "pub": "2019-06-05T14:49:19.442+02:00",
          "ref": "313603737967450638",
          "type": "comment"
        },
        {
          "date": "05 June 2019 16:18",
          "html": "I&#39;ve never been working in HFT environments, but I was told by several people who were that (A) the trading data is transferred using UDP over IP multicast and (B) they don&#39;t do retransmissions because the retransmissions take too much time. They switch over to a backup feed the moment data is lost from the primary feed (and hope the backup feed will survive the day).<br /><br />Also, all switches I&#39;ve seen from Arista and Cisco that were targeting HFT environments were shallow-buffer switches focused on minimizing latency.<br /><br />But then, of course, everyone might have been missing something really important. Wouldn&#39;t be the first time.",
          "id": "6990046977849627566",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2019-06-05T16:18:17.683+02:00",
          "ref": "313603737967450638",
          "type": "comment"
        },
        {
          "date": "05 June 2019 16:28",
          "html": "Hi Ivan,<br /><br />Thanks for the reply. Feed is usually send over MoldUdp in multicast. But for order entry, ouch is the protocol used by nasdaq systems and it is running on top of TCP soupbin. So it is vulnerable to packet loss as much it is to latency.",
          "id": "8935397940898503466",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/11429375361407084940",
          "pub": "2019-06-05T16:28:06.816+02:00",
          "ref": "313603737967450638",
          "type": "comment"
        },
        {
          "date": "05 June 2019 17:48",
          "html": "Thank you... and so I learn something new every day ;)) <br /><br />However, considering how much money is made in HFT, wouldn&#39;t it be simpler to increase link speeds to make sure they&#39;re not congested?",
          "id": "3114525979749020787",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2019-06-05T17:48:41.822+02:00",
          "ref": "313603737967450638",
          "type": "comment"
        },
        {
          "date": "05 June 2019 18:17",
          "html": "Yes, for sure. But due to the fairness rule, every competitor who is trying to buy/sell first needs to be queued on the same physical link of the gateway server, in fifo logic. This link can be a 100gbps nic with kernel bypass support on hw. This is one expensive solution. On the other hand, 10g nic with kernel bypass and a large buffers with minimal latency possible switch can be achive nearly similar results in terms of trading. There are some other parameters of course, the matching capacity of the trading system, the instrument diversity etc.. by the way, let me introduce my self, Serdar Kut. You may remember me from evpn ebgp nexthop talk we had in mail. My best regards",
          "id": "7744568936979149224",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/11429375361407084940",
          "pub": "2019-06-05T18:17:56.217+02:00",
          "ref": "313603737967450638",
          "type": "comment"
        }
      ],
      "date": "05 June 2019 08:55",
      "html": "Especially in hft trading environments, very first packets after the market is open are very important and they need to be delivered with minimal latency and without loss. There is a microburst traffic to be handled, and in that case it is better to buffer the packet instead of dropping and relying on tcp recovery. If dropped, retransmission timeout and the faith of the regenerated packet can cause million dollars..",
      "id": "313603737967450638",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/11429375361407084940",
      "pub": "2019-06-05T08:55:23.885+02:00",
      "ref": "8336508271375019561",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "05 June 2019 16:21",
          "html": "Do I understand correctly that you had ~300 hosts doing mostly writes to 9 iSCSI targets - a typical incast scenario where buffers matter most.<br /><br />It would be really nice to know whether things like ECN and DC-TCP would make things better... and having a pointer to real-life test results would be awesome.",
          "id": "5046932792527170706",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2019-06-05T16:21:37.447+02:00",
          "ref": "5449698275033265922",
          "type": "comment"
        },
        {
          "date": "07 June 2019 22:57",
          "html": "Very close understanding. It was 9 VMWare hosts, x2 v4 xeons, DDR4 2600 ~300 MongoDB VM&#39;s using Ubuntu on top of those hosts with the default big block filesystem sized configured. The VMWare hosts were tied to a cut-through 25Gb leaf (where discarding occurred). Everything was deployed with orchestration using cloud foundry, so t-shirt sizing without much customization of what mongodb did nor the file-system tuning that also could have helped. Also the iSCSI storage was Kaminario tied to a big buffer leaf (0 discarding) to help incast buffering from many hosts on many other leafs in the environment. During the discard storm you would see storage latency on the Kaminario as &#39;fabric&#39; indicating pipe or host issue. VMWare would get bad enough where it would loose it&#39;s datastore. Within the mongo VM&#39;s it appeared as a blink of an eye and didn&#39;t appear like anything was wrong. I&#39;d love to go back in time and create a detailed post showing it all including the ECN/DCTCP breakdown of what it was doing. <br /><br />I know on the HCI designs for VMWare VSAN and all out of box solutions like vxRAIL recommend big buffer switches when using 6+ nodes. My guess is VMWare might be a bit slow and having a packet traverse it&#39;s large network stack causing an artificial problem coupled with the large ingestion of synchronized data. Again, if I had a lab to do it to get the facts. Sorry man, as always I enjoy your write ups. :)",
          "id": "892381143393203744",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Jugganutz",
          "profile": "https://www.blogger.com/profile/12549295634827448573",
          "pub": "2019-06-07T22:57:42.765+02:00",
          "ref": "5449698275033265922",
          "type": "comment"
        }
      ],
      "date": "05 June 2019 15:36",
      "html": "I&#39;ve seen large farms of mongodb VMs used in a microservice world utilizing 25Gb iscsi across 9 hosts where shallow buffers were worse off than having big buffer. Big buffer helped taking +200ms of network induced storage latency at 260Mbps causing datastore loss to something more friendly around 43ms of storage latency and beyond 10Gbps throughput. It showed significantly reducing discards that were causing storage latency. The only other option I would have is to introduce more VM hosts to spread the micro burst of 280 Mongodb VMs doing a log rotate, gzip at the same nanosecond. If I was running a cloud I wouldn&#39;t want to tempt fate with crazy customer workloads like mine. ",
      "id": "5449698275033265922",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Jugganutz",
      "profile": "https://www.blogger.com/profile/12549295634827448573",
      "pub": "2019-06-05T15:36:23.128+02:00",
      "ref": "8336508271375019561",
      "type": "comment"
    },
    {
      "date": "06 June 2019 10:59",
      "html": "We identified a case in which packet losses mattered much for performance. In a research project, we build an HPC prototype that, because of reasons, had to employ Ethernet + IP + TCP instead of Infiniband or some other lossless technology. This network was used for inter-process communication (IPC) in parallel applications. Losing a packet means that one message of the communication of the application is lost and you have to wait for the Retransmission TimeOut (RTO); note that SACK does not work with the last segment of a flow, it detects the loss when the next segment is received with a gap. No matter the RTT, RTO has a minimal value (RTOmin) which might depend on the available timers on your CPU hardware or OS kernel; in our case, we couldn&#39;t configure this value below 5ms (default is 200ms, obviously set for WAN networks, not Datacenter environments). <br /><br />Parallel applications present communication phases which can last for some microseconds, and in some cases (e.g. synchronization barriers) they need to be completed before the next computation phase starts. Because of packet loses, we found traces of the execution in which these phases were delayed several milliseconds. This problem increases with the amount of traffic in the network, so larger executions (applications running on many more nodes) typically have more traffic, more losses and more delays, increasing overall execution time. Eventually, this problem restricts application scalability (number of nodes that can run the application in parallel with proportional increase in performance). <br /><br />We considered some alternatives already suggested in your text, such as using DC-TCP, but it was not available in our kernels (would had to be backported; not a problem nowadays) and some embedded Broadcom Ethernet switches used in our boards did not support ECN marking, so it would be useless anyway. BTW, using Ethernet flow control (pauses) did not really prevent packet loses (we speculated with packet losses at the NIC or OS level, or defective implementations since these were not 802.1Qbb with a lossless implementation in mind). <br /><br />Eventually, the obtained scalability was suboptimal because of these issues; I believe this is a clear motivation for most HPC environments using lossless technologies (Infiniband, Intel Omnipath, Bull BXI, Cray Aries &amp; Slingshot, etc.), apart from RDMA support and low switching latency. ",
      "id": "3111488803170449903",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Enrique Vallejo",
      "profile": "https://www.blogger.com/profile/05942794340597858649",
      "pub": "2019-06-06T10:59:58.399+02:00",
      "ref": "8336508271375019561",
      "type": "comment"
    },
    {
      "date": "07 June 2019 22:01",
      "html": "<br />One of these years ivan will mention &quot;fq_&quot;codel in a sentence. From a<br />network operator&#39;s perspective:<br /><br />* fq_codel ensures statistical multiplexing<br />* fq_codel ensures that congestion control algorithms across many flows<br />fate share faster<br />* fq_codel is more robust against packet floods<br />* fq_codel is safer to use ecn with<br />* codel drops from the head of the queue, not the tail, which makes flows where<br />the most important data is the most current data - work better - voip, gaming and dns - with less latency.<br />* fq_codel makes delay based and loss based TCPs co-exist better<br /><br /> It&#39;s only the default on roughly 100% linux nowadays for all devices, with sch_fq a distant second. It&#39;s now available for freebsd also.<br /><br />The specialized version we did for wifi has taken off like a skyrocket<br />(default in many qca based products, like google wifi and eero, intel just added support for iwl in linux 5.1), and we got it on OSX 2 years back.<br /><br />fq_codel is now the default QoS system for I think about 4/5ths the<br />home router market - especially for inbound shaping. Etc.<br /><br />We&#39;re really not huge on running codel standalone on a single queue - it&#39;s too gentle.<br />As single queued AQMs go, pie is better. sch_cake, even better, even in<br />single queue mode (paper due out next month). (in no case am I<br />recommending ecn at present due to the l4s/sce dispute).<br /><br />&quot;fq_&quot;codel (and for that matter &quot;fq_pie&quot;, allows for delay based and loss based TCPs to co-exist better, and will in the end help break the logjam here - bbr works great with it,<br />in particular.<br /><br />A very relevant paper on the futility of conventional congestion<br />control came out recently ( https://arxiv.org/pdf/1903.03852.pdf  )  and is being discussed on the BBR mailing list: https://groups.google.com/forum/#!topic/bbr-dev/chcftJgJ3vA<br /><br />But we have not seen *any* of the new aqm or fq technologies appear in hardware offloads yet. It seemed to be straightforward - most cards/switches have a 5 tuple hw hash already, drr was long ago (2008) made to work on netfpga - I think it&#39;s mostly market demand and awareness need to continue to be raised. <br /><br />&quot;fq_codel provides great isolation\u2026 if you\u2019ve got low-rate videoconferencing and low rate web traffic they never get dropped. A lot of the issues with iw10 go away, because all that other traffic sees is the front of the queue and you don\u2019t know how big its window is and you don\u2019t care because you are not affected by it.<br /><br />And: fq_codel increases utilization across your entire networking fabric especially for bidirectional traffic\u2026 If we\u2019re sticking code into boxes to deploy codel, don\u2019t do that. Deploy fq_codel. It\u2019s just an across the board win.\u201d - Van Jacobson, IETF 84<br /><br />Which we were hoping more folk would have done by now.",
      "id": "4977677756513644349",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Dave Taht",
      "profile": "https://www.blogger.com/profile/16115281578739979183",
      "pub": "2019-06-07T22:01:17.334+02:00",
      "ref": "8336508271375019561",
      "type": "comment"
    },
    {
      "date": "07 June 2019 22:56",
      "html": "This comment has been removed by the author.",
      "id": "4415259195884850509",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Jugganutz",
      "profile": "https://www.blogger.com/profile/12549295634827448573",
      "pub": "2019-06-07T22:56:52.390+02:00",
      "ref": "8336508271375019561",
      "type": "comment"
    }
  ],
  "count": 12,
  "id": "8336508271375019561",
  "type": "post",
  "url": "2019/06/do-packet-drops-matter-for-tcp.html"
}