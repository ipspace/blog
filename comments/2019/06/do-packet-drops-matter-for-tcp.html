<div class="comments post" id="comments">
  <h4>12 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="313603737967450638">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11429375361407084940" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c313603737967450638" href="#313603737967450638">05 June 2019 08:55</a>
              </span>
            </div>
            <div class="comment-content">Especially in hft trading environments, very first packets after the market is open are very important and they need to be delivered with minimal latency and without loss. There is a microburst traffic to be handled, and in that case it is better to buffer the packet instead of dropping and relying on tcp recovery. If dropped, retransmission timeout and the faith of the regenerated packet can cause million dollars..</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="920021382625867132">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07435790160863517891" rel="nofollow">Anthony M</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c920021382625867132" href="#920021382625867132">05 June 2019 14:49</a>
              </span>
            </div>
            <div class="comment-content">Minimal latency and buffering are competing agendas in that case, aren&#39;t they?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6990046977849627566">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6990046977849627566" href="#6990046977849627566">05 June 2019 16:18</a>
              </span>
            </div>
            <div class="comment-content">I&#39;ve never been working in HFT environments, but I was told by several people who were that (A) the trading data is transferred using UDP over IP multicast and (B) they don&#39;t do retransmissions because the retransmissions take too much time. They switch over to a backup feed the moment data is lost from the primary feed (and hope the backup feed will survive the day).<br /><br />Also, all switches I&#39;ve seen from Arista and Cisco that were targeting HFT environments were shallow-buffer switches focused on minimizing latency.<br /><br />But then, of course, everyone might have been missing something really important. Wouldn&#39;t be the first time.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8935397940898503466">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11429375361407084940" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8935397940898503466" href="#8935397940898503466">05 June 2019 16:28</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan,<br /><br />Thanks for the reply. Feed is usually send over MoldUdp in multicast. But for order entry, ouch is the protocol used by nasdaq systems and it is running on top of TCP soupbin. So it is vulnerable to packet loss as much it is to latency.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3114525979749020787">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3114525979749020787" href="#3114525979749020787">05 June 2019 17:48</a>
              </span>
            </div>
            <div class="comment-content">Thank you... and so I learn something new every day ;)) <br /><br />However, considering how much money is made in HFT, wouldn&#39;t it be simpler to increase link speeds to make sure they&#39;re not congested?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7744568936979149224">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11429375361407084940" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7744568936979149224" href="#7744568936979149224">05 June 2019 18:17</a>
              </span>
            </div>
            <div class="comment-content">Yes, for sure. But due to the fairness rule, every competitor who is trying to buy/sell first needs to be queued on the same physical link of the gateway server, in fifo logic. This link can be a 100gbps nic with kernel bypass support on hw. This is one expensive solution. On the other hand, 10g nic with kernel bypass and a large buffers with minimal latency possible switch can be achive nearly similar results in terms of trading. There are some other parameters of course, the matching capacity of the trading system, the instrument diversity etc.. by the way, let me introduce my self, Serdar Kut. You may remember me from evpn ebgp nexthop talk we had in mail. My best regards</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5449698275033265922">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12549295634827448573" rel="nofollow">Jugganutz</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5449698275033265922" href="#5449698275033265922">05 June 2019 15:36</a>
              </span>
            </div>
            <div class="comment-content">I&#39;ve seen large farms of mongodb VMs used in a microservice world utilizing 25Gb iscsi across 9 hosts where shallow buffers were worse off than having big buffer. Big buffer helped taking +200ms of network induced storage latency at 260Mbps causing datastore loss to something more friendly around 43ms of storage latency and beyond 10Gbps throughput. It showed significantly reducing discards that were causing storage latency. The only other option I would have is to introduce more VM hosts to spread the micro burst of 280 Mongodb VMs doing a log rotate, gzip at the same nanosecond. If I was running a cloud I wouldn&#39;t want to tempt fate with crazy customer workloads like mine. </div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5046932792527170706">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5046932792527170706" href="#5046932792527170706">05 June 2019 16:21</a>
              </span>
            </div>
            <div class="comment-content">Do I understand correctly that you had ~300 hosts doing mostly writes to 9 iSCSI targets - a typical incast scenario where buffers matter most.<br /><br />It would be really nice to know whether things like ECN and DC-TCP would make things better... and having a pointer to real-life test results would be awesome.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="892381143393203744">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12549295634827448573" rel="nofollow">Jugganutz</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c892381143393203744" href="#892381143393203744">07 June 2019 22:57</a>
              </span>
            </div>
            <div class="comment-content">Very close understanding. It was 9 VMWare hosts, x2 v4 xeons, DDR4 2600 ~300 MongoDB VM&#39;s using Ubuntu on top of those hosts with the default big block filesystem sized configured. The VMWare hosts were tied to a cut-through 25Gb leaf (where discarding occurred). Everything was deployed with orchestration using cloud foundry, so t-shirt sizing without much customization of what mongodb did nor the file-system tuning that also could have helped. Also the iSCSI storage was Kaminario tied to a big buffer leaf (0 discarding) to help incast buffering from many hosts on many other leafs in the environment. During the discard storm you would see storage latency on the Kaminario as &#39;fabric&#39; indicating pipe or host issue. VMWare would get bad enough where it would loose it&#39;s datastore. Within the mongo VM&#39;s it appeared as a blink of an eye and didn&#39;t appear like anything was wrong. I&#39;d love to go back in time and create a detailed post showing it all including the ECN/DCTCP breakdown of what it was doing. <br /><br />I know on the HCI designs for VMWare VSAN and all out of box solutions like vxRAIL recommend big buffer switches when using 6+ nodes. My guess is VMWare might be a bit slow and having a packet traverse it&#39;s large network stack causing an artificial problem coupled with the large ingestion of synchronized data. Again, if I had a lab to do it to get the facts. Sorry man, as always I enjoy your write ups. :)</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3111488803170449903">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/05942794340597858649" rel="nofollow">Enrique Vallejo</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3111488803170449903" href="#3111488803170449903">06 June 2019 10:59</a>
              </span>
            </div>
            <div class="comment-content">We identified a case in which packet losses mattered much for performance. In a research project, we build an HPC prototype that, because of reasons, had to employ Ethernet + IP + TCP instead of Infiniband or some other lossless technology. This network was used for inter-process communication (IPC) in parallel applications. Losing a packet means that one message of the communication of the application is lost and you have to wait for the Retransmission TimeOut (RTO); note that SACK does not work with the last segment of a flow, it detects the loss when the next segment is received with a gap. No matter the RTT, RTO has a minimal value (RTOmin) which might depend on the available timers on your CPU hardware or OS kernel; in our case, we couldn&#39;t configure this value below 5ms (default is 200ms, obviously set for WAN networks, not Datacenter environments). <br /><br />Parallel applications present communication phases which can last for some microseconds, and in some cases (e.g. synchronization barriers) they need to be completed before the next computation phase starts. Because of packet loses, we found traces of the execution in which these phases were delayed several milliseconds. This problem increases with the amount of traffic in the network, so larger executions (applications running on many more nodes) typically have more traffic, more losses and more delays, increasing overall execution time. Eventually, this problem restricts application scalability (number of nodes that can run the application in parallel with proportional increase in performance). <br /><br />We considered some alternatives already suggested in your text, such as using DC-TCP, but it was not available in our kernels (would had to be backported; not a problem nowadays) and some embedded Broadcom Ethernet switches used in our boards did not support ECN marking, so it would be useless anyway. BTW, using Ethernet flow control (pauses) did not really prevent packet loses (we speculated with packet losses at the NIC or OS level, or defective implementations since these were not 802.1Qbb with a lossless implementation in mind). <br /><br />Eventually, the obtained scalability was suboptimal because of these issues; I believe this is a clear motivation for most HPC environments using lossless technologies (Infiniband, Intel Omnipath, Bull BXI, Cray Aries &amp; Slingshot, etc.), apart from RDMA support and low switching latency. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4977677756513644349">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/16115281578739979183" rel="nofollow">Dave Taht</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4977677756513644349" href="#4977677756513644349">07 June 2019 22:01</a>
              </span>
            </div>
            <div class="comment-content"><br />One of these years ivan will mention &quot;fq_&quot;codel in a sentence. From a<br />network operator&#39;s perspective:<br /><br />* fq_codel ensures statistical multiplexing<br />* fq_codel ensures that congestion control algorithms across many flows<br />fate share faster<br />* fq_codel is more robust against packet floods<br />* fq_codel is safer to use ecn with<br />* codel drops from the head of the queue, not the tail, which makes flows where<br />the most important data is the most current data - work better - voip, gaming and dns - with less latency.<br />* fq_codel makes delay based and loss based TCPs co-exist better<br /><br /> It&#39;s only the default on roughly 100% linux nowadays for all devices, with sch_fq a distant second. It&#39;s now available for freebsd also.<br /><br />The specialized version we did for wifi has taken off like a skyrocket<br />(default in many qca based products, like google wifi and eero, intel just added support for iwl in linux 5.1), and we got it on OSX 2 years back.<br /><br />fq_codel is now the default QoS system for I think about 4/5ths the<br />home router market - especially for inbound shaping. Etc.<br /><br />We&#39;re really not huge on running codel standalone on a single queue - it&#39;s too gentle.<br />As single queued AQMs go, pie is better. sch_cake, even better, even in<br />single queue mode (paper due out next month). (in no case am I<br />recommending ecn at present due to the l4s/sce dispute).<br /><br />&quot;fq_&quot;codel (and for that matter &quot;fq_pie&quot;, allows for delay based and loss based TCPs to co-exist better, and will in the end help break the logjam here - bbr works great with it,<br />in particular.<br /><br />A very relevant paper on the futility of conventional congestion<br />control came out recently ( https://arxiv.org/pdf/1903.03852.pdf  )  and is being discussed on the BBR mailing list: https://groups.google.com/forum/#!topic/bbr-dev/chcftJgJ3vA<br /><br />But we have not seen *any* of the new aqm or fq technologies appear in hardware offloads yet. It seemed to be straightforward - most cards/switches have a 5 tuple hw hash already, drr was long ago (2008) made to work on netfpga - I think it&#39;s mostly market demand and awareness need to continue to be raised. <br /><br />&quot;fq_codel provides great isolation… if you’ve got low-rate videoconferencing and low rate web traffic they never get dropped. A lot of the issues with iw10 go away, because all that other traffic sees is the front of the queue and you don’t know how big its window is and you don’t care because you are not affected by it.<br /><br />And: fq_codel increases utilization across your entire networking fabric especially for bidirectional traffic… If we’re sticking code into boxes to deploy codel, don’t do that. Deploy fq_codel. It’s just an across the board win.” - Van Jacobson, IETF 84<br /><br />Which we were hoping more folk would have done by now.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4415259195884850509">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12549295634827448573" rel="nofollow">Jugganutz</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4415259195884850509" href="#4415259195884850509">07 June 2019 22:56</a>
              </span>
            </div>
            <div class="comment-content">This comment has been removed by the author.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
