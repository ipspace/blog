<div class="comments post" id="comments">
  <h4>8 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="397284029762380470">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17892204563666956100" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c397284029762380470" href="#397284029762380470">12 June 2019 17:51</a>
              </span>
            </div>
            <div class="comment-content">Good guesswork but it might still not be enough for some of your followers to trust you on the shallow buffer mystery.<br />What about the uplinks? Do they also share the buffer space?<br />Pay attention to the fact that one-way delay != RTT . Also your definition of latency would be helpful.<br />An other assumption would be that most if not all traffic is TCP based. Then of course the bandwidth delay product comes into play.<br />Now I&#39;m looking forward to the deep buffers advocates to join the party.<br />PS: It seems you got inspired by my youtube reference about buffer requirements in networking from your beloved vendor ;)</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4302779810891876989">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4302779810891876989" href="#4302779810891876989">12 June 2019 19:13</a>
              </span>
            </div>
            <div class="comment-content">&quot;it might still not be enough for some of your followers to trust you on the shallow buffer mystery&quot; &lt;&lt; I don&#39;t have an answer (apart from &quot;it depends&quot;) and we cannot solve the mystery because (A) it&#39;s really hard to generalize anything to a very wide range of environments (I&#39;ve heard there&#39;s still a bit of a gap between general relativity and quantum mechanics ;) and (B) we don&#39;t have a solid understanding of how things work thanks to Broadcom &amp; co.<br /><br />If I ever get as far as &quot;it depends on...&quot; followed by a solid (and justifiable) set of parameters, I&#39;ll be a happy person, and will know more than I know today.<br /><br />As for &quot;one-way delay versus RTT&quot;, I spent a lot of time thinking about that, and got to the conclusion that RTT is the one that really matters (but maybe I got it wrong together with everyone else).<br /><br />And yeah, I would love to hear the cheers from the deep buffer crowd :D<br /><br />More to come, stay tuned...</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5311688739443839597">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17266719222478374259" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5311688739443839597" href="#5311688739443839597">12 June 2019 20:44</a>
              </span>
            </div>
            <div class="comment-content">You also forgot about multiple incast frames arriving at exactly the same time as a need for buffer, but usually in this case you don&#39;t need massive buffers.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="890163712054406339">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/05113143598625696997" rel="nofollow">Terlo</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c890163712054406339" href="#890163712054406339">13 June 2019 14:04</a>
              </span>
            </div>
            <div class="comment-content">The need for deep buffers definitely depends on your environment/types of traffic.<br /><br />One example where you need deep buffers, is when you forward market data over high-latency circuits. Let&#39;s say you&#39;ve spent a lot of time and money to make sure your circuits are 100% clean, wouldn&#39;t you be a bit mad if your switches end up discarding packets and causing a few seconds of latency?<br /><br />With shallow-buffer switches, increase the size of your switch uplinks, then you might have discards from uplinks to hosts. Keep your uplinks at low speed, then your will have discards from hosts to uplinks. If your uplinks are connected to a router, it might also be costly to get 40G/100G ports on that router: which also means you will save some money (on the router side) if you use deep buffer switches with 10G uplinks. Why would anyone do that? Well, what&#39;s the point of using 40G/100G uplinks, if your router has a 10G circuit (not 40G/100G) to other locations? Might as well buffer on the (cheap) switch than on the expensive router.<br /><br />Now, vendors using deep-buffer Broadcom ASICs  were not really honest when they said you could buffer Gbytes of data. What did they forget to tell you? That it depends on the frame size. Broadcom Jericho uses buffer cells of 1000 bytes. Any frame smaller than 1000 bytes will use the entire cell, so if you try to buffer 200-byte frames (about the average size of market data), you can only use 20% of the buffer, and the rest is wasted. Switch spec  shows 8 GBytes, but you can only use 300 Mbytes for 200-byte frames, less than some Catalyst switches.<br /><br />All Broadcom ASICs use this buffer cell method, with a different cell size for each ASIC (usually 208 bytes). The ASIC used in Arista 7020R doesn&#39;t have this limitation (one buffer cell can buffer multiple frames), and Jericho 2 doesn&#39;t have this problem either I was told (to be tested). Anyway, what this means is your deep-buffer switch&#39;s buffer is not as deep as you thought, and definitely not ready to replace your expensive PE which can buffer up to 500ms of data per port.<br /><br />One last comment: if you look at all popular shallow-buffer ASICs, and look at how much buffer in ms you have for a single port (let&#39;s say your port is 10G on Trident, and 400G on Tomahawk III), you will notice the buffer size in ms keeps decreasing. It was 7.2 ms with Trident, it&#39;s now 1.3 ms with Tomahawk III. Do you really think that&#39;s enough buffer?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="416936638215363780">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08479860629623524293" rel="nofollow">Andrea Di Donato</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c416936638215363780" href="#416936638215363780">17 June 2019 11:31</a>
              </span>
            </div>
            <div class="comment-content">Very interesting topic and replies - well done everybody. For my part, I&#39;ll carry on being quite &#39;anal&#39; on the use of the very same Fabrics that were architected for DC traffic (i.e. in particular with Leaves of 16MB of total buffer which can make sense in a DC as highlighted in this Ivan&#39;s article) but in an environment that is far from being a DC: The SP&#39;s Virtualised EDGE (i.e. NAS, NAM and PE control and, more importantly, data plane deployed as VNF along with most of the services such as CGNAT, MobilePacketCore and so forth) where there&#39;s a huge difference in terms of the following:<br /><br />- RTT is much more variable and can be much higher than a typical DC RTT <br />- The TCP Congestion avoidance algorithm cannot be optimised for shallow buffering as it <br />        happens for Google, Amazon and so forth as the SP is simply not in control of the <br />        TCP/Applications as it just transports them.<br />- There can still be huge interface speed difference especially at the Leaf layer going <br />        from , say, 100Ge to 10Ge.<br /><br />- Last but not least, the assumptions at the basis of the &#39;Stanford Model&#39; of buffer sizing (advocating small buffers in SP networks) have been challenged from several angles and by several papers (see below some of the challenges):<br />1. Itâ€™s not just the link utilisation and the queueing delay to be optimized but the quality of service perceived by TCP flows too as at the end of the day that&#39;s where your customer is...<br />2. A fixed number N of long-lived TCP flows is a pretty poor model. A SP network is more complex than this<br />3. The model should differentiate between Core and Edge/access links where the interface speed difference can be important<br /><br />Such a challenge opened up a much more complex scenario to be considered to properly model a SP network capable of providing a more realistic formula about the best buffer size in every context. I guess this can be part of the next blog that Ivan said it&#39;d be about inter-DC traffic (or maybe actually of a third one since the environment I am talking about is not inter-DC but Virtualized SP Edge.. ) ?<br />Ciao<br />Andrea<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8915464020786303971">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07496348222409131474" rel="nofollow">D.Shephard</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8915464020786303971" href="#8915464020786303971">01 July 2019 16:13</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan, FYI Mr Warner&#39;s incredible pages seem to have been discontinued, ( I bet the switch vendors are happy on that one). Maybe he retired,  I doubt its just a glitch, as he has disappeared from the list of &quot;Staff individual home pages&quot;, he was there on the MAY 2019 capture in web.archive.org, but not today. Maybe we should petition to have those pages restored or curated by another interested party at UCSC.  BR David Shephard</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="239396956148444696">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c239396956148444696" href="#239396956148444696">01 July 2019 16:16</a>
              </span>
            </div>
            <div class="comment-content">Thanks for the info! Enno Rey noticed that yesterday and saved a snapshot of the page. <br /><br />https://twitter.com/enno_insinuator/status/1145389102906454016?s=12</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2799538356862889588">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08479860629623524293" rel="nofollow">Andrea Di Donato</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2799538356862889588" href="#2799538356862889588">11 July 2019 14:14</a>
              </span>
            </div>
            <div class="comment-content">thank you guys for saving such a key info - much appreciated</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
