<div class="comments post" id="comments">
  <h4>4 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2036175573281350232">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17892204563666956100" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2036175573281350232" href="#2036175573281350232">20 June 2019 10:37</a>
              </span>
            </div>
            <div class="comment-content">Only way to get around this is a proof of concept (due diligence) and to test extensively. Test node failures, link failures and grey failures. Also test multiple failure scenarios at the same time (here&#39;s where it get&#39;s interesting).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2798922047501271330">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2798922047501271330" href="#2798922047501271330">20 June 2019 11:49</a>
              </span>
            </div>
            <div class="comment-content">This is complex topic. Regardless of the type of distributed system:<br /><br />- collect requirements related to Failure &amp; Recovery (&#39;what if&#39; scenarios where the failure of links / devices / controllers) - Customer tends to expect &quot;miracles&quot; - the low of physic is the limit (+ your imagination what may happen) - think about two failures at the same time (define what is the same time i.e. a gap between failures)<br /><br />- put redundant controllers wherever required (especially in isolated parts of the systems) and DEFINE &amp; implement system logic behind to handle failure scenarios(e.g. when the isolated part of the system is taking over the responsibility for controlling the routing path, how the failure is detected, what is considered the failure, split-brain scenarios handling, etc)<br /><br />-PUT ALL the constraints coming from the above in the contract to avoid being sued for not handling a failure (so be VERY SPECIFIC what is possible and supported!!)<br /><br />- TEST, TEST, and again TEST and be ready for defects from the field (you will be surprised how different the real life issues are from those in the lab)<br /><br />Easy, isn&#39;t it? Joking. If you work in this kind of field you know how complex it is, and how big is the GREY area (undefined failure scenarios, how difficult is to define the failure, etc). </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1027820689296184508">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07702813532972713307" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1027820689296184508" href="#1027820689296184508">21 June 2019 04:30</a>
              </span>
            </div>
            <div class="comment-content">Perhaps not the same scale but even google got it (somewhat) wrong:<br /><br /><br />« Google&#39;s resilience strategy relies on the principle of defense in depth. Specifically, despite the network control infrastructure being designed to be highly resilient, the network is designed to &#39;fail static&#39; and run for a period of time without the control plane being present as an additional line of defense against failure. The network ran normally for a short period - several minutes - after the control plane had been descheduled. After this period, BGP routing between specific impacted physical locations was withdrawn, resulting in the significant reduction in network capacity observed by our services and users, and the inaccessibility of some Google Cloud regions. End-user impact began to be seen in the period 11:47-11:49 US/Pacific. »<br /><br />https://status.cloud.google.com/incident/cloud-networking/19009<br /><br /><br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1214912916018303754">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17220365000849227779" rel="nofollow">Saravanan R</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1214912916018303754" href="#1214912916018303754">03 July 2019 13:31</a>
              </span>
            </div>
            <div class="comment-content">SD-WAN vendors like Versa Networks providers controller redundancy with multiple controllers across different geographical location. Also there are mechanisms in place to counter situation where a branch that loses connectivity to all controllers can still use the local information to route packets to other branch devices.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
