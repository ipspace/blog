<div class="comments post" id="comments">
  <h4>11 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="6651716835434318537">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6651716835434318537" href="#6651716835434318537">17 October 2019 12:24</a>
              </span>
            </div>
            <div class="comment-content">Let me share you system vendor&#39;s perspective &amp; experience: from this perspective Failure &amp; Recovery (Disaster recovery) should be sold as any other feature (should be a part os Acceptance tests). <br />Vendor does not want to avoid responsibility and being blamed for not doing miracles.<br />The crucial thing is to define what kind of failures should be covered (handled) and what is expected system response to this failures (the connectivity gap, etc). <br />So the whole problem is not only about technology which is neither good or bad (still cheap solutions win as lomg as the consequences to the business continuity are clearly shown adn accepted).<br />The problem is when the Customer expects &#39;miracles&#39; where the technology cannot handle it.<br />---<br />So define the requirements to the system surivalibility (what kind of failures are supposed to be handled, do we support more than ONE failure?, what about the gap between subsequent failures - sometimes systems cannot survive two failures when the gap between them is short to recover, recovery is NOT only about the networking part - we have servers as well, improve th eweakest part first ,etc)<br />Then test them during the System Acceptance Testing.<br />Of course the real life is much more complex and you will be hit but something unexpected. But be specific what you sell/advertise. The customer tends to assume that there is a &#39;magic&#39; under the hood. So ask how this magic does work.<br />Don&#39;t expect the magic is going to &quot;fix&quot; low of physics. The honest vendor should talk to the Customer about shortcomings to avoid being blamed in the future.<br /><br />Again, this is vendor&#39;s perspective.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="493100510913735691">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c493100510913735691" href="#493100510913735691">17 October 2019 18:27</a>
              </span>
            </div>
            <div class="comment-content">&quot;Again, this is vendor&#39;s perspective&quot; &lt;&lt; and I totally agree with everything you wrote, and I would love to have a vendor delivering and taking responsibility for an end-to-end solution.<br /><br />The crux of the problem is somewhere else - most IT vendors supply spare parts for DIY kits, tell their customers how wonderful the creations based on those DIY kits can be, but never take any responsibility for the end result. Some go as far as blaming everyone else, and telling people how to misconfigure boxes connected to their miraculous creations to make the whole thing work.<br /><br />Now we can say &quot;but it&#39;s the customers claiming that every network is different&quot;, and that&#39;s absolutely right... but I doubt that it&#39;s in any vendors&#39; interest to change that mentality.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="366813022215077639">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c366813022215077639" href="#366813022215077639">17 October 2019 18:43</a>
              </span>
            </div>
            <div class="comment-content">Yes. You are right. When you deliver the SYSTEM as the PRODUCT and want to build it out of &#39;DYI Kits&#39; you need EXTRA developer support from a vendor (=they customize/develop the feature you need to achieve what you need). This is sad side of the IT &#39;DIY Kits&#39; business: whenever you want to make something more advanced the vendor offers extra support for extra money. In my quite long professional history I saw it several times.<br /><br />This proves that &#39;DIY Kits&#39; are often useless. That&#39;s why programmable devices (when you have own development forces) is sometimes the way to go. You don&#39;t need spend time and money on working with vendor&#39;s development team to teach them what is the use case.<br /><br />You touched the important part: often vendors of the &#39;DYI Kits&#39; do not address real life use cases - they are somewhat alienated from real life - but it&#39;s another story.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="735230549768464448">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06074343110093815035" rel="nofollow">Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c735230549768464448" href="#735230549768464448">17 October 2019 19:16</a>
              </span>
            </div>
            <div class="comment-content">Ivan, I support your mission to avoid stretched VLANs, but this is not a fairy tale of virtualization consultants. :)<br />The virtualization layer in DR does not require the stretched layer-2. As described in this case, a cluster of firewall had a split-brain. So the question comes how many firewall vendors support L3 clustering (not over VXLAN)? Checkpoint? Anyone else? Even without this feature, still, L2 is not needed for a DR scenario. A different story is Active-Active and Active-Standby scenarios. L2 is overused, but with overlays, it can be at least limited here. To have fully L3 separated domains in the Active-Active model, a customer would need to rewrite his applications. Stateless architecture is an answer. Then the status of applications won&#39;t be stored in the front/app layer, which frequently requires the L2 extension.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="6061322130742385642">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6061322130742385642" href="#6061322130742385642">17 October 2019 20:10</a>
              </span>
            </div>
            <div class="comment-content">Let me start with a real-life story: https://blog.ipspace.net/2013/01/long-distance-vmotion-stretched-ha.html - I know for a fact what color the background on those slides were.<br /><br />Also, let&#39;s be realistic. Which vendor promotes these in their marketing materials as DR solution:<br /><br />* stretched compute clusters with affinity rules to prevent VMs from escaping into the wrong data center (don&#39;t get me started on this one...);<br />* stretched distributed file system instead of storage replication;<br />* long-distance VM mobility for &quot;disaster avoidance&quot;<br /><br />... and what industry segment is that vendor most known for?<br /><br />Disaster recovery done right does NOT require stretched anything, but in that case the teams in an organization have to TALK to each other and SYNCHRONIZE their actions. OMG, what a niche to exploit ;) Remind me again, which vendor was the first one telling their customers &quot;you don&#39;t need to talk to anyone else, just ask for stretched VLAN and we can do the rest&quot; story?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1432635784637233988">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06074343110093815035" rel="nofollow">Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1432635784637233988" href="#1432635784637233988">17 October 2019 20:43</a>
              </span>
            </div>
            <div class="comment-content">Ivan, similarly we can say that EVPN is not recommended now because in 2013 there was no route-type 5 supported in BGP. Vmotion does not require L2 since 2015. At that time, every vendor promoted stretched VLANs by offering solutions requiring L2 - firewalls, load-balancers, MS clusters, storage clusters. All those fabrics based on VPLS, FP, TRILL, OTV were solutions to do the same - extent L2. At that time, everyone was a promoter. But saying today that stretched VLANs is a fairy tale of virtualization consultants is not valid. Remind me again, which component had a failure after power off physical switches? :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8064360052551137323">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8064360052551137323" href="#8064360052551137323">17 October 2019 20:51</a>
              </span>
            </div>
            <div class="comment-content">You conveniently ignored &quot;even worse, the storage didn&#39;t survive&quot; ;) .. oh and don&#39;t blame Microsoft. They had DNS-based clustering for ages.<br /><br />In any case, while there are no innocents in this mess, we are both old enough to know how the stretched VLAN story started, and no amount of lipstick will make this pig look any better.<br /><br />I&#39;m stopping my end of the discussion.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8316974851985212395">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06074343110093815035" rel="nofollow">Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8316974851985212395" href="#8316974851985212395">17 October 2019 21:04</a>
              </span>
            </div>
            <div class="comment-content">We discussed virtualization consultants and their fairy tales. Who is the vendor of this iSCSI storage? If VMware then, of course, it contributed to the failure outcomes. ..and a good DNS option does not mean that customers are actually using it always; unfortunately, the mcast based solution was quite common.<br /><br />Today, there is a new promoter of L2 extension - containers. In my opinion, we should try to evangelize developers and business decision-makers more than vendors. ;)</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2865537518458425145">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17892204563666956100" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2865537518458425145" href="#2865537518458425145">17 October 2019 19:19</a>
              </span>
            </div>
            <div class="comment-content">Thank you that you dedicated an own blog post based on what I experienced. As often after the total failure the system administrators blamed the network. I had to justify myself that there was no problem on layer 2 with the help of MAC address table-, interface-, spanning-tree and LACP events. Finally they stopped blaming and wrote the problem off because in their eyes a DC failure would be a rare case. Notice that they sold a &quot;geo redundant high availability solution&quot; to the customer.<br /><br />Also with the same employer I had a second experience (with own DCs) where I hadn&#39;t that much fun. They still have SLAs with huge financial impact which would be triggered if connectivity is lost for seconds. To &quot;meet&quot; the SLAs they spanned VLANs for a lot of applications but not for all (they sold the non redundant applications as &quot;high available&quot; regardless). Their assumption was that everything but the non redundant applications would survive a DC failure. I wasn&#39;t sure about their assumption so I suggested another shutdown of the core switches. I reached the change management to find a maintenance window and prepare a change. The answer was &quot;Don&#39;t dare you! If you would do so everybody can look for a new job&quot;. So as you said it&#39;s a ticking bomb.<br /><br />For years I&#39;m working in the networking field I always saw spanned VLAN deployments failing miserably. A spanned VLAN is still a single point of failure no matter what. Most high available (failover) solution out there will fail in the event of a DC failure because they are untested. I don&#39;t give up: every time there&#39;s the possibility for a DR test I will do a core switch shutdown and enjoy the mess.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="7670633430060164665">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06074343110093815035" rel="nofollow">Piotr Jablonski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7670633430060164665" href="#7670633430060164665">17 October 2019 19:37</a>
              </span>
            </div>
            <div class="comment-content">Agree, that sooner or later the architecture based on stretched VLANs fails. That&#39;s why I like the decoupled architecture promoted by AWS. L2 cannot be extended between Availability Zones there. Even if someone does not like the public cloud, this approach can be replicated to a local Data Centers. :)</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2026074056008921740">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14052258971284872140" rel="nofollow">Ken Lai</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2026074056008921740" href="#2026074056008921740">23 October 2019 03:03</a>
              </span>
            </div>
            <div class="comment-content">I always respect the ChaosMonkey in Netflix, which randomly shuts down services to test the resilience and recovery plan, after I&#39;ve seen so many perfect-in-PPT-only architectures. I thinks the problem is not only limited to the strentched VLANs but also other things. <br /><br />Just as Bogdan Golab said, a perfect solution may not be good since some times vendors need something to be broken so they can sell more.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
