comments:
- comments:
  - date: 06 March 2019 20:09
    html: 'For people who are interested in learning about non-Clos topologies I recommend
      this presentation: https://www.microsoft.com/en-us/research/video/network-topologies-for-large-scale-datacenters-its-the-diameter-stupid/
      Obviously it&#39;s somewhat biased since he&#39;s promoting his own SlimFly
      topology but generally I agree with his conclusions.<br /><br />I looked into
      a Red Rock Canyon torus design in 2015-2016 and declined to implement it due
      to cost of optics and poor scalability of bisection bandwidth. Ultimately I
      don&#39;t think a switch-per-server design will ever make sense, especially
      considering the increasing throughput and radius of merchant switch ASICs. Rack-level
      direct switching topologies (like Jellyfish and SlimFly above) make more sense
      but there&#39;s a question of whether the routing complexity is worth the cost
      savings.<br /><br />I don&#39;t think the Silicon Photonics Rapture is coming
      for the same reason that flash won&#39;t replace hard disks: if SiPh becomes
      briefly cheaper than VCSELs the surge in demand would push the price back up
      to parity. At best SiPh will reach parity and then both technologies will reduce
      costs over time, maybe with the mix slowly shifting from one to another.'
    id: '4426299312644170174'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Wes Felter
    profile: https://www.blogger.com/profile/01395217775195260835
    pub: '2019-03-06T20:09:22.430+01:00'
    ref: '1450447805887421127'
    type: comment
  - date: 07 March 2019 18:30
    html: 'Wes, nailed it ;-) '
    id: '1380152687237343970'
    image: https://lh6.googleusercontent.com/-W5FLdEJA7TI/AAAAAAAAAAI/AAAAAAAAACI/zamqlhAD0WQ/s32-c/photo.jpg
    name: Tony Przygienda
    profile: https://www.blogger.com/profile/07732539097585801151
    pub: '2019-03-07T18:30:52.993+01:00'
    ref: '1450447805887421127'
    type: comment
  date: 06 March 2019 10:16
  html: Intel pivoted Uri&#39;s Fulcrum work (Mellanox&#39;s Ethernet switch is lower
    latency btw) to Red Rock Canyon, which would have been an awesome tray-level (4
    NICs plus a modest radix switch) torus/hypercube type design (no need for leaf
    and spine switches in data center) if just the physical layer had ended up cheap
    enough for the torus solution to be cheaper than leaf/spine fat tree at the row
    level.<br /><br />Red Rock Canyon was, best I can tell, repositioned as a value
    add NIC (after all what NIC has a full switch packet processing pipeline available).<br
    /><br />And do not forget that when optics directly from the big CMOS ASICs (manufacturable
    at scale) happens in the 2020s, the economics of those torus-etc designs will
    be better than leaf/spine for configurations like hyperscale (bought and installed
    rows at a time and not upgraded or reconfigured).
  id: '1450447805887421127'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Steve Chalmers
  profile: https://www.blogger.com/profile/03172563417086934763
  pub: '2019-03-06T10:16:45.252+01:00'
  ref: '8717807261811015623'
  type: comment
- date: 06 March 2019 19:43
  html: FM6000 probably could have beaten Broadcom Trident if it was released on time.
    For some reason, possibly related to the acquisition, FM6000 was massively delayed
    so all the switch vendors used Trident instead. That 2010-2011 time period was
    a real inflection point for merchant silicon and Intel missed it. Then the FM10000
    (Red Rock Canyon) was targeted at a niche market which turned out to not exist
    and that&#39;s all she wrote.<br /><br />Intel was a leader in 10G NICs and they
    threw that away by not releasing mainstream 25/50/100G NICs; this may be tied
    up in their 10 nm problem. Now Mellanox has 75% NIC market share and there are
    rumors about Intel buying them.<br /><br />Omni-Path appears to be derived from
    Cray and QLogic Infiniband technology and probably has nothing to do with Fulcrum.
    I would not recommend using Infiniband/Omni-Path outside of HPC now that Ethernet
    is the same speed.
  id: '3294668956376827525'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Wes Felter
  profile: https://www.blogger.com/profile/01395217775195260835
  pub: '2019-03-06T19:43:51.517+01:00'
  ref: '8717807261811015623'
  type: comment
- date: 06 March 2019 20:28
  html: It&#39;s fascinating to see how networking folks are being driven by requirements
    (at least on IP fabrics) that start to resemble NUMA more and more (i.e. uniform
    low delay, zero loss) while in reality non-uniform addressing, scale, cabling
    &amp; connector length/density/cost is preventing them from re-using the usual
    NUMA architectures (hypercubes, meshes, draconoid flavors ;-) The promise of NUMA
    never extended beyond few racks AFAIK with all the Flexi, extended PCI and so
    on ... Infiniband was driven hard (and made work over long distances long time
    ago) but for some reason never really took of en masse, I suspect cost vs. dirt-cheap-packet-tech
    a.k.a. as Ethernet ... Same battle as messaging vs. distributed memory distributed
    architecture approaches where the outcome was pretty counter-intuitive ;-)
  id: '5149478493376772806'
  image: https://lh6.googleusercontent.com/-W5FLdEJA7TI/AAAAAAAAAAI/AAAAAAAAACI/zamqlhAD0WQ/s32-c/photo.jpg
  name: Tony Przygienda
  profile: https://www.blogger.com/profile/07732539097585801151
  pub: '2019-03-06T20:28:27.341+01:00'
  ref: '8717807261811015623'
  type: comment
count: 5
id: '8717807261811015623'
type: post
url: 2019/03/anyone-using-intel-omni-path.html
