<div class="comments post" id="comments">
  <h4>5 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1450447805887421127">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03172563417086934763" rel="nofollow">Steve Chalmers</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1450447805887421127" href="#1450447805887421127">06 March 2019 10:16</a>
              </span>
            </div>
            <div class="comment-content">Intel pivoted Uri&#39;s Fulcrum work (Mellanox&#39;s Ethernet switch is lower latency btw) to Red Rock Canyon, which would have been an awesome tray-level (4 NICs plus a modest radix switch) torus/hypercube type design (no need for leaf and spine switches in data center) if just the physical layer had ended up cheap enough for the torus solution to be cheaper than leaf/spine fat tree at the row level.<br /><br />Red Rock Canyon was, best I can tell, repositioned as a value add NIC (after all what NIC has a full switch packet processing pipeline available).<br /><br />And do not forget that when optics directly from the big CMOS ASICs (manufacturable at scale) happens in the 2020s, the economics of those torus-etc designs will be better than leaf/spine for configurations like hyperscale (bought and installed rows at a time and not upgraded or reconfigured).</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4426299312644170174">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01395217775195260835" rel="nofollow">Wes Felter</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4426299312644170174" href="#4426299312644170174">06 March 2019 20:09</a>
              </span>
            </div>
            <div class="comment-content">For people who are interested in learning about non-Clos topologies I recommend this presentation: https://www.microsoft.com/en-us/research/video/network-topologies-for-large-scale-datacenters-its-the-diameter-stupid/ Obviously it&#39;s somewhat biased since he&#39;s promoting his own SlimFly topology but generally I agree with his conclusions.<br /><br />I looked into a Red Rock Canyon torus design in 2015-2016 and declined to implement it due to cost of optics and poor scalability of bisection bandwidth. Ultimately I don&#39;t think a switch-per-server design will ever make sense, especially considering the increasing throughput and radius of merchant switch ASICs. Rack-level direct switching topologies (like Jellyfish and SlimFly above) make more sense but there&#39;s a question of whether the routing complexity is worth the cost savings.<br /><br />I don&#39;t think the Silicon Photonics Rapture is coming for the same reason that flash won&#39;t replace hard disks: if SiPh becomes briefly cheaper than VCSELs the surge in demand would push the price back up to parity. At best SiPh will reach parity and then both technologies will reduce costs over time, maybe with the mix slowly shifting from one to another.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1380152687237343970">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07732539097585801151" rel="nofollow">Tony P</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1380152687237343970" href="#1380152687237343970">07 March 2019 18:30</a>
              </span>
            </div>
            <div class="comment-content">Wes, nailed it ;-) </div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3294668956376827525">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01395217775195260835" rel="nofollow">Wes Felter</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3294668956376827525" href="#3294668956376827525">06 March 2019 19:43</a>
              </span>
            </div>
            <div class="comment-content">FM6000 probably could have beaten Broadcom Trident if it was released on time. For some reason, possibly related to the acquisition, FM6000 was massively delayed so all the switch vendors used Trident instead. That 2010-2011 time period was a real inflection point for merchant silicon and Intel missed it. Then the FM10000 (Red Rock Canyon) was targeted at a niche market which turned out to not exist and that&#39;s all she wrote.<br /><br />Intel was a leader in 10G NICs and they threw that away by not releasing mainstream 25/50/100G NICs; this may be tied up in their 10 nm problem. Now Mellanox has 75% NIC market share and there are rumors about Intel buying them.<br /><br />Omni-Path appears to be derived from Cray and QLogic Infiniband technology and probably has nothing to do with Fulcrum. I would not recommend using Infiniband/Omni-Path outside of HPC now that Ethernet is the same speed.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5149478493376772806">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07732539097585801151" rel="nofollow">Tony P</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5149478493376772806" href="#5149478493376772806">06 March 2019 20:28</a>
              </span>
            </div>
            <div class="comment-content">It&#39;s fascinating to see how networking folks are being driven by requirements (at least on IP fabrics) that start to resemble NUMA more and more (i.e. uniform low delay, zero loss) while in reality non-uniform addressing, scale, cabling &amp; connector length/density/cost is preventing them from re-using the usual NUMA architectures (hypercubes, meshes, draconoid flavors ;-) The promise of NUMA never extended beyond few racks AFAIK with all the Flexi, extended PCI and so on ... Infiniband was driven hard (and made work over long distances long time ago) but for some reason never really took of en masse, I suspect cost vs. dirt-cheap-packet-tech a.k.a. as Ethernet ... Same battle as messaging vs. distributed memory distributed architecture approaches where the outcome was pretty counter-intuitive ;-)</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
