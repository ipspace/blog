comments:
- comments:
  - date: 30 September 2019 07:39
    html: Firewall and storage techies have problems around understanding many basic
      networking concepts.  Strange...
    id: '6067695334796498755'
    image: https://2.bp.blogspot.com/-hwvTR2UGngM/XIThML0bLJI/AAAAAAAAAHs/KQAKCSF3hDkUZGhNpVFkHQx8dCrQJy2cACK4BGAYYCw/s32/1_Amastelek_Image_Main_Colour.png
    name: Amastelek Technologies
    profile: https://www.blogger.com/profile/01516467255777865408
    pub: '2019-09-30T07:39:34.359+02:00'
    ref: '2129449723933838974'
    type: comment
  date: 12 September 2019 17:12
  html: Once I was tasked to do a DR test before handing over the solution to the
    customer. To simulate the loss of a data center I suggested to physically shutdown
    all core switches in the active data center. A word and a blow. Spanning tree
    converged pretty fast. The stretched VLANs were all functional on the DR site.
    But there were split brain scenarios on firewalls all over the place. Even worse
    the storage (iSCSI) didn&#39;t survive because it couldn&#39;t build the quorum
    and so the storage wasn&#39;t accessible on the DR site. After some minutes no
    machines (also virtual load balancers) were reachable. Also vCenter wasn&#39;t
    reachable any more and so they couldn&#39;t do anything. After bringing the core
    switches back online it took them hours to recover from the mess.<br />They never
    fixed the broken architecture (firewall/load balancer cluster and iSCSI storage).<br
    />On the second attempt they faked disaster recovery tests like you were describing
    by carefully moving some machines to the DR site but did not failover firewalls
    and load balancers.<br />In the internal review discussion they had to admit that
    the network failed over as expected and in a reasonable amount of time.<br />Most
    everyone these days is relying on 100 % network availability. It&#39;s a fallacy.
    Sometimes they have to learn it the hard way.
  id: '2129449723933838974'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Anonymous
  profile: https://www.blogger.com/profile/17892204563666956100
  pub: '2019-09-12T17:12:10.348+02:00'
  ref: '859802649301491730'
  type: comment
- comments:
  - date: 14 September 2019 08:50
    html: "I have nothing against doing tests within maintenance windows as long as\
      \ everyone is aware that there\u2019s room for improvement if we\u2019re not\
      \ confident enough to pull the plug no matter when... because the disaster won\u2019\
      t wait for the next scheduled maintenance window."
    id: '5854226951775417066'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2019-09-14T08:50:36.739+02:00'
    ref: '3132369027912567025'
    type: comment
  date: 13 September 2019 16:25
  html: While I agree with the rest of your article and have faced similar issues
    myself, I disagree with the notion that doing your DR testing during a maintenance
    window is a bad thing.  If your goal is to learn what systems break and you&#39;re
    fairly certain that SOME things will break, why risk corporate reputation or revenue
    during the test?<br /><br />Maybe eventually once you&#39;ve identified what you
    think are all the issues and you&#39;re ready to do a full live production DR
    test, you should do it during the middle of the day, but I&#39;d argue against
    it initially and as significant infrastructure changes are made.<br /><br />Thoughts?  Did
    I just misunderstand your angle on this?
  id: '3132369027912567025'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Avery Abbott
  profile: https://www.blogger.com/profile/02172854860991897596
  pub: '2019-09-13T16:25:12.410+02:00'
  ref: '859802649301491730'
  type: comment
- date: 30 September 2019 07:36
  html: Most people cover up any DR failings.  Azure failed and much was said about
    the data center cooling and power but not a word about why the geographic redundancy
    did not work.
  id: '3982825554985859240'
  image: https://2.bp.blogspot.com/-hwvTR2UGngM/XIThML0bLJI/AAAAAAAAAHs/KQAKCSF3hDkUZGhNpVFkHQx8dCrQJy2cACK4BGAYYCw/s32/1_Amastelek_Image_Main_Colour.png
  name: Amastelek Technologies
  profile: https://www.blogger.com/profile/01516467255777865408
  pub: '2019-09-30T07:36:40.152+02:00'
  ref: '859802649301491730'
  type: comment
count: 5
id: '859802649301491730'
type: post
url: 2019/09/disaster-recovery-test-faking-another.html
