{
  "comments": [
    {
      "comments": [
        {
          "date": "30 September 2019 07:39",
          "html": "Firewall and storage techies have problems around understanding many basic networking concepts.  Strange...",
          "id": "6067695334796498755",
          "image": "https://2.bp.blogspot.com/-hwvTR2UGngM/XIThML0bLJI/AAAAAAAAAHs/KQAKCSF3hDkUZGhNpVFkHQx8dCrQJy2cACK4BGAYYCw/s32/1_Amastelek_Image_Main_Colour.png",
          "name": "Amastelek Technologies",
          "profile": "https://www.blogger.com/profile/01516467255777865408",
          "pub": "2019-09-30T07:39:34.359+02:00",
          "ref": "2129449723933838974",
          "type": "comment"
        }
      ],
      "date": "12 September 2019 17:12",
      "html": "Once I was tasked to do a DR test before handing over the solution to the customer. To simulate the loss of a data center I suggested to physically shutdown all core switches in the active data center. A word and a blow. Spanning tree converged pretty fast. The stretched VLANs were all functional on the DR site. But there were split brain scenarios on firewalls all over the place. Even worse the storage (iSCSI) didn&#39;t survive because it couldn&#39;t build the quorum and so the storage wasn&#39;t accessible on the DR site. After some minutes no machines (also virtual load balancers) were reachable. Also vCenter wasn&#39;t reachable any more and so they couldn&#39;t do anything. After bringing the core switches back online it took them hours to recover from the mess.<br />They never fixed the broken architecture (firewall/load balancer cluster and iSCSI storage).<br />On the second attempt they faked disaster recovery tests like you were describing by carefully moving some machines to the DR site but did not failover firewalls and load balancers.<br />In the internal review discussion they had to admit that the network failed over as expected and in a reasonable amount of time.<br />Most everyone these days is relying on 100 % network availability. It&#39;s a fallacy. Sometimes they have to learn it the hard way.",
      "id": "2129449723933838974",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Anonymous",
      "profile": "https://www.blogger.com/profile/17892204563666956100",
      "pub": "2019-09-12T17:12:10.348+02:00",
      "ref": "859802649301491730",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "14 September 2019 08:50",
          "html": "I have nothing against doing tests within maintenance windows as long as everyone is aware that there\u2019s room for improvement if we\u2019re not confident enough to pull the plug no matter when... because the disaster won\u2019t wait for the next scheduled maintenance window.",
          "id": "5854226951775417066",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2019-09-14T08:50:36.739+02:00",
          "ref": "3132369027912567025",
          "type": "comment"
        }
      ],
      "date": "13 September 2019 16:25",
      "html": "While I agree with the rest of your article and have faced similar issues myself, I disagree with the notion that doing your DR testing during a maintenance window is a bad thing.  If your goal is to learn what systems break and you&#39;re fairly certain that SOME things will break, why risk corporate reputation or revenue during the test?<br /><br />Maybe eventually once you&#39;ve identified what you think are all the issues and you&#39;re ready to do a full live production DR test, you should do it during the middle of the day, but I&#39;d argue against it initially and as significant infrastructure changes are made.<br /><br />Thoughts?  Did I just misunderstand your angle on this?",
      "id": "3132369027912567025",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Avery Abbott",
      "profile": "https://www.blogger.com/profile/02172854860991897596",
      "pub": "2019-09-13T16:25:12.410+02:00",
      "ref": "859802649301491730",
      "type": "comment"
    },
    {
      "date": "30 September 2019 07:36",
      "html": "Most people cover up any DR failings.  Azure failed and much was said about the data center cooling and power but not a word about why the geographic redundancy did not work.",
      "id": "3982825554985859240",
      "image": "https://2.bp.blogspot.com/-hwvTR2UGngM/XIThML0bLJI/AAAAAAAAAHs/KQAKCSF3hDkUZGhNpVFkHQx8dCrQJy2cACK4BGAYYCw/s32/1_Amastelek_Image_Main_Colour.png",
      "name": "Amastelek Technologies",
      "profile": "https://www.blogger.com/profile/01516467255777865408",
      "pub": "2019-09-30T07:36:40.152+02:00",
      "ref": "859802649301491730",
      "type": "comment"
    }
  ],
  "count": 5,
  "id": "859802649301491730",
  "type": "post",
  "url": "2019/09/disaster-recovery-test-faking-another.html"
}