{
   "comments": [
      {
         "comments": [
            {
               "date": "18 May 2024 07:58",
               "html": "<p>You&#39;re absolutely right and I would never recommend anything else (see https://blog.ipspace.net/2013/08/virtual-appliance-routing-network.html).</p>\n\n<p>Thanks a million for pointing out another detail I keep forgetting.</p>\n",
               "id": "2264",
               "name": "Ivan Pepelnjak",
               "pub": "2024-05-18T07:58:31",
               "ref": "2261",
               "type": "comment"
            },
            {
               "date": "19 May 2024 12:13",
               "html": "<p>o, no... now all our networking is in different ADs. how could we live in even single AS with different routing domains?</p>\n",
               "id": "2265",
               "name": "andrii oliinyk",
               "pub": "2024-05-19T12:13:17",
               "ref": "2264",
               "type": "comment"
            },
            {
               "date": "19 May 2024 04:40",
               "html": "<p>&gt; how could we live in even single AS with different routing domains?</p>\n\n<p>I&#39;ll start off with:\nI do not personally agree with the eBGP design described in RFC7938 (eBGP everywhere forever/always).</p>\n\n<p>However, I&#39;ve worked in a large DC network, where this &ldquo;segregation&rdquo; between net eng and &ldquo;server&rdquo; people existed, politics was not particularly terrible, but ran into it a few times.</p>\n\n<p>We used an eBGP-driven network design and the &ldquo;AD&rdquo; (in the layer 8 sense + configuration sense, not routing protocol sense) is a single domain from the perspective of the NOC team, from the perspective of C-suite and from the perspective of our adjacent system ops team. ASN numbering was pre-defined, it is eBGP all the way down, iBGP for adjacent neighbours intra-rack over an LACP bonding iface (or Aggregated Ethernet as Juniper calls it), OSPF (or is-is) for inter-rack or inter-site iBGP overlay. No MC-LAG/Layer 2 spanning/stacking/anything nonsense anywhere, no route reflectors and <em>no</em> iBGP full-mesh either. 100% layer 3-only networking.</p>\n\n<p>Each network device (everything is 100% routing, there&#39;s no layer 2-only-switching device) only had a default route upwards from the bottom, routes were cleanly aggregated with blackholes on the way down, always (our IPAM, was IPv6-heavy, and it was based on a version that I created, that&#39;s similar to what I wrote in my IPv6 Architecture blog post). The concept of a million of routes flooding our core routers (which are downstream of edge routers), Spine/Leaf (routed layer 3) was NIL, each device at most only had a few routes (less than a 100 or so because of the default route for egress back up), Leaf switches with N number of ports, only had N number of /64s for the link-interface addressing etc.</p>\n\n<p>It in fact simplified configuration and traffic engineering, because we can now deploy ECMP/UCMP network-wide up-to the host itself (the physical server hypervisor ran eBGP with FRR and peers with the Leaf switch) with BGP multipathing. Augment it with pre-defined custom BGP communities, and you&#39;re golden for ensuring all links are evenly saturated most of the time.</p>\n\n<p>OSPF/is-is simply can&#39;t do advance and complex traffic engineering, granular route filtering, injection of business policy etc to the level BGP can. IMO, BGP is a policy-driven routing protocol vs link-state IGPs.</p>\n\n<p>Unfortunately, there&#39;s no public documentation on this <em>specific</em> eBGP design (that does not match RFC7938 nor does it match a clos topology, I know I mentioned spine/leaf, but we interconnected them to each other which is, to my knowledge, not a feature of clos topology and very different paradigm). I&#39;ve been doing my part in sharing my knowledge on this specific design when I get some bandwidth.</p>\n\n<p>You can find some examples below of the specific design I tried my best to describe in this ultra-short comment, the very same design was used for Ceph networking (no layer 2 nonsense, 100% layer 3-only):\nhttps://blog.widodh.nl/2024/05/using-l3-bgp-routing-for-your-ceph-storage/</p>\n\n<p>I successfully ported this eBGP-driven DC networking to SP networks as well, talked a bit about it below:\nhttps://blog.ipspace.net/2024/04/repost-ebgp-only-sp-network.html</p>\n\n<p>I do intend to write properly on the eBGP-driven SP network design in the future, when my bandwidth permits.</p>\n",
               "id": "2267",
               "name": " Daryll Swer",
               "pub": "2024-05-19T16:40:48",
               "ref": "2265",
               "type": "comment"
            }
         ],
         "date": "17 May 2024 01:26",
         "html": "<p>As far as I understood, sometimes BGP is preferred over OSPF because of administrative domain separation. Usually, the people managing the hosts and the people managing the network are in different departments with a lot of nasty politics involved.\nBGP gives you more control. A single OSPF domain creates too much interdependency between those departments.</p>\n",
         "id": "2261",
         "name": "Bela Varkonyi",
         "pub": "2024-05-17T13:26:45",
         "type": "comment"
      }
   ],
   "count": 1,
   "type": "post",
   "url": "2024/05/worth-reading-ospf-protocol-analysis.html"
}
