<div class="comments post" id="comments">
  <h4>1 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2261">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Bela Varkonyi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2261" href="#2261">17 May 2024 01:26</a>
              </span>
            </div>
            <div class="comment-content"><p>As far as I understood, sometimes BGP is preferred over OSPF because of administrative domain separation. Usually, the people managing the hosts and the people managing the network are in different departments with a lot of nasty politics involved.
BGP gives you more control. A single OSPF domain creates too much interdependency between those departments.</p>
</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="2264">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2264" href="#2264">18 May 2024 07:58</a>
              </span>
            </div>
            <div class="comment-content"><p>You&#39;re absolutely right and I would never recommend anything else (see https://blog.ipspace.net/2013/08/virtual-appliance-routing-network.html).</p>

<p>Thanks a million for pointing out another detail I keep forgetting.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2265">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">andrii oliinyk</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2265" href="#2265">19 May 2024 12:13</a>
              </span>
            </div>
            <div class="comment-content"><p>o, no... now all our networking is in different ADs. how could we live in even single AS with different routing domains?</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2267">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Daryll Swer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2267" href="#2267">19 May 2024 04:40</a>
              </span>
            </div>
            <div class="comment-content"><p>&gt; how could we live in even single AS with different routing domains?</p>

<p>I&#39;ll start off with:
I do not personally agree with the eBGP design described in RFC7938 (eBGP everywhere forever/always).</p>

<p>However, I&#39;ve worked in a large DC network, where this &ldquo;segregation&rdquo; between net eng and &ldquo;server&rdquo; people existed, politics was not particularly terrible, but ran into it a few times.</p>

<p>We used an eBGP-driven network design and the &ldquo;AD&rdquo; (in the layer 8 sense + configuration sense, not routing protocol sense) is a single domain from the perspective of the NOC team, from the perspective of C-suite and from the perspective of our adjacent system ops team. ASN numbering was pre-defined, it is eBGP all the way down, iBGP for adjacent neighbours intra-rack over an LACP bonding iface (or Aggregated Ethernet as Juniper calls it), OSPF (or is-is) for inter-rack or inter-site iBGP overlay. No MC-LAG/Layer 2 spanning/stacking/anything nonsense anywhere, no route reflectors and <em>no</em> iBGP full-mesh either. 100% layer 3-only networking.</p>

<p>Each network device (everything is 100% routing, there&#39;s no layer 2-only-switching device) only had a default route upwards from the bottom, routes were cleanly aggregated with blackholes on the way down, always (our IPAM, was IPv6-heavy, and it was based on a version that I created, that&#39;s similar to what I wrote in my IPv6 Architecture blog post). The concept of a million of routes flooding our core routers (which are downstream of edge routers), Spine/Leaf (routed layer 3) was NIL, each device at most only had a few routes (less than a 100 or so because of the default route for egress back up), Leaf switches with N number of ports, only had N number of /64s for the link-interface addressing etc.</p>

<p>It in fact simplified configuration and traffic engineering, because we can now deploy ECMP/UCMP network-wide up-to the host itself (the physical server hypervisor ran eBGP with FRR and peers with the Leaf switch) with BGP multipathing. Augment it with pre-defined custom BGP communities, and you&#39;re golden for ensuring all links are evenly saturated most of the time.</p>

<p>OSPF/is-is simply can&#39;t do advance and complex traffic engineering, granular route filtering, injection of business policy etc to the level BGP can. IMO, BGP is a policy-driven routing protocol vs link-state IGPs.</p>

<p>Unfortunately, there&#39;s no public documentation on this <em>specific</em> eBGP design (that does not match RFC7938 nor does it match a clos topology, I know I mentioned spine/leaf, but we interconnected them to each other which is, to my knowledge, not a feature of clos topology and very different paradigm). I&#39;ve been doing my part in sharing my knowledge on this specific design when I get some bandwidth.</p>

<p>You can find some examples below of the specific design I tried my best to describe in this ultra-short comment, the very same design was used for Ceph networking (no layer 2 nonsense, 100% layer 3-only):
https://blog.widodh.nl/2024/05/using-l3-bgp-routing-for-your-ceph-storage/</p>

<p>I successfully ported this eBGP-driven DC networking to SP networks as well, talked a bit about it below:
https://blog.ipspace.net/2024/04/repost-ebgp-only-sp-network.html</p>

<p>I do intend to write properly on the eBGP-driven SP network design in the future, when my bandwidth permits.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
