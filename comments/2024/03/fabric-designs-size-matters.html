<div class="comments post" id="comments">
  <h4>2 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2132">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Daniel S</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2132" href="#2132">15 March 2024 08:45</a>
              </span>
            </div>
            <div class="comment-content"><p>Absolutly right. I&#39;m working at a MSP and we do a lot of project work for enterprises which are between 500 and 2000 people. That means the IT department is not that big, it&#39;s usally just a costcenter for them. </p>

<p>Then, some one of us comes in (often the presales guys first) and make suggestions on what new shiny thing they could buy. ACI for your DC that doesn&#39;t change at all! NSX for your 4 VMware servers and you can save on networking! VXLAN EVPN for your 4 networking guys that do mostly client patching (the cables), never heard of an overlay and didn&#39;t touch BGP in their lives! YAY! 
Don&#39;t get me started on the whole automation stuff that is out there. Most companies are way in over their head with a lot of that stuff. And it all comes crashing down if the wrong guy leaves for another better paying job. </p>

<p>Be realistic and consult (you MSP guys) and buy (you enterprise guys) the right tool for your team that meets your needs. Maybe two or four switches with MLAG/vPC is not cool and shiny but it gets the job done and you can troubleshoot it at 2 in the night. Overlays in a campus are really practical and can help solve a lot of &quot;issues&quot; (mostly issues that are caused by someone who doesn&#39;t want or doesn&#39;t know how to do it better). But can you support it? As we started with campus fabrics (espacially Ciscos SDA) most customers where shocked to learn about dot1x and what it means to roll that out. And thats just the start of all that. We always recommend dot1x for different reasons, but most companies can&#39;t handle that on their own. Those are all the 99%. </p>

<p>And what are we all talking about? The new stuff for the 1%. And it takes years to see that trickle down to the 99%. </p>

<p>Sorry for the rant, but my 2cents. </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2133">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Bram</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2133" href="#2133">15 March 2024 08:54</a>
              </span>
            </div>
            <div class="comment-content"><p>With current compute density exceeding 50+ CPU cores and 1TB+ memory per 1-2RU servers the amount of pizza boxes required has gone down drastically. This impacts resiliency.
In the past it was possible to create a N+1 design where a complete rack with a dozen virtualization hosts and associated TOR switches could go down (physical issues / TOR maintenance).
Nowadays you can run the same virtual workloads on less hardware, so in the average enterprise datacenter you are down to just a handful of switches, which become more mission critical. 
It becomes even worse if those &lsquo;average&rsquo; enterprises decide to go for a blade chassis and only offer 2-4 100G uplinks towards your network.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
