{
  "comments": [
    {
      "comments": [
        {
          "date": "15 May 2018 15:46",
          "html": "I know. Daniel Dib sent me a tweet before Randy even started ;) Randy was talking about an order of magnitude (or more) larger data centers, and set the IGP scalability limit to ~500 nodes (so I&#39;m way more conservative than he is ;).",
          "id": "762155913059882118",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2018-05-15T15:46:14.094+02:00",
          "ref": "2743823771628885090",
          "type": "comment"
        }
      ],
      "date": "15 May 2018 13:48",
      "html": "hi,<br /><br />just yesterday randy bush at RIPE76:<br />https://ripe76.ripe.net/wp-content/uploads/presentations/30-180514.ripe-clos.pdf<br /><br />thank you<br />--<br />antonio",
      "id": "2743823771628885090",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Antonio Prado",
      "profile": "https://www.prado.it",
      "pub": "2018-05-15T13:48:04.552+02:00",
      "ref": "76276412074987350",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "15 May 2018 21:00",
          "html": "Yep, I was just like you when I was your age. Then reality intervened ;)",
          "id": "1500020445850595647",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2018-05-15T21:00:05.201+02:00",
          "ref": "8491255615636817250",
          "type": "comment"
        }
      ],
      "date": "15 May 2018 18:10",
      "html": "I just finished my homework so I&#39;m not busy anymore. OSPF or IS-IS in the fabric is something that old grumpy greyed half-baldies use because it worked for centuries (and to change something means a lot of work). If you really want to be on the cutting edge you program your switch asics (FPGA) yourself with a programming language (probably P4). Also you would develop your own directory service for reachability information distribution (and you also have the invent your own UDP encapsulation but that&#39;s the easiest part). So with your own solution you would have a huge competitive advantage.",
      "id": "8491255615636817250",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2018-05-15T18:10:46.717+02:00",
      "ref": "76276412074987350",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "15 May 2018 21:03",
          "html": "Agree. I just wanted to point out that if you&#39;re dealing with nails, sometimes a small hammer is good enough (no need to invest in a fancy multi-tool ;).",
          "id": "110811646056207634",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2018-05-15T21:03:47.828+02:00",
          "ref": "8562022858829573575",
          "type": "comment"
        },
        {
          "date": "16 May 2018 00:14",
          "html": "An interesting discussion is also whether the IP control plane will extend &quot;down&quot; to the servers which seriously stretches any current scalability assumptions. There are lots pluses to that IMO if you can manage the scale and need IP multi-homing and/or simpler hypervisor integration. Because it&#39;s currently not being done often/considered feasible/being solved over MC-LAG does not mean it can&#39;t be done ;-)",
          "id": "8349831704360886614",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Tony P",
          "profile": "https://www.blogger.com/profile/07732539097585801151",
          "pub": "2018-05-16T00:14:13.488+02:00",
          "ref": "8562022858829573575",
          "type": "comment"
        },
        {
          "date": "19 May 2018 17:23",
          "html": "&quot;An interesting discussion is also whether the IP control plane will extend &quot;down&quot; to the servers&quot; &lt;&lt; in case I&#39;d be running BGP no matter what, as I believe servers and networking gear shouldn&#39;t be in the same trust (and flooding) domain. <br /><br />Whether I&#39;d do it in combination with IGP (assuming the fabric is small enough for that) or go for BGP-only fabric would depend only on what&#39;s easiest to do on the gear the customer wants to use.",
          "id": "5288212018106262340",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2018-05-19T17:23:17.609+02:00",
          "ref": "8562022858829573575",
          "type": "comment"
        }
      ],
      "date": "15 May 2018 18:19",
      "html": "Some additional thoughts:<br />* ISIS/OSPF scales actually to something more like 3K in very good implementations (on a sparse mesh) but other problems than scalability become relevant most of the time before this number is hit<br />* Limiting scalability IGP factor IME is not really &quot;switches&quot;, limiting factor is how much and how many links you have to flood out &amp; process flooding on so the #switches is an easily understood but not so meaningful number<br />* some mobility/container architectures I see talking about exceed your 30-50 numbers ;-) and the lifetimes talked about seem to put the assumption of &quot;not much ever changes&quot; in question a bit<br />* Generally, I end up in many more discussions about ZTP than &quot;scalability&quot; when discussing the IP fabric problems with the relevant parties. ZTP is boring of course but seems operationally much more of a pain-point though talking scalability is much cooler of course ;-)  <br /><br />And ultimately, one flavor does not fit all tastes, especially in networking ... <br /><br /><br />",
      "id": "8562022858829573575",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Tony P",
      "profile": "https://www.blogger.com/profile/07732539097585801151",
      "pub": "2018-05-15T18:19:11.574+02:00",
      "ref": "76276412074987350",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "18 May 2018 15:21",
          "html": "Right, and I don&#39;t understand what&#39;s wrong with EBGP everywhere (overlay/underlay) - lowering default timers and/or BFD solves convergence concerns, there certainly aren&#39;t scale issues (hello Internet), as you mention above the flexible policy control, and one protocol to know/troubleshoot is certainly better than many so why are people (Ivan) holding on to IGP&#39;s at all? ",
          "id": "7081343293529653361",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Anonymous",
          "profile": null,
          "pub": "2018-05-18T15:21:49.171+02:00",
          "ref": "9030615670583779712",
          "type": "comment"
        },
        {
          "date": "18 May 2018 15:47",
          "html": "Explore my blog a bit more, and you might find that I have nothing against using BGP in the data center.<br /><br />However, once vendors start promoting overlay IBGP or EBGP (between loopbacks) over underlay EBGP (between directly-connected interfaces) to implement EVPN, it&#39;s time to say &quot;if you can&#39;t fix your stuff so I&#39;m able to use a simple design like EBGP between directly-connected interfaces for underlay and overlay, then maybe I should move back to IBGP-over-IGP&quot;<br /><br />Also, the point of this blog post was &quot;don&#39;t say stupid things like IGP can&#39;t scale in an average data center, because it can&quot; not &quot;don&#39;t use BGP&quot;",
          "id": "5122728752565468109",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2018-05-18T15:47:01.555+02:00",
          "ref": "9030615670583779712",
          "type": "comment"
        },
        {
          "date": "22 May 2018 13:52",
          "html": "&quot;&quot;if you can&#39;t fix your stuff so I&#39;m able to use a simple design like EBGP between directly-connected interfaces for underlay and overlay, then maybe I should move back to IBGP-over-IGP&quot;&quot; &lt;&lt;&lt; <br />Can you point to any specific vendor that &quot;promoting overlay IBGP or EBGP (between loopbacks) over underlay EBGP (between directly-connected interfaces) to implement EVPN&quot;  and can&#39;t do &quot;simple design like EBGP between directly-connected interfaces for underlay and overlay&quot;?<br />Maybe they promote this more complex design simply because they really believe in its value? But no, they should be blamed, because they simply provide you with more options.<br />On the other hand, I can point to vendors that can&#39;t implement this more complex designs yet, and therefore promote their &quot;simple and perfect&quot; solution (and politely do not mention it&#39;s restrictions).",
          "id": "1068230996783408874",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Alex",
          "profile": "https://www.blogger.com/profile/00644778105219383913",
          "pub": "2018-05-22T13:52:29.459+02:00",
          "ref": "9030615670583779712",
          "type": "comment"
        }
      ],
      "date": "15 May 2018 20:33",
      "html": "I thought the reason to choose BGP over OSPF/ISIS is not just for scalability but for easier, more flexible/extensible policy control,  easier integration with central controller -- whether you need those functionalities is a different story.",
      "id": "9030615670583779712",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/02692959370712732451",
      "pub": "2018-05-15T20:33:10.013+02:00",
      "ref": "76276412074987350",
      "type": "comment"
    },
    {
      "date": "15 May 2018 23:15",
      "html": "So here is the math for the 6144 ports mystery: We want to achieve a non-blocking spine layer, so each spine has 16 downstream and 16 upstream ports. So 32 spines get connected to 16 superspines. Every leaf has 4 uplinks and every leaf gets connected to 4 spines. So a group of 4 spines can take 16 leaves which makes 8 * 16 = 128 leaves. Now the calculation: 128 leaves x 48 ports = 6144 ports.<br /><br />And here is the math for the 144 switches with breakout cables (how nice is that) mystery: With breakout cables every leaf now has 4 times the uplinks so 16 uplinks per leaf. Each leaf gets connected to 16 spines. Each spine has 128 breakout ports (32 x 4). So 128 leaves x 16 uplinks = 16 spines x 128 downstream ports<br /><br />But remember ECMP isn&#39;t the same as load balancing. ECMP is just some sort of load distribution (based on a hashed function). As my namesake rightly said you have to program your asics in your switches with forwarding information to do proper load balancing. Otherwise your fabric doesn&#39;t perform well and your oversubscription gets even worse.",
      "id": "4983282662975449683",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2018-05-15T23:15:38.616+02:00",
      "ref": "76276412074987350",
      "type": "comment"
    },
    {
      "date": "21 May 2018 19:48",
      "html": "one benefit to is-is that i see is single topology ipv6.  there are certainly pros and cons there, but for our use case shared fate isn&#39;t a bad thing.  however, i chose bgp for my datacenter because of rfc 5549.  my internal links are all ipv6 only, but they transport the ipv4 from the edge servers fine with no tunnelling needed.",
      "id": "8747352080091884057",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2018-05-21T19:48:55.873+02:00",
      "ref": "76276412074987350",
      "type": "comment"
    }
  ],
  "count": 14,
  "id": "76276412074987350",
  "type": "post",
  "url": "2018/05/is-ospf-or-is-is-good-enough-for-my.html"
}