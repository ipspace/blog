<div class="comments post" id="comments">
  <h4>3 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="6295338417457619634">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6295338417457619634" href="#6295338417457619634">15 June 2018 10:36</a>
              </span>
            </div>
            <div class="comment-content">It&#39;s like a snake who bites its own tail. To solve problem you have to establish more TCP sessions than your competitors. So overall you would still be faster than the others. BTW I&#39;m glad to made in the news.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8639634158536451256">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11597478293075004448" rel="nofollow">sourcejedi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8639634158536451256" href="#8639634158536451256">19 June 2018 18:16</a>
              </span>
            </div>
            <div class="comment-content">Bad link.  HTTP/2 is not TCP-over-TCP.  The inner layer does not implement retransmissions.<br /><br />Good war story.  They&#39;ll have to wait for QUIC - as mnot pointed out in the second comment :).</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="6591863383595290428">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11597478293075004448" rel="nofollow">sourcejedi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6591863383595290428" href="#6591863383595290428">19 June 2018 18:44</a>
              </span>
            </div>
            <div class="comment-content">And the first comment is perhaps more relevant than I first thought.  Google say they use BBR congestion control for all WAN RPC.  Maybe that&#39;s one potential explanation for ending up with this situation in Envoy.  BBR can&#39;t solve head of line blocking, but it wouldn&#39;t have the second problem &quot;TCP window size will drop dramatically, and all streams will be simultaneously throttled down.&quot;  BBR does not use packet loss as a sign of congestion.  (Or, it has a heuristic to detect policers using packet loss, but they set the threshold for this to 20% :), so I would not expect it to trigger here.  You can see a bulk throughput v.s. loss graph here: https://queue.acm.org/detail.cfm?id=3022184 ).</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
