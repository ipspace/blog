<div class="comments post" id="comments">
  <h4>3 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="9027440098089258177">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9027440098089258177" href="#9027440098089258177">05 November 2018 08:38</a>
              </span>
            </div>
            <div class="comment-content">I took part (several times ) in product selection process to replace already existing solutions with new components (mostly due to End Of Life reasons).<br /><br />At the beginning we do select some products using manuals but almost always it did not work in our case. Sometimes because the vendor statements about performance require very specific test conditions (trivial examples: MTU size was much bigger than we used, signing algorithm was supported by HW vs what we needed which was handled by SW).<br /><br />Then we talk to the vendor to help us select the right hardware. This often helps us but in some cases it also fails. This time we fail because of the system dynamic / timing.<br />Some vendor hardware (general purpose hardware for the price we are willing to accept) is too slow to setup for example new FW session (I selected trivial example for simplicity; actually we have more subtle issues like cached packet reordering, etc).<br /><br />So we do more test upfront in the lab to identify most critical parts in the vendor product and we talk to the vendor again. <br />This part is critical because the vendor has to decide if he can fix the issue or enhance d the product within the current architecture, or we need to buy something more expensive.<br /><br />Critical because we risk the time spent on integration of the potentially working product before we are 100% sure that the potential fixes / enhancement will solve the performance problem on the selected architectures.<br /><br />It happens that vendor is wrong and cannot meet the expectation within the architecture and we need to repeat the process several times.<br /><br />I know our requirements are somewhat challenging because the product is specific (Mission Critical Voice Communication Systems).<br /><br />I want to say that for quite complex systems the manual might not be enough. The crucial part is the vendor involvement and willingness (and risk associated with this) because some especially timing critical requirements cannot be verified without heavy testing before product selection. And many times the vendor has to add new feature or tune performance of its product.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="7684734699436126072">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7684734699436126072" href="#7684734699436126072">05 November 2018 12:03</a>
              </span>
            </div>
            <div class="comment-content">As always, we&#39;re in perfect agreement. Unfortunately it&#39;s mandatory to do thorough testing for anything but trivial deployments, more so if your requirements deviate from the average expected by the vendors.<br /><br />What I wanted to point out was that having decent documentation is a mandatory prerequisite. If the $vendor can&#39;t get that act together, who knows what will happen when you hit the next roadblock.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3505706026238613515">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3505706026238613515" href="#3505706026238613515">05 November 2018 13:11</a>
              </span>
            </div>
            <div class="comment-content">Yes, sure. <br /><br />Funny story: When I reported an issue of packets reordering inside a box (two different vendors having parallel processing inside the box), both vendors answered immediately: it&#39;s impossible!. Yes, it was impossible according to the manual (and the officially available architecture details)!<br /><br />But both vendors fixed the issue because the issue was real!. It is all about the timing (time spent on FW session setup vs packet pace, talking about UDP of course).<br /><br />This case shows how the manual is only very first step, and how the architecture might mislead even the vendor. </div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
