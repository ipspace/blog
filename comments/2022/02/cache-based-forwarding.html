<div class="comments post" id="comments">
  <h4>4 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1044">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Jeroen van Bemmel</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1044" href="#1044">24 February 2022 03:10</a>
              </span>
            </div>
            <div class="comment-content"><p>This is how OpenVSwitch (https://blog.ipspace.net/2013/04/open-vswitch-under-hood.html) works, too: A first packet triggers a lookup and evaluation of ACLs, then the flow entry is cached for X seconds. ECMP forwarding is preserved because there is a &#39;multipath&#39; action that is evaluated for each packet; I know because I personally fixed the hashing (https://mail.openvswitch.org/pipermail/ovs-dev/2019-June/359489.html)</p>

<p>This implementation is not quite so &quot;obsolete&quot; as one might think; for example, 1000s of Nuage VRS/NSGs use it as we speak</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1048">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Bela Varkonyi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1048" href="#1048">25 February 2022 12:38</a>
              </span>
            </div>
            <div class="comment-content"><p>LISP is a more complex animal nowadays. Your knowledge of LISP seems to be quite outdated.</p>

<p>Nowadays it is used with reliable transport and full PubSub.
There is no caching behavior at all. Each xTR has a full routing table.</p>
</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1049">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1049" href="#1049">25 February 2022 03:58</a>
              </span>
            </div>
            <div class="comment-content"><p>Thanks for the comment. Updated the blog post accordingly.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1050">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Erik Auerswald</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1050" href="#1050">26 February 2022 02:03</a>
              </span>
            </div>
            <div class="comment-content"><p>In general, I support the main point of the blog post: cache based forwarding is a problematic scalability mechanism, i.e., scalability is limited and performance collapses beyond the limit.</p>

<p>Topology based forwarding has scalability limits, too. Once there are more routes than can fit the (hardware) forwarding tables, networking devices crash, fall back to CPU based forwarding (i.e., performance collapses), or deliver some packets to the wrong destination (I&#39;ve seen all of those&hellip;).</p>

<p>Firewalls, i.e., devices where network topology is just a small part of the forwarding decision, often employ flow caches (for individual data flows) for performance optimization. In some &quot;high-end&quot; firewalls this flow cache is offloaded to hardware. Since per flow offload is often the best such a device can use, it is done in practice, and usually works well enough. [A stateless packet filter in front of a stateful firewall can help staying inside the performance envelope of said firewall.]</p>

<p>Regarding networking devices primarily used for routing and bridging, the Enterasys Networks N-Series and their successors, K-Series and S-Series, based on CoreFlow resp. CoreFlow2 ASICs, used caching of individual flows in hardware forwarding tables as their only forwarding architecture. In practice, those switches worked well in the &quot;enterprise&quot; networks they were designed for. Of course it was possible to create overload with specific tests to demonstrate the potential for problems.</p>

<p>Since per flow hardware offload allowed implementing complex, but still high performance, traffic filtering policies, replacing CoreFlow(2) based devices with networking devices using topology based forwarding often provided challenges regarding traffic filtering policies (i.e., either keep complex line-rate traffic filters, or use a different networking device with faster interfaces and topology based forwarding, but not both).</p>

<p>I provide the above examples to illustrate the complexities involved. In theory, cache based forwarding does not scale. In practice, it may scale sufficiently, at least for a while (e.g., a decade inside an &quot;enterprise&quot; network before deploying the next device generation). It definitely breaks down when overloaded.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1051">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Henk</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1051" href="#1051">27 February 2022 05:13</a>
              </span>
            </div>
            <div class="comment-content"><p>&gt; we had to use router CPU to walk</p>

<p>I think you are mixing up two things. The forwarding entries had a simple combination of: (destination, outgoing interface, mac-address (actually encaps header)). So during packet-forwarding, no walk needed to be done. Just a simple lookup. The &quot;walking you are referring to happened during &quot;routing table maintenance&quot;. E.g. BGP walking its NLRI entries, to check if the nexthop still had the best IGP-metric, and whether nexthops were still valid. This was not done during route-lookup.</p>

<p>Two-stage lookups during forwarding were introduce later. Because of PIC. This allows the RIB to send 1 message to the FIB, and the FIB makes 1 adjustment, impacting many (possibly tens or hunderds of thousands of routes). The trade-off here is: slightly more complex lookup (2 stages) versus much quicker updating the FIB. Note, FIBs in hardware can do in the order of 10K+ changes per second. Not very fast.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
