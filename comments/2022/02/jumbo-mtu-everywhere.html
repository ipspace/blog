<div class="comments post" id="comments">
  <h4>3 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1028">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> darkfader</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1028" href="#1028">17 February 2022 11:53</a>
              </span>
            </div>
            <div class="comment-content"><p>&quot;fixed-size cells (64 bytes) to move the packets between forwarding engineers.&quot;</p>

<p>so, if we allow for an evil oversimplification, ATM actually never died but retreated into the brains?</p>
</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1029">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1029" href="#1029">17 February 2022 04:23</a>
              </span>
            </div>
            <div class="comment-content"><p>&quot;between forwarding engineers&quot; &lt;&lt; damn autocorrect &#x1F923;</p>

<p>And yes, while ATM died, we still have cell-based transport where latency matters. Did you notice that the cells have sane size this time?</p>
</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1032">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> mobartz</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1032" href="#1032">17 February 2022 07:08</a>
              </span>
            </div>
            <div class="comment-content"><p>I usually set all my switches to their maximum, but ran into an issues with a Cisco 4500-X which has a maximum MTU of only 9170.  OSPF routing with a 3850 wouldn&#39;t work with the MTU set to 9198, so I had to move it down to 9170 to match.</p>
</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1033">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Erik Auerswald</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1033" href="#1033">18 February 2022 02:29</a>
              </span>
            </div>
            <div class="comment-content"><p>When I use an IP MTU higher than 1500B, I usually set it to 9000B, since this is widely supported on different networking devices from various vendors, and is the maximum supported by VMware ESXi. This avoids silently losing frames sent from a system with larger MTU to one with lower (Ethernet) MTU, which can easily happen when using the maximum value of a given device and then introducing a different model into the network.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1034">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1034" href="#1034">19 February 2022 06:36</a>
              </span>
            </div>
            <div class="comment-content"><p>Hi Ivan, according to the MX Walkthrough doco, the parcel is generated by the WI block of the MQ/XM chip to send the first 256 bytes of a packet to the Lookup Unit, if the packet size &gt; 320 bytes, or the entire packet to the LU otherwise (pg 22). Hence the name parcel. The packet itself is stored in the MQ memories. This kind of arrangement seems to be pretty common for VOQ-based (3rd generation) devices. What seems rather inefficient is transporting the parcel back to MQ after the lookup is done. From an engineering POV, since we&#39;re talking ns here, duplication and interchip communication should be avoided whenever possible. And since MX is VOQ-based router, the MQ chip is also responsible for cellification of packets to transmit across the crossbar to the egress, in ATM-like cells. The cell headers get stored in VOQs (pg 35). This memory, being on-chip, is much faster accessed than the bulk memory used to store the packets themselves. </p>

<p>With this kind of buffering, GBs worth of packets can be stored, so whatever the frame size, it&#39;s not a problem. Remember, only the headers are in the VOQ, and that&#39;s fixed. The problem is the packet memory, being off-chip and having much slower RTT than TCAM, presents a latency problem if lots of packets come in at the same time. </p>

<p>The MQ&#39;s off-chip buffer used to store the packets can follow any buffering paradigm: contiguous or particle-based/scatter-gather. NCS platform has documented their memory architecture, and it&#39;s pretty similar. They don&#39;t reveal it all because they use Broadcom chipsets, but I doubt Broadcom has much better idea. </p>

<p>The 7200 and 7500 series were 2nd-generation devices; they still used CPU, not ASIC, to forward packets, so they essentially operate just like computer: CPU for look-up, RAM to store packets, no TCAM. That&#39;s why particle buffering is the only form they have, vs the much more sophisticated (and power-hungry) 3rd-gen VOQ devices with different kinds of memory. Again, 7200 and 7500 routers don&#39;t have any problem with different frame size.</p>

<p>In a word, the hardware can deal with any frame size. So the impact of Jumbo frame is mostly potential MTU mismatch and packet loss. </p>
</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1035">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1035" href="#1035">19 February 2022 08:36</a>
              </span>
            </div>
            <div class="comment-content"><p>As always, thanks for an extensive comment...</p>

<p>&gt; The MQ&#39;s off-chip buffer used to store the packets can follow any buffering paradigm: contiguous or particle-based/scatter-gather.</p>

<p>Would you happen to have a pointer to something explaining that (or is it in that book but I missed it?)</p>

<p>&gt; NCS platform has documented their memory architecture, and it&#39;s pretty similar. </p>

<p>All I found was an excellent document describing how the use on-chip/off-chip buffers</p>

<p>https://xrdocs.io/ncs5500/blogs/2020-09-04-ncs-5500-buffering-architecture/</p>

<p>It contained nothing about the buffer structure, and described Jericho which is a totally different beast than Trident or Tomahawk.</p>

<p>It&#39;s also interesting that while there were several replies along the lines of &quot;everyone is using jumbo frames&quot; (which is good to hear, so things are working in production), nobody sent me anything along the lines of &quot;hey, that&#39;s a solved problem, stop worrying&quot;, so I still remain skeptical about the behind-the-scenes implementation details.</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1037">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1037" href="#1037">21 February 2022 03:44</a>
              </span>
            </div>
            <div class="comment-content"><p>Hi Ivan,</p>

<p>No, there&#39;s no mentioning of how the packets get stored on off-chip memory in MX chipset so I speculate. After all, there are only 2 forms of memory storage anyway: contiguous or scatter/gather :p.</p>

<p>The xrdoc is the same on I referred to. That&#39;s what I meant by NCS ASIC&#39;s physical memory structure (sorry if I sounded ambiguous), not the off-chip packet storage buffering scheme. I meant the NCS&#39;s Jericho&#39;s ASIC physical memory structure and the MX&#39;s are rather similar. </p>

<p>On pg 42 of the MX doc, it says &quot;the default maximum data buffer value is 100ms.&quot; So looks like if the interfaces are of 10Gbps, MX chipset (Trio I think) stores up to GBs of packets. </p>

<p>Also, while it&#39;s true the hardware buffer can deal with any frame size because of its large size, Jumbo frames have other impacts when one think about it more closely. For ex, Jumbo frame can cause unpredictable delay for smaller packets when there&#39;re mix and match of Jumbo and tiny frames in the same queue, potentially delaying certain kinds of traffic more than they can tolerate. So when Jumbo frame is enabled, it&#39;s best to use CoS to separate elephant and mice traffic, depending on the levels of criticality. </p>

<p>Another corner case: with buffered crossbar, where cells arriving at egress out of order is inherent to the architecture, as observed in practice by Andrea:</p>

<p>https://blog.ipspace.net/2020/05/ip-packet-reordering.html#64</p>

<p>When this happens with many Jumbo frames in the mix, the total cell waiting time at the egress interface can be exceeded due to cells arriving out of order, causing cell drop and consequently, frame drop, necessitating retransmission. So that&#39;s a potential side effect. In bufferless crossbar, this is not an issue. </p>

<p>Minh</p>
</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
