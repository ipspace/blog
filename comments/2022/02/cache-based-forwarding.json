{
   "comments": [
      {
         "date": "24 February 2022 03:10",
         "html": "<p>This is how OpenVSwitch (https://blog.ipspace.net/2013/04/open-vswitch-under-hood.html) works, too: A first packet triggers a lookup and evaluation of ACLs, then the flow entry is cached for X seconds. ECMP forwarding is preserved because there is a &#39;multipath&#39; action that is evaluated for each packet; I know because I personally fixed the hashing (https://mail.openvswitch.org/pipermail/ovs-dev/2019-June/359489.html)</p>\n\n<p>This implementation is not quite so &quot;obsolete&quot; as one might think; for example, 1000s of Nuage VRS/NSGs use it as we speak</p>\n",
         "id": "1044",
         "name": " Jeroen van Bemmel",
         "pub": "2022-02-24T15:10:06",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "25 February 2022 03:58",
               "html": "<p>Thanks for the comment. Updated the blog post accordingly.</p>\n",
               "id": "1049",
               "name": "Ivan Pepelnjak",
               "pub": "2022-02-25T15:58:54",
               "ref": "1048",
               "type": "comment"
            }
         ],
         "date": "25 February 2022 12:38",
         "html": "<p>LISP is a more complex animal nowadays. Your knowledge of LISP seems to be quite outdated.</p>\n\n<p>Nowadays it is used with reliable transport and full PubSub.\nThere is no caching behavior at all. Each xTR has a full routing table.</p>\n",
         "id": "1048",
         "name": "Bela Varkonyi",
         "pub": "2022-02-25T12:38:15",
         "type": "comment"
      },
      {
         "date": "26 February 2022 02:03",
         "html": "<p>In general, I support the main point of the blog post: cache based forwarding is a problematic scalability mechanism, i.e., scalability is limited and performance collapses beyond the limit.</p>\n\n<p>Topology based forwarding has scalability limits, too. Once there are more routes than can fit the (hardware) forwarding tables, networking devices crash, fall back to CPU based forwarding (i.e., performance collapses), or deliver some packets to the wrong destination (I&#39;ve seen all of those&hellip;).</p>\n\n<p>Firewalls, i.e., devices where network topology is just a small part of the forwarding decision, often employ flow caches (for individual data flows) for performance optimization. In some &quot;high-end&quot; firewalls this flow cache is offloaded to hardware. Since per flow offload is often the best such a device can use, it is done in practice, and usually works well enough. [A stateless packet filter in front of a stateful firewall can help staying inside the performance envelope of said firewall.]</p>\n\n<p>Regarding networking devices primarily used for routing and bridging, the Enterasys Networks N-Series and their successors, K-Series and S-Series, based on CoreFlow resp. CoreFlow2 ASICs, used caching of individual flows in hardware forwarding tables as their only forwarding architecture. In practice, those switches worked well in the &quot;enterprise&quot; networks they were designed for. Of course it was possible to create overload with specific tests to demonstrate the potential for problems.</p>\n\n<p>Since per flow hardware offload allowed implementing complex, but still high performance, traffic filtering policies, replacing CoreFlow(2) based devices with networking devices using topology based forwarding often provided challenges regarding traffic filtering policies (i.e., either keep complex line-rate traffic filters, or use a different networking device with faster interfaces and topology based forwarding, but not both).</p>\n\n<p>I provide the above examples to illustrate the complexities involved. In theory, cache based forwarding does not scale. In practice, it may scale sufficiently, at least for a while (e.g., a decade inside an &quot;enterprise&quot; network before deploying the next device generation). It definitely breaks down when overloaded.</p>\n",
         "id": "1050",
         "name": "Erik Auerswald",
         "pub": "2022-02-26T14:03:48",
         "type": "comment"
      },
      {
         "date": "27 February 2022 05:13",
         "html": "<p>&gt; we had to use router CPU to walk</p>\n\n<p>I think you are mixing up two things. The forwarding entries had a simple combination of: (destination, outgoing interface, mac-address (actually encaps header)). So during packet-forwarding, no walk needed to be done. Just a simple lookup. The &quot;walking you are referring to happened during &quot;routing table maintenance&quot;. E.g. BGP walking its NLRI entries, to check if the nexthop still had the best IGP-metric, and whether nexthops were still valid. This was not done during route-lookup.</p>\n\n<p>Two-stage lookups during forwarding were introduce later. Because of PIC. This allows the RIB to send 1 message to the FIB, and the FIB makes 1 adjustment, impacting many (possibly tens or hunderds of thousands of routes). The trade-off here is: slightly more complex lookup (2 stages) versus much quicker updating the FIB. Note, FIBs in hardware can do in the order of 10K+ changes per second. Not very fast.</p>\n",
         "id": "1051",
         "name": " Henk",
         "pub": "2022-02-27T17:13:31",
         "type": "comment"
      },
      {
         "date": "27 February 2022 05:26",
         "html": "<p>&gt; IP prefix (using a calculated subnet mask based on an &ldquo;interesting&rdquo; algorithm.</p>\n\n<p>Are you sure it was a prefix? That is not what I remember. (Note, I have never worked on forwarding, I probably don&#39;t know what I am talking about. And I am not speaking in behalf of previous, current or future employers).</p>\n\n<p>1) I remember doing &quot;show ip fast-switching&quot; or some sort command. Seeing destination ip-addresses. Not prefixes. It&#39;s been almost 30 years, so maybe I remember it wrong.</p>\n\n<p>2) When I heard about OpenFlow doing on-demand caching, I was already wondering how they would scale that. Because with on-demand caching, you can&#39;t do summarization. Well, I wouldn&#39;t know how to do that. Let&#39;s take an example:</p>\n\n<p>A router has 3 routes: 0/0 -&gt; eth0, 10.1/16 -&gt; eth1 and 10.1.1/24 -&gt; eth2. Suppose a packet arrives for 10.1.9.9. A cache lookup happens -&gt; no entry. So packet is sent to the controller (aka process-switched). 10.1/16 -&gt; eth1 matches. So an entry for 10.1/16-&gt;eth1 is created in the forwarding cache. Next a packet for 10.1.1.1 arrives. Box does a lookup in the cache, 10.1/16-&gt;eth1 matches. The packet is forwarded out eth1. That is wrong. I have no idea how to fix this, unless you a) do caching per ip-address in stead of prefix, or 2) prepopulate the cache (aka FIB-switching). If someone can solve this problem, please let us know.</p>\n\n<p>&gt; packets had to wait for things like OSPF SPF run or BGP table scan to complete before they could be processed</p>\n\n<p>Utter non-sense. Sorry. The IOS scheduler would indeed not interrupt processes. But that didn&#39;t mean processes would keep running forever. On the contrary. One of the arts of writing router-software was to cut you task up in smaller steps, and do so called &quot;process_yield()&quot;s between every step. The idea was that a process could run for a few milliseconds, and then voluntarily release the CPU. Next time that process was scheduled (e.g. after packets were forwarded or someone did a show-command), the process would continue with the SPF or table-walk were it left off last time. If a process didn&#39;t give up the CPU, a CPUHOG warning was displayed. And the IOS-programmer couild introduce more sched_yield()s in her/his code.</p>\n",
         "id": "1052",
         "name": "Henk Smit",
         "pub": "2022-02-27T17:26:50",
         "type": "comment"
      }
   ],
   "count": 5,
   "type": "post",
   "url": "2022/02/cache-based-forwarding.html"
}
