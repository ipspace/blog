{
  "comments": [
    {
      "date": "17 November 2017 14:17",
      "html": "100GE ports into four 10GE or 25GE lanes ===  10 10GE ports instead of 4 10GE ports<br /><br />Why are we still hanging around layer 2 scaling and multipathing. Layer 2 stretching is not a viable solution. <br />instead of layer 2 multipathing, we use anycast dns or regional ips which will suffice the need. <br />Anycast gateway or regional IP gateway works perfectly fine.<br /><br />Server provisioning , we use DHCP options will suffice. NO NEED FOR L2 SCALING.  <br /><br />Stretching L2 is not viable because of two reasons. <br /><br />1. Preserving the Layer 3 Ip address and VLAN mobility. <br />2. Layer 2 tunnels such as OTV , VXLAN  is a extra feature with licensing cost and many vendor doesnt recommend a VxLAN based solution.<br /><br />Enterprise networks needs to move to cloud which is very easy to implement such as  Azure which runs on  Layer 3 and have maximum Availability Zones  and proper Cluster to AZ mapping which has optimal redundancy fashion. <br /><br />Whitebox switching or Vendor device switching [ OEM and ODM ]  is what everyone is looking for. <br /><br />Vendor devices in Tor is absolutely fine. The topology with Enterprise shrinks to two layer topology model <br />where Tor connects to Collapsed aggregation and Layer 2 is the flavour for Enterprise which is absolute mess. <br /><br />Onboarding a service into Cloud is much more easier and safer approach . In networking terminology , Layer 3 is way to go moving forward. EbGP solution with multipath and allows-as solves the problem which is much more straight forward. <br /><br />We follow Clos topology. Two layer model doesnt scale as we need more port density and and the failure domains is very evident.<br /><br />Oversubscription ratio, port density, prefix length { IPv4 and Ipv6 ] , breakout cabling, Scaling of RIB and FIB, Redundancy on device [ Forwarding, RP , Power , Fan ]. Prefix aggregation , <br /><br />The best bet for Enterprise is to move into the cloud a it is very fast , scalable , reliable in terms of redundancy and it  scale to various regions, DCs,edges . We also offer Caching and Edge last mile connectivity. <br /><br />What are your thoughts on this.<br /><br />",
      "id": "8179480918074949353",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": " HEMANTH RAJ",
      "profile": "https://www.blogger.com/profile/08900264515934620482",
      "pub": "2017-11-17T14:17:20.670+01:00",
      "ref": "8455359556737327720",
      "type": "comment"
    },
    {
      "date": "17 November 2017 15:50",
      "html": "Technologies such as VXLAN and NVGRE are host based tunnelling solutions which requires the host needs to create tunnels to another host using host based overlay mechanisms where it uses the fabric as a forwarding layer which provides redundancy, traffic engineering,  1:1  oversubscription and protection from Attacks ( ACL ) . <br />Nowadays, Router also supports VXLAN solution but at what cost do we need overlay . Why cant use NAT instead of Overlay. Overlay means one header on top of another header. NAT virtualizes the Ip addressing which  overlay solution. Backend IPs are NATted which is SNAT [ Source NAT ]  one one way direction.<br /><br />Why we need Overlay technolgies<br />1. Wrap one header over another for  dedicated routing but not dedicated infra. <br />2. Not exposing inner IP header ie customer IP address on to the dedicated Network Fabric <br />3. TO preserve TCAM [ LEM &amp; LPM ]  on the network fabric devices which is what we presev<br /><br />VXLAN on router is equivalent to GRE r IPSec as we are going to take decisions based on Dest IP on the Inner header after decapsulating outer header. So VXLAN is equivalent to GRE or IPsec or IPinIP.<br /><br />VXLAN on hosts is what the flavour as any server to server or any server to host . Host is presented somewhere on th internet. <br /><br />So now number of VXLAN session on the hosts depends on the number of flows that each server can handle. Nowadays each server has 4 to 6 threads so it can process 6 parallel flows at the same time. <br /><br />The problem with VXLAN on hosts is basically hosts processor stack has to have the capability to encap/decap at wire rate which is not an ideal solution because we need hosts to process data at wire rate with full disk speed , RAM and default TCP/IP stack which we do these days, <br /><br />We dont need any processing overhead to be carried on to the hosts. <br /><br />Thats why i dont endorse VXLAN , NVGRE based solutions as we have much more options to solve the problem.<br /><br />The problem that VXLAN solves is distinguishing two different flows in host and two different packet level treatments ie on QOS level and memory B.W. level on the hosts which we can solve on the Tor . <br /><br />Hope this makes lot of sense. <br /><br />Hope this networking industry move away from this solution. ",
      "id": "8724510033392578039",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": " HEMANTH RAJ",
      "profile": "https://www.blogger.com/profile/08900264515934620482",
      "pub": "2017-11-17T15:50:12.387+01:00",
      "ref": "8455359556737327720",
      "type": "comment"
    }
  ],
  "count": 2,
  "id": "8455359556737327720",
  "type": "post",
  "url": "2017/11/optimize-data-center-infrastructure.html"
}