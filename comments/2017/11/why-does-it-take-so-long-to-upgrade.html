<div class="comments post" id="comments">
  <h4>8 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1072829814144919968">
          <!--
          <div class="avatar-image-container">
            <img src="//images-blogger-opensocial.googleusercontent.com/gadgets/proxy?url=http://1.bp.blogspot.com/-j3GGfKuoRb4/VYDmEVI9C7I/AAAAAAAAAjs/aj4cvQNi2uk/s151/IMG_1013.JPG&container=blogger&gadget=a&rewriteMime=image/*">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09771677856264877238" rel="nofollow">Jeff Behrns</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1072829814144919968" href="#1072829814144919968">20 November 2017 14:21</a>
              </span>
            </div>
            <div class="comment-content">CLI, API, in either case most of the time is wasted waiting for the operations framework to give network team a green light for the upgrade.  Too many sign-offs, conf calls, political posturing, etc.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="3954694467577572094">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3954694467577572094" href="#3954694467577572094">20 November 2017 16:07</a>
              </span>
            </div>
            <div class="comment-content">And that&#39;s because we can&#39;t treat &quot;software upgrade&quot; as business-as-usual, because too many times something fails. People who deploy stuff weekly (or even multiple times per day) don&#39;t have that problem ;)</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="9004252148833121369">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://www.linkedin.com/in/david-cryer" rel="nofollow">David Cryer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9004252148833121369" href="#9004252148833121369">21 November 2017 00:16</a>
              </span>
            </div>
            <div class="comment-content">I think that many of us have used the blunt instruments at our disposal (Expect etc.) to automate the upgrade of the unfriendly products that we have to work with, with varying degrees of success and reliability. So in my experience much of the challenge of deploying new code is that our testing approach is usually still back in the dark ages, and not sufficiently rigorous to convince the change management processes and other stakeholders that we should go ahead.<br /><br />Roll on the day when we actually create Ansible playbooks (or the equivalent in any other tool) that test out all of the features that we&#39;re actually using in a switch/router. Even better being able to spin up a virtual copy of your environment and prove the actual software or configuration change in situ and automatically test and validate the operation of they system afterwards. Perhaps then we get closer to being on a par with our application development colleagues with their CICD pipelines and put ourselves in a position where we are trusted to make these sorts of changes quickly and frequently.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2898889545395617658">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09992235844690768695" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2898889545395617658" href="#2898889545395617658">21 November 2017 15:17</a>
              </span>
            </div>
            <div class="comment-content">Other side is keeping track of new updates, that can be a full time job. Vendors have also done a poor job of this. We need something like WSUS to easily handle updates. It should be part of regular operations like it is for server people.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2618832667102260095">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2618832667102260095" href="#2618832667102260095">21 November 2017 16:34</a>
              </span>
            </div>
            <div class="comment-content">Often there is no reason to upgrade if no fixes for bugs/vulnerabities are contained that apply to the network and no new features are required.<br />So after studying release notes if there is no practical advantage it is often better to stay with a proven stable release (and keep rolling out new devices on the same software release).<br /><br />Of course when an update is required there should be an efficent automated process minimizing the risk and the goal should be to have a consistent SW release across all devices again within a reasonable time frame.<br /><br />So as I see it network devices often run older releases if nothing is to be gained from an update.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4506258808989959976">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07985346761439657130" rel="nofollow">Bela</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4506258808989959976" href="#4506258808989959976">22 November 2017 17:48</a>
              </span>
            </div>
            <div class="comment-content">An important aspect is the terrible quality of network device software. You install a new version and a lot of things are broken. You have a perfect configuration in software simulation, but it does not work with hardware optimized devices, since the porting is never finished and barely tested by the vendor. The documentation is just a bad joke, the support forums are full with unanswered questions.<br /><br />There is so much bad experiences that most network engineers are reluctant to touch a working system for good reasons. You should change to a better environment with fully automated regression testing. However, when you look at the new SDN projects, the same problems come back. Everyone is rushing to include new features, and the automated testing have less than 10-20% coverage.<br /><br />No one wants to pay for quality. Good enough is the target... :-)<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7835436839679595473">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://twitter.com/fcuiller" rel="nofollow">Frederic Cuiler</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7835436839679595473" href="#7835436839679595473">23 November 2017 12:04</a>
              </span>
            </div>
            <div class="comment-content">Hi.<br /><br />Thanks for the article.<br /><br />I see different aspects of this problem from my SP window:<br /><br />1 - Technical: you covered it in the article. Tools did not exist in the past for network devices, and this is being adressed/is already adressed on most platforms.<br /><br />2 - Risk culture: despite all this new new fancy features to ease and speed software upgrades, it still takes ages to upgrade a network (weither it&#39;s 10 device for a small entreprise or 1000+ device for a SP backbone). Human factor is important. Telcos for example have a strong quality culture and risk control history, and it&#39;s hard for them to imagine upgrading several devices (even 5) in parallel, in one shot, by something totaly automated. What if it goes wrong? What if this loop is not secured? What if the NOC did not restore an IGP metric during the last maintenance window?<br /><br />3 - Checks at scale: upgrading is easy. Checking the state of the device or the traffic before, after, and compare it at scale is a challenge. You need those checks to validate your software ugprade is a success or to engage a rollback. Worst: it&#39;s not apple to apple comparison: you have churn in your IGP routes (so you end up with a small difference), you might have trafic shifting somewhere else because BGP<br />Worst: most routers deep health check are vendor dependant, platform dependant and software dependant. It&#39;s a tough one to manage. From past experiences, you add more and more checks, and end up with a crazy list. Again, automation could help. I also think telemetry could help: you shoud be able to confidently say you routeur is capable of taking production traffic looking at a dashboard containing key metrics.<br /><br />Finally, if your network is correctly designed, if your architecture redundancy is regularly tested (or you are confident it&#39;s working) and you have correct tools, you don&#39;t have any technical excuses to regularly roll out new software releases at scale. Now, I think as network professionals we should also adress the human factor/education for risk.<br /><br />For the first point you mentioned (why it takes so long to validate a software and why new releases are so buggy), I will come back in the comments but with my vendor hat this time ;-)<br /><br />Fred</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="396047941894532147">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/00358549302880805285" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c396047941894532147" href="#396047941894532147">29 November 2017 12:41</a>
              </span>
            </div>
            <div class="comment-content">There is only so much automation can do for a poor monolithic architecture. To realize CD for network software, remember we are crossing the vendor-operator divide which is greater than the Dev-Ops divide, but I think it is eventually possible by taking a page from the architecture that set DevOps and general software CD on fire over the past few years: microservices. I&#39;ve recently written about it in fact: https://www.linkedin.com/pulse/micro-services-knock-knockin-devnetops-door-james-kelly/ </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
