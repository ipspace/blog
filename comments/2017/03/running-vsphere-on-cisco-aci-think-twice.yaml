comments:
- comments:
  - date: 16 March 2017 10:08
    html: 'Sounds like a poor design decision. If you would get into a NSX (DLR in
      kernel) vs deploy 1 VM for every host and hairpin all your traffic through that
      VM discussion, its obvious what would be the prefered solution. :) '
    id: '7671715006945797912'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Oliver Kaiser
    profile: https://www.blogger.com/profile/07643793289499039056
    pub: '2017-03-16T10:08:13.541+01:00'
    ref: '8135500048995480039'
    type: comment
  - date: 16 March 2017 10:41
    html: Obviously there will be some performance penalty. That is exactly what Vmware
      are trying to do. No product should fit in better than NSX.
    id: '1936412496807497867'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Michael Kashin
    profile: https://www.blogger.com/profile/14125341240086592055
    pub: '2017-03-16T10:41:17.377+01:00'
    ref: '8135500048995480039'
    type: comment
  - date: 16 March 2017 17:09
    html: 'That&#39;s what Nicira was doing and what Nuage is probably still doing.
      The &quot;only&quot; problem: traversing userland kills performance. Nuage improved
      it by using SR-IOV on the external VM connection, but still. <br /><br />While
      it&#39;s easy to make fun of how limited VMware virtual switch is, its performance
      was always stellar (at least compared to alternatives).<br /><br />Not to mention
      that this becomes a nightmare to manage, more so as you a different VLAN on
      every inter-VM connection (or traffic gets bridged before it hits _your_ virtual
      switch)'
    id: '2273220491971463848'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-16T17:09:32.180+01:00'
    ref: '8135500048995480039'
    type: comment
  - date: 18 March 2017 12:18
    html: I wouldn&#39;t say it &quot;kills&quot; performance. It&#39;s a lot less
      detrimental then traversing a chained NFV(e.g. firewall). Nuage were doing it
      because they didn&#39;t have a strong hardware story. Cisco, on the other hand,
      can ditch the VS altogether and extended these p2p vlans to the hardware switches,
      similar to what they&#39;re doing with their Neutron plugin. This way they can
      keep the per-VM granularity with hardware performance. The only price they pay
      in this case is the ability to innovate and add new features. But like you said,
      nothing that can&#39;t be fixed with a bit of NAT, PBR and VXLAN
    id: '8443593435279556387'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Michael Kashin
    profile: https://www.blogger.com/profile/14125341240086592055
    pub: '2017-03-18T12:18:19.744+01:00'
    ref: '8135500048995480039'
    type: comment
  - date: 18 March 2017 14:30
    html: '&quot;It&#39;s a lot less detrimental then traversing a chained NFV(e.g.
      firewall).&quot; &lt;&lt; It&#39;s exactly the same thing.<br /><br />&quot;Cisco,
      on the other hand, can ditch the VS altogether and extended these p2p vlans
      to the hardware switches&quot; &lt;&lt; Don&#39;t forget that you can&#39;t
      turn off vSwitch in ESXi or reconfigure it at will like you can in Linux.'
    id: '1175743262310268918'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-18T14:30:31.134+01:00'
    ref: '8135500048995480039'
    type: comment
  - date: 18 March 2017 15:09
    html: '&quot;Don&#39;t forget that you can&#39;t turn off vSwitch in ESXi or reconfigure
      it at will like you can in Linux&quot; &lt;&lt; Nothing special is needed in
      this case. SDN controller only needs basic integration with vCenter to make
      2 API calls - 1) create an sPG 2) Attach a VM to this sPG. '
    id: '2618645602918073146'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Michael Kashin
    profile: https://www.blogger.com/profile/14125341240086592055
    pub: '2017-03-18T15:09:39.084+01:00'
    ref: '8135500048995480039'
    type: comment
  - date: 18 March 2017 19:24
    html: To recap:<br />* One port group (or more) per EPG<br />* Separate VLAN for
      every EPG<br />* Private VLANs if you don&#39;t want VMs within EPG to communicate<br
      />* SDN controller creating vSphere port groups and attaching VMs to port groups
      behind the scenes.<br /><br />I&#39;m sure it will work and scale amazingly
      well, and everyone will be delighted (including vSphere admins), and I&#39;m
      only imagining problems where there are none...
    id: '6796723340915715778'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-18T19:24:27.444+01:00'
    ref: '8135500048995480039'
    type: comment
  date: 16 March 2017 09:25
  html: There is a way of out this. Have your nexus 1k as a VM on every host and user
    VMs, each in their own locally unique VLAN, connected to the nexus VM via these
    p2p VLANs. There&#39;s one SDN vendor that does exactly that.
  id: '8135500048995480039'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Michael Kashin
  profile: https://www.blogger.com/profile/14125341240086592055
  pub: '2017-03-16T09:25:39.566+01:00'
  ref: '7682908166204599785'
  type: comment
- date: 16 March 2017 10:08
  html: '&quot;Some VMware training material&quot; - could you be more specific?'
  id: '7332710613897920956'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2017-03-16T10:08:31.395+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 16 March 2017 11:21
    html: Well true at high level but the implementation detail is more complicated,
      which allows for usage of uSeg at scale.<br />Also to be honest, VMM integration
      is great, but far from mandatory. You can achieve exactly the same level of
      integration by putting some automation in place and use physical domains...no
      functional difference in the end. So who cares except VMware fanboys who are
      anyway too emotional to limit DC design to real requirements?
    id: '69485760491740216'
    image: https://lh4.googleusercontent.com/-lFA_VPHP_fA/AAAAAAAAAAI/AAAAAAAAAKU/o9xIHK32IpA/s32-c/photo.jpg
    name: Nicolas VERMANDE
    profile: https://www.blogger.com/profile/09272777243328964166
    pub: '2017-03-16T11:21:33.308+01:00'
    ref: '6836036850575096083'
    type: comment
  - date: 17 March 2017 02:03
    html: s/VMware fanboys/Cisco fanboys/<br />Seriously, why are Cisco staff so petty?
    id: '6859870381642290978'
    image: https://resources.blogblog.com/img/blank.gif
    name: Herman
    profile: null
    pub: '2017-03-17T02:03:49.496+01:00'
    ref: '6836036850575096083'
    type: comment
  date: 16 March 2017 10:29
  html: Note that Cisco states that Microsegmentation in ACI can be achieved also
    when running DVS, through creative use of private vlans on DVS + Mac filters in
    Leaf switches + Proxy ARP. Would this be a case of just because you can, doesn&#39;t
    mean you should?
  id: '6836036850575096083'
  image: https://resources.blogblog.com/img/blank.gif
  name: g9ais
  profile: null
  pub: '2017-03-16T10:29:46.757+01:00'
  ref: '7682908166204599785'
  type: comment
- date: 17 March 2017 00:29
  html: If VMWare eliminated the API they would burn bridges with existing customers
    who are using VSphere + ACI.  Not just small ones, I&#39;m referring to large
    corporations such as banks.<br /><br />They would also screw other partners such
    as Bigswitch who rely on the same APIs.<br /><br />If VMWare would want to do
    such a thing and push the world even closer to using open source hypervisors and
    containers, so be it.
  id: '1043931369789895127'
  image: https://lh5.googleusercontent.com/-Gaf6ZNCRZxc/AAAAAAAAAAI/AAAAAAAAZJU/DtQGlZoOS3o/s32-c/photo.jpg
  name: Stefano Pirrello
  profile: https://www.blogger.com/profile/04433412437498680888
  pub: '2017-03-17T00:29:02.260+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 17 March 2017 19:06
    html: Well not fanboy, but I still do like NSX and all the VMware products in
      general and still working with them on a daily basis. Path to VCDX taught me
      how to tackle global design exercises...which has nothing to do with products,
      but requirements. Sometimes people tend to forget this and this is a pity.
    id: '2272364154054254757'
    image: https://lh4.googleusercontent.com/-lFA_VPHP_fA/AAAAAAAAAAI/AAAAAAAAAKU/o9xIHK32IpA/s32-c/photo.jpg
    name: Nicolas VERMANDE
    profile: https://www.blogger.com/profile/09272777243328964166
    pub: '2017-03-17T19:06:51.770+01:00'
    ref: '9222430102665936795'
    type: comment
  - date: 21 March 2017 13:53
    html: '@anonymous So, what&#39;s your insinuation? This is still hopefully a technology
      page not a high-school debate gotcha! <br />He wrote a blog on how to use NSX
      for a particular usecase. Is that bad? Atleast, he can say he understands both
      products. <br />'
    id: '4460420065614380424'
    image: https://4.bp.blogspot.com/_U8WuLZSW4ek/StIFWpmRUNI/AAAAAAAADpw/LMDcD6oWBlY/S220-s32/DSC01624.JPG
    name: Murali
    profile: https://www.blogger.com/profile/16488361009215312027
    pub: '2017-03-21T13:53:07.926+01:00'
    ref: '9222430102665936795'
    type: comment
  date: 17 March 2017 14:55
  html: Times change, Nicolas...<br /><br />http://static-void.io/vmware-nsx-use-case-simplifying-disaster-recovery-part-1/<br
    /><br />Before joining Cisco, you have been a VMware / NSX fanboy as well ;-)
  id: '9222430102665936795'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2017-03-17T14:55:02.925+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 17 March 2017 17:04
    html: There&#39;s the management API and then there&#39;s the &quot;3rd party
      data plane API&quot; that switches like Nexus 1000V use to integrate with vDS
      framework within vSphere. Based on the tone of your comment I would expect you
      to know that.<br /><br />Also, based on the fact you chose to remain anonymous
      my end of this discussion stops right here.
    id: '7875879705285605996'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-17T17:04:42.693+01:00'
    ref: '9114424419391138438'
    type: comment
  date: 17 March 2017 16:44
  html: Other than speculating and pointing to other blogs hinting at technical difficulties
    you haven&#39;t cared to verify yourself, what actual facts can you point us all
    to? If VMware discontinues support for the vDS API, how do you suggest their own
    tools provision vDS?
  id: '9114424419391138438'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2017-03-17T16:44:46.169+01:00'
  ref: '7682908166204599785'
  type: comment
- date: 21 March 2017 08:43
  html: Spot on :)
  id: '7010290319203644161'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Mitko
  profile: https://www.blogger.com/profile/17252322269397556994
  pub: '2017-03-21T08:43:43.426+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 21 March 2017 20:58
    html: Thanks for an extensive reply. I&#39;ll stick to the technical details,
      in particular the PVLAN part. While I admire the ingenuity of the idea, the
      deviation from IEEE 802.1 forwarding paradigm worries me. For example, if you
      have to flood traffic from ToR switch to an EPG member, everyone in the same
      PVLAN will get the traffic even if they&#39;re not in the same EPG.
    id: '6913840323339929612'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-21T20:58:05.373+01:00'
    ref: '9030401547431762114'
    type: comment
  - date: 22 March 2017 09:59
    html: There is no deviation Ivan. There is nothing magical here. Again, I extend
      the offer to spend time with you actually testing this, so we can move on from
      theoretical discussion. It is the best way to stick to the technical details
      and remove assumptions.
    id: '8548676146492851638'
    image: https://2.bp.blogspot.com/-GM5UJCXAEl4/VBWSCoacMjI/AAAAAAABJ2w/z1aWT4kLWbk/s33/*
    name: Nillo
    profile: https://www.blogger.com/profile/17721867360338634399
    pub: '2017-03-22T09:59:18.279+01:00'
    ref: '9030401547431762114'
    type: comment
  - date: 23 March 2017 10:56
    html: I&#39;m not sure I understand this point, this is the principle of a bridge-domain,
      ie. flooding boundary, PVLAN or not, it&#39;s always been the case. You can
      also optionally flood in EPG.<br />In addition, although this is PVLAN in the
      VDS, it&#39;s not PVLAN on the ToR. This means:<br />- normal ACI BD semantics
      are applied, regardless of whether EPG is enabled for uSeg or not (understand
      PVLAN pushed to port-group). All EPGs inside the same bridge-domain receive
      the same PVLAN ids. (no burning of VLANs). Then secondary isolated VLAN in ACI
      translates into a reserved pcTag that is set with deny({src,dst}=isolated encap).
      no PVLAN implementation there at all...<br />- MAC-based classification is pushed
      on ToR for the base EPG<br />- List is updated upon VM-based attributes defined
      by the user to re-classify EP in to specific uSeg EPGs.<br />- You need to specify
      a BD for uSeg EPG<br />- Flooding will happen THERE.
    id: '4345879726059125600'
    image: https://lh4.googleusercontent.com/-lFA_VPHP_fA/AAAAAAAAAAI/AAAAAAAAAKU/o9xIHK32IpA/s32-c/photo.jpg
    name: Nicolas VERMANDE
    profile: https://www.blogger.com/profile/09272777243328964166
    pub: '2017-03-23T10:56:22.748+01:00'
    ref: '9030401547431762114'
    type: comment
  - date: 24 March 2017 20:57
    html: I defy anyone to look at the diagram contained in the ACI Virtualization
      Guide on this topic and say this is a supportable or scalable solution...<br
      /><br />http://www.cisco.com/c/dam/en/us/td/i/500001-600000/500001-510000/500001-501000/500655.jpg
    id: '4455386084874894738'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2017-03-24T20:57:21.956+01:00'
    ref: '9030401547431762114'
    type: comment
  - date: 26 March 2017 20:09
    html: Hey Anonymous :-)<br />Apparently you didn&#39;t understand the principle.
      This is one (Pimary ,Secondary) pair per Bridge Domain (BD), not per ACI VLAN
      encap. You can have many EPGs per BD...so scale is really not the issue.<br
      />But as you&#39;ve chosen to stay anonymous, I guess the discussion doesn&#39;t
      need to go any further anyway.<br />
    id: '1157597898075761856'
    image: https://lh4.googleusercontent.com/-lFA_VPHP_fA/AAAAAAAAAAI/AAAAAAAAAKU/o9xIHK32IpA/s32-c/photo.jpg
    name: Nicolas VERMANDE
    profile: https://www.blogger.com/profile/09272777243328964166
    pub: '2017-03-26T20:09:19.685+02:00'
    ref: '9030401547431762114'
    type: comment
  - date: 29 March 2017 07:47
    html: There is always a workaround or right architecture when you deploy these
      solutions specially in enterprise environments and not in lab. Forcing traffic
      via PVLAN (i cant believe we still using this technology), proxy ARP, inconsistent
      state between what you see on vpshere vs ACI and not to mention sub optimal
      traffic forwarding and inspection. Does make sense to me just because you need
      to have a tick mark
    id: '6767729136984496053'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Unknown
    profile: https://www.blogger.com/profile/04798538618197248046
    pub: '2017-03-29T07:47:39.622+02:00'
    ref: '9030401547431762114'
    type: comment
  date: 21 March 2017 08:58
  html: "Ivan, I believe you are miss informed. I am not talking specifically about\
    \ the vSphere API that AVS uses, and the constant rumours about it, but more about\
    \ ACI in general.<br /><br />It is true that VMware is becoming a company where\
    \ their technology is increasingly vertically integrated and eventually they want\
    \ that if you use their hypervisor you have to use their SDN solution and their\
    \ orchestration system and their VDI and ... I think the market will steer things\
    \ otherwise \u2026 but we shall see! \U0001F603<br /><br />I don\u2019t know how\
    \ much of an opportunity you had to have hands-on with ACI recently. I\u2019d\
    \ love to spend time showing you how some of the things we do with ACI work. Meanwhile\
    \ I provide here my respectful feedback.<br /><br />I definitely disagree with\
    \ some comments that you made. Some are in fact open for debate, for instance:<br\
    \ /><br />\u201CYou can\u2019t do all that on ToR switches, and need control of\
    \ the virtual switch\u201D.<br /><br />I wouldd say you can do a lot of what you\
    \ need to do for server networking on a modern ToR. And yet you are right, and\
    \ you do need a virtual switch. That is clear. How much functionality you put\
    \ on one vs. another is a subject for debate with pros and cons.<br /><br />But\
    \ in an SDN world with programmable APIs, it does not mean you need YOUR virtual\
    \ switch in order to control it. You just need one virtual switch that you can\
    \ program. That is all.<br /><br />There\u2019s a lot that we can do on AVS that\
    \ we can do on OVS too. And we do it on OVS too. There\u2019s a lot with do on\
    \ AVS that we can\u2019t do with VDS. But there\u2019s enough that we can do in\
    \ VDS so that when combining it with what we do on the ToR we deliver clever things\
    \ (read more below).<br /><br />The below comment on the other hand is imho misinformed:<br\
    \ /><br />\u201CThat [running without AVS] would degrade Cisco ACI used in vSphere\
    \ environments into a smarter L2+L3 data center fabric. Is that worth the additional\
    \ complexity you get with ACI? &quot;<br /><br />First, it is wrong to assume\
    \ that in a vSphere environment ACI is no more than smarter L2+L3 data center\
    \ fabric. But even if it was only that \u2026 it is a WAY smarter L2+L3 fabric.<br\
    \ /><br />And that leads me to your second phrase. What is up with the \u201C\
    additional complexity you get with ACI?\u201D.<br /><br />This bugs me greatly.\
    \ ACI has a learning curve, no doubt. But we need to understand that the line\
    \ between \u201Ccomplex\u201D and \u201Cdifferent\u201D is crossed by eliminating\
    \ ignorance.<br /><br />Anyone that has done an upgrade on anymore than a handful\
    \ of switches from any vendor and then conducts a network upgrade (or downgrade)\
    \ on dozens and dozens of switches under APIC control will see how it becomes\
    \ much simpler.<br /><br />Configuring a new network service involving L2 and\
    \ L3 across dozens and dozens of switches on multiple data centers is incredibly\
    \ simpler. Presenting those new networks to multiple vCenters? Piece of cake.\
    \ Finding a VM on the fabric querying for the VM name? \u2026 done. Reverting\
    \ the creation on the previously created network service is incredibly simpler\
    \ too. I mean \u2026 compared to traditional networking \u2026 let me highlight:\
    \ INCREDIBLY simpler.<br /><br />Changing your routing policies to announce or\
    \ not specific subnets from your fabric, or making a change in order to upgrade\
    \ storm control policies to hundreds or thousands of ports, \u2026 or - again\
    \ - reverting any of those changes, becomes real simple. Particularly when you\
    \ think how you were doing it on NX-OS or on any other vendor\u2019s box-by-box\
    \ configuration system.<br /><br />And the truth is that APIC accomplishes all\
    \ of that, and more, with a very elegant architecture based on distributed intelligence\
    \ in the fabric combined with a centralised policy and management plane on a scale-out\
    \ controller cluster.<br /><br />Other vendors require combining six different\
    \ VM performing three different functions just to achieve a distributed default\
    \ gateway that is only available to workloads running on a single-vendor hypervisor.\
    \ Now that\u2019s complex, regardless of how well you know the solution.<br /><br\
    \ />Full response with details about how we do uSeg with VDS here:<br />http://nillosmind.blogspot.com/2017/03/a-response-to-running-vsphere-on-cisco.html"
  id: '9030401547431762114'
  image: https://2.bp.blogspot.com/-GM5UJCXAEl4/VBWSCoacMjI/AAAAAAABJ2w/z1aWT4kLWbk/s33/*
  name: Nillo
  profile: https://www.blogger.com/profile/17721867360338634399
  pub: '2017-03-21T08:58:03.508+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 21 March 2017 20:47
    html: I agree the whole thing is a massive deja vu all over again. Unfortunately
      that&#39;s how the industry we work in works. See also RFC 1925.<br /><br />As
      for &quot;It would be a really interesting conversation that this vendor would
      have with customers who have deployed AVS and N1KV if they decided to turn APIs
      off.&quot; - they did that with dvFilter API which was used by way more products.
      There must have been heated conversations. I haven&#39;t heard about them from
      any of my customers.<br /><br />&quot;would be nice if you spent some time with
      someone who actually breathes ACI&quot; &lt;&lt; I did ;)<br /><br />&quot;you
      make it sound like closing down APIs is a good thing&quot; &lt;&lt; where did
      I do that?<br /><br />&quot;or an inevitable fact of life.&quot; &lt;&lt; That&#39;s
      probably true. BTW, what happened to OnePK API?
    id: '1947634524048460206'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-21T20:47:13.090+01:00'
    ref: '1849654279041742874'
    type: comment
  - date: 22 March 2017 00:51
    html: This comment has been removed by the author.
    id: '6447179054021753276'
    image: https://lh4.googleusercontent.com/-lFA_VPHP_fA/AAAAAAAAAAI/AAAAAAAAAKU/o9xIHK32IpA/s32-c/photo.jpg
    name: Nicolas VERMANDE
    profile: https://www.blogger.com/profile/09272777243328964166
    pub: '2017-03-22T00:51:21.563+01:00'
    ref: '1849654279041742874'
    type: comment
  date: 21 March 2017 14:13
  html: 'I do work for Cisco in the ACI team. However, I used to work at Oracle. This
    whole topic is a massive deja vu.<br /><br />&quot;.. it seems VMware might throw
    another spanner in the works.&quot;<br />Funny how Vmware played the open/choice
    card vis-a-vis Oracle software running on Vmware hypervisor environment but now
    are making Oracle&#39;s arguments against Cisco ;-). Hypocrisy much?<br />Let
    me refresh your mind:<br /><br />Oracle support policy&gt;&gt;<br />https://mikedietrichde.com/2011/01/17/is-oracle-certified-to-run-on-vmware/<br
    />Did you know about what Vmware says to Oracle that has a similar approach when
    someone runs Oracle <br />software/databases in an Vmware ESXi environment?<br
    />&gt;&gt;https://www.vmware.com/support/policies/oracle-support.html<br /><br
    />So, coming back to AVS/N1KV being turned off...<br /><br />It would be a really
    interesting conversation that this vendor would have with customers who have deployed
    AVS and N1KV if they decided to turn APIs off. <br /><br />Ivan (@ioshints), would
    be nice if you spent some time with someone who actually breathes ACI and/or validate
    it in detail yourself. <br />You make some comments here that seem to be assumptions
    like &quot;would degrade Cisco ACI&quot; that would be nice to validate. <br /><br
    />Further, Ivan(@ioshints), you make it sound like closing down APIs is a good
    thing or an inevitable fact of life. '
  id: '1849654279041742874'
  image: https://4.bp.blogspot.com/_U8WuLZSW4ek/StIFWpmRUNI/AAAAAAAADpw/LMDcD6oWBlY/S220-s32/DSC01624.JPG
  name: Murali
  profile: https://www.blogger.com/profile/16488361009215312027
  pub: '2017-03-21T14:13:46.243+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 22 March 2017 03:32
    html: 'Have to disagree with that:  Having ACI as your fabric effectively gives
      you ONE moving piece, which in turn enables you to consolidate the hundreds
      of moving pieces we&#39;ve been dealing with for decades.'
    id: '6895623561715482534'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2017-03-22T03:32:01.505+01:00'
    ref: '2631700387507194274'
    type: comment
  date: 21 March 2017 15:28
  html: 'As has been mentioned here, ACI has too many moving parts. I recall reading
    &quot;The Policy Driven Data Center with ACI: Architecture, Concepts, and Methodology&quot;
    a few years back and just going through that text back then I came to the same
    conclusion. '
  id: '2631700387507194274'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: jeff sicuranza
  profile: https://www.blogger.com/profile/07267516785767923381
  pub: '2017-03-21T15:28:16.724+01:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 24 March 2017 05:58
    html: Right on. Ronak
    id: '3034996903648244575'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: rodeo
    profile: https://www.blogger.com/profile/17739226158584019302
    pub: '2017-03-24T05:58:40.609+01:00'
    ref: '4657828699234939319'
    type: comment
  - date: 24 March 2017 08:53
    html: Praveen,<br /><br />Thanks for the comment. Could we please focus on what
      I wrote and not what Cisco&#39;s engineers read between the lines, or what marketing
      messages they might like to propagate?<br /><br />Please note that there are
      no generic comments on ACI applicability, or how your customers might perceive
      it in this blog post, it deals with the potential impact of not having AVS in
      vSphere anymore.<br /><br />Whoever claims that you can get the exact same functionality
      in vSphere environment without AVS is mistaken, as I&#39;ll explain in a follow-up
      blog post.<br /><br />Finally, you might want to watch the Networking Field
      Day videos (including the latest ones from CLEUR) to get a wider perspective
      on my ACI views.<br /><br />Kind regards,<br />Ivan
    id: '3315300503465931240'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-03-24T08:53:10.897+01:00'
    ref: '4657828699234939319'
    type: comment
  - date: 24 March 2017 14:14
    html: Ivan,<br /><br />First, AVS is an optional component of ACI. Let me give
      an example of using vDS and implementing micro-segmentation. <br /><br />And
      before I do that, let me ask you, why micro-segmentation is only for VMware
      Vms, why bare metals and containers, and other hypervisors don&#39;t deserve
      that treatment? That&#39;s what exactly customers get when they deploy ACI,
      a consistent policy, a consistent micro-segmentation, across all workloads.<br
      /><br />EPG is micro-segments in ACI. Define any number of them and put them
      under same BD (a segment). Put contract/policy across them. When using VMware,
      each EPG will be pushed as separate port-groups and policy enforced by EPG.
      At the same time flood semantic of BD is maintained across. Same thing works
      with bare metal, containers, Hyper-V etc. <br /><br />Here is my blog on micro-seg:<br
      />http://blogs.cisco.com/datacenter/microsegmentation<br /><br />There are other
      ways to achieve that in ACI using private vlans but let&#39;s leave it for some
      other time.<br /><br />If you say &quot;That would degrade Cisco ACI used in
      vSphere environments into a smarter L2+L3 data center fabric.&quot; Citing this
      without deep dive is what I&#39;m not happy about.<br /><br />I&#39;m not going
      to compare feature by feature here; please do a hands on or talk to customers
      who deployed this.<br /><br />BTW, I&#39;m no longer in Cisco and now I only
      get news like everybody else does. Here is what I saw yesterday<br /><br />https://www.linkedin.com/feed/update/urn:li:activity:6250791574444347392/<br
      /><br />Cheers<br />Praveen
    id: '7939000134541335574'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: praveen jain
    profile: https://www.blogger.com/profile/05799900326087453343
    pub: '2017-03-24T14:14:38.260+01:00'
    ref: '4657828699234939319'
    type: comment
  - date: 24 March 2017 14:28
    html: Direct link on DataVita datacenter built on ACI. <br /><br />http://blogs.cisco.com/datacenter/never-better-time-for-cisco-aci-enabled-data-center-providers
    id: '3429071281161489176'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: praveen jain
    profile: https://www.blogger.com/profile/05799900326087453343
    pub: '2017-03-24T14:28:46.172+01:00'
    ref: '4657828699234939319'
    type: comment
  - date: 26 March 2017 20:21
    html: Ivan,<br /><br />you can be sure that it&#39;s not because VMware is closing
      APIs that we won&#39;t propose any alternative architecture to the AVS.<br />It&#39;s
      clearly not the &quot;death&quot; of AVS, and customers currently running AVS
      can be guaranteed that we&#39;ll do everything required to maintain their commitment
      to ACI and add AVS &quot;services&quot; to their environment post vsphere 6.5u1.<br
      />
    id: '1803036932241005065'
    image: https://lh4.googleusercontent.com/-lFA_VPHP_fA/AAAAAAAAAAI/AAAAAAAAAKU/o9xIHK32IpA/s32-c/photo.jpg
    name: Nicolas VERMANDE
    profile: https://www.blogger.com/profile/09272777243328964166
    pub: '2017-03-26T20:21:43.562+02:00'
    ref: '4657828699234939319'
    type: comment
  date: 23 March 2017 16:36
  html: "Ivan, I&#39;ve been a reader of your blogs for many years and appreciate\
    \ your contents.<br /><br />I was one of the founders of ACI and very disappointing\
    \ to see how it is represented here. I&#39;ll stay away from NSX comparison but\
    \ want to highlight value prop of ACI.<br /><br />In datacenter, you have VMs,\
    \ Bare metal (storage and servers) and evolving into containers in a VM, or containers\
    \ nativly in BM. <br /><br />Now think about managing networking/vSwitches in\
    \ vSphere, Hyper-V, KVM for your VMs - may be you have one type or you are considering\
    \ to deploy another vendor, physical switches for bare metals, container networking,\
    \ and then vxlan-vlan gatewatey for BM-&lt;&gt;VM connectivity etc. <br /><br\
    \ />Not only managing complexity but then operation, troubleshooting and policy\
    \ consistency. And then making sure it all works at scale and hopefully doesn&#39;t\
    \ change when new technology like container is to be deployed.<br /><br />Please\
    \ read one of my blogs:<br /><br />http://blogs.cisco.com/datacenter/aci-the-sdn-purpose-built-for-data-center-operations-and-automation<br\
    \ /><br />At the end, customer validate the technology and I&#39;m very proud\
    \ to hear from customers that the ACI is game changer.<br /><br />I&#39;ll close\
    \ with one paragraph from my blog:<br /><br />&quot;ACI was architected to provide\
    \ a unified network independent of the type of workload \u2013 bare metal, virtual\
    \ machine, or container. At the end of the day, workloads are IP endpoints and\
    \ two IP end points want to connect. They need load balancing, security, and other\
    \ services. Their life cycle may be static, may need migration, or simply a quick\
    \ start/stop. ACI handles it all.&quot;"
  id: '4657828699234939319'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: praveen jain
  profile: https://www.blogger.com/profile/05799900326087453343
  pub: '2017-03-23T16:36:49.762+01:00'
  ref: '7682908166204599785'
  type: comment
- date: 28 March 2017 15:15
  html: sadly when big vendor do these changes/movement on direction the only loss
    is for the small customer...it was the same when cisco said to through all nexus
    7k/5k and buy the nexus 9k if you want automation...it&#39;s just the customer
    that invest money has to deal with these changes and the loss :(
  id: '8584272581594237830'
  image: https://resources.blogblog.com/img/blank.gif
  name: Faek Soussi
  profile: null
  pub: '2017-03-28T15:15:59.620+02:00'
  ref: '7682908166204599785'
  type: comment
- date: 02 April 2017 03:33
  html: "&quot;Update 2017-03-21: As is often the case I stand corrected (thanks to\
    \ g9ais and Nillo) - you can use PVLAN to pull all traffic out of the hypervisor\
    \ to the ToR switch, and process it there. I still think there are things that\
    \ can go wrong with that approach with just the right mix of flooded traffic,\
    \ so I don&#39;t think it&#39;s functionally equivalent to having full control\
    \ of the virtual switch. Can&#39;t figure out at the moment whether that&#39;s\
    \ relevant or not - any comment would be highly welcome.&quot;<br /><br />The\
    \ functionality equivalent to having full control of the virtual switch is definitely\
    \ not the same. Loosing a host-based local switching and routing is one of the\
    \ examples. But in this case as Nillo mentioned the traffic optimization here\
    \ is not the big issue from a practical standpoint. What is a bigger disadvantage\
    \ from the real-life scenario is a lack of service insertion into the L2 path\
    \ or intra-L3 subnet path. In theory ACI can use usegments and use contracts between\
    \ them but still no service insertion into the L2 path. What\u2019s more usegments\
    \ with so many contracts do not scale even on the next gen 93180YC-EX. In comparison\
    \ NSX do this natively with many security vendors. Without the effective micro\
    \ segmentation ACI cannot offer spoofguard to protect against man in the middle\
    \ attacks or IP address spoofing. The best switch on the market will no see flows\
    \ which are switched locally on the host. How the security can be fully provided\
    \ if there is no full visibility of applications running on hosts? So security\
    \ I would say is the biggest concern in terms of the solution which is hw-switch\
    \ based not vswitch based.<br /><br />To anticipate some arguments:<br />Yes,\
    \ NSX-v is for vSphere and NSX-T is for multihypervisor.<br />Yes, I also recommend\
    \ hardware based solutions where critical apps are on the bare metal.<br />Yes,\
    \ Visibility of the underlay is not an issue.<br />Yes, Virtual firewalls plays\
    \ a vital role to protect apps in the virtual environment.<br />Yes, Both NSX\
    \ and ACI have its own pros and cons. :)"
  id: '6143193291641226636'
  image: https://resources.blogblog.com/img/blank.gif
  name: Piotr Jablonski
  profile: null
  pub: '2017-04-02T03:33:21.285+02:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 15 May 2017 16:54
    html: Hi Craig.  I work for vArmour and wanted to correct you on one point.  You&#39;re
      right about vArmour for the most part, but I think you&#39;re working with somewhat
      outdated information.  There is no forwarding or offloading to separate FW VMs.  This
      was the behavior of the system in a previous generation.  The current VXLAN
      implementation is for control plane only.  Everything inside the hypervisor
      gets L7 inspected and forwarded on the normal switching path.  This is true
      regardless of whether it&#39;s deployed in ACI or non-ACI mode.<br /><br />And
      you&#39;re right about the performance.  With some of Intel&#39;s latest libraries,
      you can make userspace VMs run pretty fast from an I/O perspective.  8+ Gbps
      performance per vCPU with application ID and verbose logging with 1500 byte
      MTU.
    id: '7286130937198278632'
    image: https://resources.blogblog.com/img/blank.gif
    name: Matt
    profile: null
    pub: '2017-05-15T16:54:52.373+02:00'
    ref: '6066134457747492207'
    type: comment
  date: 24 April 2017 20:18
  html: Most of our unfortunate coupling of network technologies has been in pursuit
    of performance. E.g., VMware will talk your ear off about the value of NSX local
    switching/routing/FW (though they&#39;ll go curiously quiet when working on NSX
    edge designs).<br /><br />But it seems that with VMXNET3 driver support, DirectPath
    I/O, SR-IOV, and VXLAN+TCP offload support, we&#39;ve got enough network performance
    that maybe it&#39;s time to shift all the vSwitch bloat into a VM.<br /><br />vArmour
    gets by just fine completely with VM-based switching -- they use one VM on each
    host for traffic interception and redirection. Some traffic is redirected w/VXLAN
    to the main firewall VMs. Other traffic is forwarded directly to the uplinks or
    other VMs on the host. Their approach is totally agnostic to the hypervisor vSwitch,
    so it works with every hypervisor and cloud environment. It doesn&#39;t have the
    raw speed of heavily-coupled hardware/hypervisor forwarding, but it&#39;s close
    enough that the only ones who will raise a stink about it are the sales engineers.<br
    /><br />Cisco did the same thing with Virtual Topology System (VTS) -- it began
    as a totally decoupled software VXLAN engine with distributed L2 control plane
    -- perfect for extending L2 between different hypervisors located anywhere in
    the world. They&#39;ve since spun VTS into something slightly different, but maybe
    Cisco should bring back and reemphasize its hypervisor-agnostic switching capabilities.
  id: '6066134457747492207'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Craig Weinhold
  profile: https://www.blogger.com/profile/10562240567667879482
  pub: '2017-04-24T20:18:37.480+02:00'
  ref: '7682908166204599785'
  type: comment
- comments:
  - date: 14 June 2017 10:36
    html: no it s closed the API and will focus the vmware engineer to work on their
      products like the DVS API etc..
    id: '4484138945765811237'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Unknown
    profile: https://www.blogger.com/profile/15264124054761045280
    pub: '2017-06-14T10:36:12.804+02:00'
    ref: '2672226442192204306'
    type: comment
  date: 08 June 2017 17:07
  html: "One my favourite things in Ivan\xB4s posts is the discussion that spontaneously\
    \ develops in the comments, call it the right topic or hitting the nerve... <br\
    \ />Im a true ACI and NSX fan, also VCIX-NV, but I really don\xB4t like that VMware\
    \ closed the APIs for 3rd party integration, its a bit coward-ish facing the competitors\
    \ technology that way. a real potential of SDN is in how you handle the Hypervisor\
    \ Virtual Switch, and both Cisco and Nuage need to completely change the strategy.\
    \ <br />My question is: Does anyone know if VMware just stopped supporting the\
    \ solutions with a 3rd party virtual switch, or are the APIs closed all together?\
    \ Could we, as a System Integrator, support vCenter+AVS?<br />"
  id: '2672226442192204306'
  image: https://resources.blogblog.com/img/blank.gif
  name: Mateja Jovanovic
  profile: http://www.snarchs.com
  pub: '2017-06-08T17:07:41.460+02:00'
  ref: '7682908166204599785'
  type: comment
- date: 20 June 2017 04:09
  html: "Hi!Your way of writing blog is so nice and interesting one too.Thank you.<br\
    \ /><a href=\"https://www.gclub-casino.com/baccarat/\" rel=\"nofollow\">\u0E1A\
    \u0E32\u0E04\u0E32\u0E23\u0E48\u0E32</a><br /><a href=\"https://www.gclub-casino.com\"\
    \ rel=\"nofollow\">gclub \u0E08\u0E35\u0E04\u0E25\u0E31\u0E1A</a><br /><a href=\"\
    http://www.gtznk.com\" rel=\"nofollow\">gclub casino</a>"
  id: '1971453690128264615'
  image: https://lh6.googleusercontent.com/-CjVBFmZR9ns/AAAAAAAAAAI/AAAAAAAAAC0/omdLm50Q090/s32-c/photo.jpg
  name: ly heng
  profile: https://www.blogger.com/profile/07168050563653074130
  pub: '2017-06-20T04:09:06.974+02:00'
  ref: '7682908166204599785'
  type: comment
- date: 21 November 2017 12:32
  html: Hi.<br />interesting blog and comments.<br /><br />As we&#39;re 9 months down
    the line, what is the situation now?<br /><br />AVS will not work at all in vSphere
    6.5 ?<br />Are Cisco working on a VM-based v-switch to replace AVS?<br />Raw speed
    is not important to me, I&#39;d rather have the functionality of an ACI-integrated
    vswitch.<br />I&#39;m not interested in using VMware&#39;s VDS, not in the slightest
    (I hope VMware are reading this!).<br />
  id: '1379515139965674363'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: wicksee
  profile: https://www.blogger.com/profile/15940142788428222663
  pub: '2017-11-21T12:32:36.055+01:00'
  ref: '7682908166204599785'
  type: comment
- date: 21 November 2017 12:35
  html: I think i&#39;ve answered my own question:<br />https://supportforums.cisco.com/t5/application-centric/aci-integration-with-vm/td-p/3030792<br
    />
  id: '577258843699433353'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: wicksee
  profile: https://www.blogger.com/profile/15940142788428222663
  pub: '2017-11-21T12:35:08.399+01:00'
  ref: '7682908166204599785'
  type: comment
count: 46
id: '7682908166204599785'
type: post
url: 2017/03/running-vsphere-on-cisco-aci-think-twice.html
