<div class="comments post" id="comments">
  <h4>8 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1994967595168326528">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/15242699990682926509" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1994967595168326528" href="#1994967595168326528">13 March 2017 11:44</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan - thanks for the post.  It&#39;s been a great project to work on.  When we started we found it very hard to find references at the scale we were planning.  I hope the White Paper will help others in this position and give something back to the community.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6619957364581778071">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08050361399160342780" rel="nofollow">Claudio Pires</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6619957364581778071" href="#6619957364581778071">13 March 2017 12:47</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan,<br />Thanks for posting this. <br />It&#39;s great to know this platform is following what you&#39;ve been teaching on your courses.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5380773003066368906">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://fri.uni-lj.si" rel="nofollow">Matjaz Pancur</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5380773003066368906" href="#5380773003066368906">13 March 2017 13:54</a>
              </span>
            </div>
            <div class="comment-content">Regarding Neutron&#39;s default implementation - things are not so bad as they used to be (e.g. see https://www.mirantis.com/blog/openstack-neutron-performance-and-scalability-testing-summary/). </div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1815535395349472347">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1815535395349472347" href="#1815535395349472347">13 March 2017 19:09</a>
              </span>
            </div>
            <div class="comment-content">&quot;not as bad as they used to be&quot; &lt;&lt; that&#39;s a nice summary. Anywhere near line rate only with latest NICs, latest kernel, and 9K MTU. Seems 1Gbps is still ludicrous speed in OpenStack world ;))<br /><br />http://blog.ipspace.net/2014/11/open-vswitch-performance-revisited.html</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="359035612868581076">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11059180104069090363" rel="nofollow">Ariel Liguori</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c359035612868581076" href="#359035612868581076">13 March 2017 22:39</a>
              </span>
            </div>
            <div class="comment-content">&quot;Don’t reinvent the wheel – use a commercial distribution &quot; -- been there, tried to move forward and find quite difficult, a commercial distro not only help you in the deployment also allow you to move on and be able to meet deadlines (internals and for your personal interest :) ), doing by hand is possible and you learn a lot but is not the path for all organizations for sure.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6765250730706335965">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/04847257511165693348" rel="nofollow">Albert Siersema</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6765250730706335965" href="#6765250730706335965">19 March 2017 17:13</a>
              </span>
            </div>
            <div class="comment-content">&quot;Don’t reinvent the wheel – use a commercial distribution&quot;<br />As usual: it really depends. Going with a commercial distribution has its fair share of drawbacks: financial scalability, vendor lock-in, bloat, debatable architectural choices &amp; decisions, required in-house knowledge or rather lack thereof. Don&#39;t fool yourself, if you choose to run a private cloud in the first place and depend on your vendor of choice to solve all or even most of the issues that will pop up, especially those high priority more complicated critical ones occuring at undesirable times... good luck to you. To rephrase an earlier post: been there. And be sure to research (continuous) upgrades; a nice installer and .ppt promises are worthless. Granted, recent commercial distributions are (getting) better, but if you already employ the required subject-savvy people and automation tools &amp; processes, rolling + maintaining your own deployment isn&#39;t all that complicated. You can still fall back on support from your vendor, but you&#39;re not completely dependent on it. Until vendors are willing to move from license pushers to actually offering services and building a symbiotic relationship with their customers where interests are aligned I&#39;d seriously consider all requirements, pro&#39;s &amp; con&#39;s. And not only the technical ones.<br />Might be your private cloud is a smaller setup for testing and development purposes only, or there are other reasons where using a commercial distribution makes perfectly sense. But I certainly wouldn&#39;t claim choosing a commercial OpenStack distribution is an obvious no-brainer.<br /><br />And as pointed out by Matjaz, as it&#39;s now 2017, even bare bones OVS neutron has moved on. It might not yet be able to saturate a 100GbE NIC in a single compute/network node, but I&#39;m not sure which scalable cloud workload would need that. It&#39;s sure nice to know neutron OVS would be able to drive 100GbE at line rate, but it&#39;s all about scaling out and I&#39;d say that you&#39;d be hitting a ToR leaf-spine oversubscription before running into compute node network I/O bound issues. Ceph is a valid use case for e.g. 25/40GbE NIC&#39;s, low latency (although there&#39;s still the code path), but as Ceph doesn&#39;t need neutron/OVS thats not an issue.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8165682209364676737">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8165682209364676737" href="#8165682209364676737">19 March 2017 18:06</a>
              </span>
            </div>
            <div class="comment-content">&quot;Going with a commercial distribution has its fair share of drawbacks&quot; &lt;&lt; Couldn&#39;t agree more. I&#39;m also positive people had exactly the same sentiment regarding Linux distros a decade ago. I don&#39;t know too many people building Linux from sources these days.<br /><br />Also, I know teams that spent a year setting up OpenStack from sources. I know other teams that tried (because it would be cheaper than buying something) and failed miserably wasting months and man-years.<br /><br />I think a fair summary would be &quot;if you never tried it before, start with a distro, and if you know what you&#39;re doing you&#39;re not listening to my rambling anyway&quot; Agreed?<br /><br />As for OVS performance, do read the details, and don&#39;t be misled by 9K MTU measurements. The performance documented in that report is still not anywhere close to where VMware vSwitch was years ago.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="865011451685971063">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/04847257511165693348" rel="nofollow">Albert Siersema</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c865011451685971063" href="#865011451685971063">19 March 2017 19:29</a>
              </span>
            </div>
            <div class="comment-content">I knew the Linux distro argument would pop up, and of course it&#39;s totally valid :-) I should have stated that I didn&#39;t necessarily meant building on OpenStack upstream but rather using distro OpenStack packages, with(out) vendor support if you like. Then deploy OpenStack with own orchestration tools, i.e. &quot;yum/apt install &amp; configuration files&quot;. This is what we did, using saltstack. There&#39;s even quite a number of downloadable deployment projects on e.g. github by now.<br />Ask if $vendor is willing to support their own packages when rolled out with your own orchestration tooling and/or see if $vendor will be using that fact to cover up a support organization lacking sufficient OpenStack know how (sadly that last addition isn&#39;t purely drivel from a cynical mind.)<br /><br />I might come across as grumpy rambling, just wanted to share a perspective from some folks who have gone through all this in reality. YMMV of course. I appreciate your blog and indeed it&#39;s awesome to see someone taking the effort to share their implementation. I hope we&#39;re allowed (time-wise as well) to do the same sometime soon.<br /><br />Starting out experiments or a PoC with an installer is certainly a good idea, even if that installer is only devstack/RDO. And using a commercial installer all the way will make and keep some people happy, sure of that. The point is, after the PoC you need to choose a path for your production private cloud. We&#39;ve experienced the pain of migrating production clouds away from a commercial installer to our own orchestration &amp; deployment (but we&#39;re still glad we did, and had very valid reasons for that move to begin with).<br /><br />Regarding OVS, obviously your VMware vSwitch comment is valid, I did read the details (and did some benchmarking ourselves after all, we had to choose an option). neutron/OVS is more like NSX though (multi tenant, self service, e.g. vxlan overlay). As we&#39;re not using NSX I haven&#39;t looked into actual NSX benchmarking.  <br />The Mirantis link fails to mention if they&#39;re using the native OVS firewall driver, already a cleaner &amp; faster (without the veth/linux bridges/eb- &amp; iptables mess).<br />Also see e.g.:<br />https://software.intel.com/en-us/articles/implementing-an-openstack-security-group-firewall-driver-using-ovs-learn-actions<br />And I still stand by my comment regarding VMWare (ESX) enterprise versus private cloudy workloads :-) I&#39;d rather scale out my cloud/deep learning/big data workloads over e.g. N racks with say 30-40 compute nodes per rack. Even with the current abominable OVS performance that could mean up to 40Gbps throughput per rack with a single ToR or up to double that if you&#39;re ECMP-ing over two ToR&#39;s (which is what we do). As Ceph is a valid and popular storage option, your design needs to include that traffic on the 10(/25?)GbE NIC(s) in your compute node as well (or sure, you could technically &amp; financially emulate a SAN by dedicating network infrastructure to Ceph IP storage).<br /></div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
