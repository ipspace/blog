comments:
- comments:
  - date: 27 June 2017 08:12
    html: That&#39;s a totally different story (the question was coming from a data
      center perspective). <br /><br />Jumbo frames are probably the least horrible
      option in transport networks to avoid customer traffic fragmentation (or worse).
      Will update the blog post.
    id: '2044436710218078672'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-06-27T08:12:52.154+02:00'
    ref: '543231050142712762'
    type: comment
  - date: 30 June 2017 22:12
    html: 'I&#39;d say :<br />- Transport networks : go-go-go ! Lowest denominator
      that I could identify is 9150/9164 (NCS5000)<br />- Hosts : IF you have a good
      reason to do it, don&#39;t go beyond 9000.<br /><br />As for the reasons to
      have jumbo on hosts, heavy transfers on networks disjoint from the Internet
      would be the only decent reason... and maybe still not decent enough...'
    id: '4103071631084750039'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: R.-Adrian F.
    profile: https://www.blogger.com/profile/11418671261451939355
    pub: '2017-06-30T22:12:47.583+02:00'
    ref: '543231050142712762'
    type: comment
  date: 27 June 2017 08:10
  html: What about jumbo frames &amp; tunnels e.g. IPIP, GRE, etc?<br />If you do
    not use jumbo frames you need to do fragmentation on routers terminating the tunnels
    or make the source reduce the frame size (PMTUD, etc). Do you see any benefits
    here?
  id: '543231050142712762'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Bogdan Golab
  profile: https://www.blogger.com/profile/12912702162710760711
  pub: '2017-06-27T08:10:07.072+02:00'
  ref: '5055437072622735902'
  type: comment
- date: 27 June 2017 09:10
  html: The title was very general but your content was more specific. That&#39;s
    way I added the comment. I am sure you know this side of the network perfectly
    (like me spending years just in-between tunnels and asymmetry).
  id: '3830077846104537817'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Bogdan Golab
  profile: https://www.blogger.com/profile/12912702162710760711
  pub: '2017-06-27T09:10:26.436+02:00'
  ref: '5055437072622735902'
  type: comment
- date: 27 June 2017 11:37
  html: One more interesting example of vmxnet3 driver and IPv6 (in case that &gt;1500
    is treated as jumbo frame) http://blog.donatas.net/blog/2016/02/24/vmxnet3-drops/
  id: '7294579598546104571'
  image: https://3.bp.blogspot.com/-0yDrxa0sfWI/V5sF6eo4q6I/AAAAAAAAHAs/n3VkBY82SbgXVnHCcCDSY7_sOZaHvufEwCK4B/s32/avatar.jpg
  name: Donatas Abraitis
  profile: https://www.blogger.com/profile/10268349264835307217
  pub: '2017-06-27T11:37:57.106+02:00'
  ref: '5055437072622735902'
  type: comment
- date: 27 June 2017 13:41
  html: The efforts to solve TCP Incast at a higher layer of the stack always felt
    somewhat misguided. An ethernet problem requires an ethernet solution, right?
    I have perused a number of research papers over the years and they all seem to
    overlook switching strategy. I found that cut-through switching is the only thing
    that has improved the situation. https://www.linkedin.com/pulse/solving-tcp-incast-cut-through-switching-dennis-olvany
  id: '4527057017695095651'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/06913023963950222205
  pub: '2017-06-27T13:41:32.009+02:00'
  ref: '5055437072622735902'
  type: comment
- comments:
  - date: 27 June 2017 18:45
    html: Might be hard to do @ 40GE/100GE speeds ;)
    id: '4613720543561779052'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-06-27T18:45:47.739+02:00'
    ref: '2530367401541363181'
    type: comment
  date: 27 June 2017 18:44
  html: Wan optimisation techniques using Cisco WAAS and Riverbed would surely be
    another way to optimise the TCP traffic flows...
  id: '2530367401541363181'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/18284379100886713671
  pub: '2017-06-27T18:44:31.137+02:00'
  ref: '5055437072622735902'
  type: comment
- date: 28 June 2017 00:07
  html: Nowadays the relative win with jumbos is effectively zero at 1G, occasionally
    useful at 10G.  When 10G first hit the scene on the host side (practically speaking
    &#39;04 -&#39;05) jumbos definitely did offer the possibility of allowing the
    CPU&#39;s at the time to avoid interrupt saturation long enough to actually saturate
    the wire (nb - I recall bigger Sun boxes at the time gaining 30-40% useful throughput
    by enabling jumbos).  <br /><br />That said, with modern CPU&#39;s (...call it
    Nehalem generation and newer on the x86 side) the benefits of jumbos on 10GE tend
    to be marginal at best.  For much the same reasons as above, though, at 40- or
    100- gigabit it&#39;s absolutely worthwhile as, again, most implementations end
    up being bottlenecked by interrupt handling on a single core and a 5X-6X reduction
    in packets processed can represent a lot more useful throughput on the wire (...especially
    in bulk-data situations, like storage/backup).  <br /><br />It is, of course,
    likely that CPU speeds will eventually catch up - or, perhaps more realistically,
    we&#39;ll see practical PCIe limitations, host economics and app methodologies
    start to drive 25G and 50G (...thus reducing the aggregate need for jumbos from
    the point of view of CPU capacity).  Either way being very conservative about
    the use of jumbos is a good idea, as is being incredibly strict about making sure
    their use is both consistent between host and network and fully manageable/repeatable
    in operation.<br />
  id: '66632283765739114'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/17375766778583869461
  pub: '2017-06-28T00:07:48.261+02:00'
  ref: '5055437072622735902'
  type: comment
- comments:
  - date: 29 June 2017 17:40
    html: 'You opened a huge can of worms:<br /><br />Defaults: It would make sense
      to make jumbo frames default on L2 switches in 2017, but having different platforms
      (or different software releases) with different defaults is a recipe for disaster.<br
      /><br />Enabling jumbo frames by default: it&#39;s one more parameter that has
      to be consistent across all devices, and is easy to miss. No problem if you
      automated configuration deployment. Hope you already did it. Many others didn&#39;t.<br
      /><br />Having jumbo frames in network, but not on end devices: what happens
      when a non-jumbo host OS receives a jumbo frame? Sometimes it depends on the
      protocol (v4 versus v6). Do you really want to know?'
    id: '1349059023965081762'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2017-06-29T17:40:46.425+02:00'
    ref: '5977963084379121438'
    type: comment
  - date: 29 June 2017 20:02
    html: There is a reddit thread on this very blog post. It reveals some strong
      consensus on the topic. https://www.reddit.com/r/networking/comments/6jvjk2/to_jumbo_or_not_to_jumbo/
    id: '5554017396242876846'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Unknown
    profile: https://www.blogger.com/profile/06913023963950222205
    pub: '2017-06-29T20:02:24.852+02:00'
    ref: '5977963084379121438'
    type: comment
  date: 29 June 2017 16:27
  html: There might not be a lot to be -gained- by enabling jumbo frames, but on the
    other hand what is there to be lost by enabling them?  I have never understood
    why the default layer 2 MTU on Cisco switches was not to allow the maximum possible
    frame MTU.<br /><br />Now the Layer 3 (IP) MTU is something entirely different,
    and I agree that that should be 1500 by default.<br /><br />But what&#39;s the
    problem with enabling jumbo frames by default on a switch and leave it up to the
    end hosts/administrators/operators if they want to enable it on their storage
    or server system NICs or not?<br /><br />I can&#39;t think of any myself, and
    I&#39;ve never heard anyone come up with a reason why that Jumbo Frames on by
    default wouldn&#39;t be the most flexible way of setting a switch up.  As far
    as I know there are no adverse effects from having a smaller MTU on the end hosts
    than the switch in the middle.  Nor are there any by enabling Jumbo Frames but
    not using them.  But there certainly are ill effects of enabling Jumbo Frames
    on end hosts but not on the switches and L2 links in between them.
  id: '5977963084379121438'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Reuben
  profile: https://www.blogger.com/profile/00579758736650236902
  pub: '2017-06-29T16:27:06.040+02:00'
  ref: '5055437072622735902'
  type: comment
- date: 10 July 2017 21:13
  html: I just had a multicast publisher pushing messages to a solar flare nic for
    packaging and the message size being pushed was causing ip fragments to be sent
    on wire. The fragmentation was causing reordering across some lags because the
    multicast hash used ip src-dst + src-dst-port if available and ip src-dst if not.<br
    /><br />Cranking up the MTU a bit solved this issue but we could have decreased
    the msg size at the cost of an increase in pps and fixed the reordering as well.  The
    box did NOT support changing the multicast hash.
  id: '7599161285841983859'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: DuaneO
  profile: https://www.blogger.com/profile/09571175342652550804
  pub: '2017-07-10T21:13:06.523+02:00'
  ref: '5055437072622735902'
  type: comment
count: 13
id: '5055437072622735902'
type: post
url: 2017/06/to-jumbo-or-not-to-jumbo.html
