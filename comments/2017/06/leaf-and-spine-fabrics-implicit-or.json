{
  "comments": [
    {
      "comments": [
        {
          "date": "07 June 2017 15:25",
          "html": "I didn&#39;t say &quot;reload&quot;. I said &quot;fail&quot; ;)) ... and if you have 6 spine switches, you don&#39;t need any of the high-availability mechanisms, but you can&#39;t get rid of them in a chassis switch (of course you can decide not to use them).",
          "id": "6639023314129203575",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2017-06-07T15:25:30.306+02:00",
          "ref": "5556754104609469041",
          "type": "comment"
        }
      ],
      "date": "07 June 2017 13:20",
      "html": "&quot;Large chassis switches are precious, and you don\u2019t want them to fail. Ever. Welcome to the morass of ISSU, NSF, graceful restart.&quot;<br /><br />Well, it depends. :) Even a large enterprise, working 24/7/365, has less busy hours when you can afford losing 16-25% of your bandwidth (with 4-6 chassis switches as spines). In such case, just reload that switch and have a coffee while it&#39;s booting.<br /><br />Disclaimer: large enterprise \u2260 public cloud providers (obviously).",
      "id": "5556754104609469041",
      "image": "https://4.bp.blogspot.com/-ZOXsPp2jWZM/WLFLiMZondI/AAAAAAAADJ4/Ut7WBaNvJIInOF_9feWqAIeZiw3nyuEmwCK4B/s32/en2.jpg",
      "name": "Andras Dosztal",
      "profile": "https://www.blogger.com/profile/04707560608698340062",
      "pub": "2017-06-07T13:20:12.093+02:00",
      "ref": "6199992007755310582",
      "type": "comment"
    },
    {
      "date": "07 June 2017 13:23",
      "html": "I 200% agree, that it doesn&#39;t matter how many switches do you manage if it&#39;s already automated. I prefer small devices over big chassis.",
      "id": "5235783799148539445",
      "image": "https://3.bp.blogspot.com/-0yDrxa0sfWI/V5sF6eo4q6I/AAAAAAAAHAs/n3VkBY82SbgXVnHCcCDSY7_sOZaHvufEwCK4B/s32/avatar.jpg",
      "name": "Donatas Abraitis",
      "profile": "https://www.blogger.com/profile/10268349264835307217",
      "pub": "2017-06-07T13:23:36.472+02:00",
      "ref": "6199992007755310582",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "07 June 2017 18:38",
          "html": "Have you read the whole blog post and followed the links?",
          "id": "444515762781378050",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2017-06-07T18:38:10.436+02:00",
          "ref": "1849515777713988734",
          "type": "comment"
        }
      ],
      "date": "07 June 2017 18:13",
      "html": "You can use a chassis with all the downsides you mentioned. The Facebook Backpack or the similar model from Accton which are basically a CLOS in a box, would take away most of those arguments. <br /><br />What would you think of using those chassis? <br /><br />",
      "id": "1849515777713988734",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Attilla",
      "profile": null,
      "pub": "2017-06-07T18:13:07.543+02:00",
      "ref": "6199992007755310582",
      "type": "comment"
    },
    {
      "date": "08 June 2017 14:59",
      "html": "I agree... smaller switches tend to result in smaller failure domains, simpler connectivity issues and increased overal standardised behaviour... when automating all provisioning &amp; maintenance operations, then even the length of each patch cable in the DC could be pre-provisioned and its length pre-calculated etc... Its how i know some (really) big DCs run their DC Fabrics rather effectively. To me, bigger chassis for switching tends to give bigger problems... smaller pizza box type of components seems the IKEA style of DC switch fabrics and delivers its job perfectly while at same time give benefit in cost and quality and understanding in components used...",
      "id": "6766775450628515198",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Gunter Van de Velde",
      "profile": null,
      "pub": "2017-06-08T14:59:50.473+02:00",
      "ref": "6199992007755310582",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "09 June 2017 16:29",
          "html": "I really don&#39;t understand why you&#39;d use layer-2 across multiple management switches (unless layer-3 switches were too expensive ;). After all, wiring is fixed, addresses are fixed, DHCP relaying is usually available on decent switches...",
          "id": "5757457903258398878",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2017-06-09T16:29:01.536+02:00",
          "ref": "2473710391201817832",
          "type": "comment"
        },
        {
          "date": "09 June 2017 20:37",
          "html": "thanks for your feedback Ivan, I really appreciate it. <br /><br />We have layer-2 across multiple management switches partially due to the legacy reasons. We are using juniper EX copper switches bonded in virtual chassis, so there is no problem with layer-3, or DHCP support. VC size is limited up to 10 switches, so we have a couple of VC clusters. Each cluster is a separate layer-3 domain. <br />The alternative solution would be having a dedicated small management network per rack. It has some drawbacks, such as - additional complexity (will have to write additional ansible templates for management switches), subnetting existing management subnet or introducing new management subnets. ",
          "id": "5389837766880231064",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Roman Romanyak",
          "profile": null,
          "pub": "2017-06-09T20:37:17.351+02:00",
          "ref": "2473710391201817832",
          "type": "comment"
        }
      ],
      "date": "09 June 2017 16:23",
      "html": "What&#39;s your opinion about management network? We build a spine and leaf network, but every switch is still connected to one big layer-2 management switch. Until recently we didn&#39;t have stp and storm control enabled in management network, and we accidentally created a l2 loop while adding a virtual chassis member to the management network. This l2 loop brought down RE on all ip fabric switches, some REs even dumped core. After that we enabled rstp and storm control, retested the loop and everything is fine. But we are wondering whether we should break down this l2 management network into smaller domains. Not sure what is the best practice...",
      "id": "2473710391201817832",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Roman Romanyak",
      "profile": null,
      "pub": "2017-06-09T16:23:34.109+02:00",
      "ref": "6199992007755310582",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "01 December 2017 08:44",
          "html": "I actually have a blog post written on the subject, just have to publish it. And yes, you&#39;re correct, VCF is a single failure domain.<br /><br />I know a very large customer who&#39;s using two QFabric fabrics per data center (very much like SAN-A/SAN-B), so your idea of using two VCFs is not exactly outlandish.",
          "id": "5982943575273182702",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2017-12-01T08:44:29.907+01:00",
          "ref": "1905914115943331807",
          "type": "comment"
        },
        {
          "date": "01 December 2017 09:18",
          "html": "I&#39;ll wait for the blogpost then! :)<br /><br />My guess:<br /><br />MC-LAG/VC/VCF/QFabric/JunOS Fusion: single<br />IP Fabric/EVPN VXLAN: not a single<br />NSX (not really the same thing but you get the idea I hope): I don&#39;t know enough, I hope not single!<br /><br />At the end of the day I guess that if the devices are able to work completely indepedently of each other for all operations (except exchanging routes to the neighbours and forwarding &amp; receiving frames) then its not a single failure domain",
          "id": "3413528222314841189",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Pere Camps",
          "profile": "https://www.blogger.com/profile/11059947665554553037",
          "pub": "2017-12-01T09:18:42.331+01:00",
          "ref": "1905914115943331807",
          "type": "comment"
        }
      ],
      "date": "01 December 2017 00:39",
      "html": "Hello!<br /><br />Don&#39;t know if this is the right blog post to comment this on, but this seems relatively appropiate and it is far newer than the rest. :)<br /><br />I&#39;m considering deploying Juniper&#39;s Virtual Chassis Fabric in my core and I&#39;m facing a dilemma<br /><br />I&#39;ve never used VCF before and I don&#39;t know how stable it is during operations, how do upgrades really work and what happens when individual switches fail.<br /><br />I&#39;m so unsure of it all that I&#39;m thinking that I should consider a VCF a single failure domain (even if it is multiple discrete switches) and if we end up going that way then we really need 2x VCFs setup in the datacentre (with all that it entails -- reminds me of FC)<br /><br />If we move around the issue a little we could say: EVPN VXLAN and an overlay... could I then consider having a single &#39;fabric&#39;? <br /><br />In summary: given the tons of different &#39;fabric&#39; options that we have these days, which ones could be considered a &quot;single failure domain&quot; and which ones not?<br /><br />Any thoughts?",
      "id": "1905914115943331807",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Pere Camps",
      "profile": "https://www.blogger.com/profile/11059947665554553037",
      "pub": "2017-12-01T00:39:21.016+01:00",
      "ref": "6199992007755310582",
      "type": "comment"
    }
  ],
  "count": 12,
  "id": "6199992007755310582",
  "type": "post",
  "url": "2017/06/leaf-and-spine-fabrics-implicit-or.html"
}