<div class="comments post" id="comments">
  <h4>13 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="543231050142712762">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c543231050142712762" href="#543231050142712762">27 June 2017 08:10</a>
              </span>
            </div>
            <div class="comment-content">What about jumbo frames &amp; tunnels e.g. IPIP, GRE, etc?<br />If you do not use jumbo frames you need to do fragmentation on routers terminating the tunnels or make the source reduce the frame size (PMTUD, etc). Do you see any benefits here?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="2044436710218078672">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2044436710218078672" href="#2044436710218078672">27 June 2017 08:12</a>
              </span>
            </div>
            <div class="comment-content">That&#39;s a totally different story (the question was coming from a data center perspective). <br /><br />Jumbo frames are probably the least horrible option in transport networks to avoid customer traffic fragmentation (or worse). Will update the blog post.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4103071631084750039">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11418671261451939355" rel="nofollow">R.-Adrian F.</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4103071631084750039" href="#4103071631084750039">30 June 2017 22:12</a>
              </span>
            </div>
            <div class="comment-content">I&#39;d say :<br />- Transport networks : go-go-go ! Lowest denominator that I could identify is 9150/9164 (NCS5000)<br />- Hosts : IF you have a good reason to do it, don&#39;t go beyond 9000.<br /><br />As for the reasons to have jumbo on hosts, heavy transfers on networks disjoint from the Internet would be the only decent reason... and maybe still not decent enough...</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3830077846104537817">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12912702162710760711" rel="nofollow">Bogdan Golab</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3830077846104537817" href="#3830077846104537817">27 June 2017 09:10</a>
              </span>
            </div>
            <div class="comment-content">The title was very general but your content was more specific. That&#39;s way I added the comment. I am sure you know this side of the network perfectly (like me spending years just in-between tunnels and asymmetry).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7294579598546104571">
          <!--
          <div class="avatar-image-container">
            <img src="https://3.bp.blogspot.com/-0yDrxa0sfWI/V5sF6eo4q6I/AAAAAAAAHAs/n3VkBY82SbgXVnHCcCDSY7_sOZaHvufEwCK4B/s32/avatar.jpg">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/10268349264835307217" rel="nofollow">Donatas Abraitis</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7294579598546104571" href="#7294579598546104571">27 June 2017 11:37</a>
              </span>
            </div>
            <div class="comment-content">One more interesting example of vmxnet3 driver and IPv6 (in case that &gt;1500 is treated as jumbo frame) http://blog.donatas.net/blog/2016/02/24/vmxnet3-drops/</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4527057017695095651">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06913023963950222205" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4527057017695095651" href="#4527057017695095651">27 June 2017 13:41</a>
              </span>
            </div>
            <div class="comment-content">The efforts to solve TCP Incast at a higher layer of the stack always felt somewhat misguided. An ethernet problem requires an ethernet solution, right? I have perused a number of research papers over the years and they all seem to overlook switching strategy. I found that cut-through switching is the only thing that has improved the situation. https://www.linkedin.com/pulse/solving-tcp-incast-cut-through-switching-dennis-olvany</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2530367401541363181">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/18284379100886713671" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2530367401541363181" href="#2530367401541363181">27 June 2017 18:44</a>
              </span>
            </div>
            <div class="comment-content">Wan optimisation techniques using Cisco WAAS and Riverbed would surely be another way to optimise the TCP traffic flows...</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4613720543561779052">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4613720543561779052" href="#4613720543561779052">27 June 2017 18:45</a>
              </span>
            </div>
            <div class="comment-content">Might be hard to do @ 40GE/100GE speeds ;)</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="66632283765739114">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17375766778583869461" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c66632283765739114" href="#66632283765739114">28 June 2017 00:07</a>
              </span>
            </div>
            <div class="comment-content">Nowadays the relative win with jumbos is effectively zero at 1G, occasionally useful at 10G.  When 10G first hit the scene on the host side (practically speaking &#39;04 -&#39;05) jumbos definitely did offer the possibility of allowing the CPU&#39;s at the time to avoid interrupt saturation long enough to actually saturate the wire (nb - I recall bigger Sun boxes at the time gaining 30-40% useful throughput by enabling jumbos).  <br /><br />That said, with modern CPU&#39;s (...call it Nehalem generation and newer on the x86 side) the benefits of jumbos on 10GE tend to be marginal at best.  For much the same reasons as above, though, at 40- or 100- gigabit it&#39;s absolutely worthwhile as, again, most implementations end up being bottlenecked by interrupt handling on a single core and a 5X-6X reduction in packets processed can represent a lot more useful throughput on the wire (...especially in bulk-data situations, like storage/backup).  <br /><br />It is, of course, likely that CPU speeds will eventually catch up - or, perhaps more realistically, we&#39;ll see practical PCIe limitations, host economics and app methodologies start to drive 25G and 50G (...thus reducing the aggregate need for jumbos from the point of view of CPU capacity).  Either way being very conservative about the use of jumbos is a good idea, as is being incredibly strict about making sure their use is both consistent between host and network and fully manageable/repeatable in operation.<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5977963084379121438">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/00579758736650236902" rel="nofollow">Reuben</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5977963084379121438" href="#5977963084379121438">29 June 2017 16:27</a>
              </span>
            </div>
            <div class="comment-content">There might not be a lot to be -gained- by enabling jumbo frames, but on the other hand what is there to be lost by enabling them?  I have never understood why the default layer 2 MTU on Cisco switches was not to allow the maximum possible frame MTU.<br /><br />Now the Layer 3 (IP) MTU is something entirely different, and I agree that that should be 1500 by default.<br /><br />But what&#39;s the problem with enabling jumbo frames by default on a switch and leave it up to the end hosts/administrators/operators if they want to enable it on their storage or server system NICs or not?<br /><br />I can&#39;t think of any myself, and I&#39;ve never heard anyone come up with a reason why that Jumbo Frames on by default wouldn&#39;t be the most flexible way of setting a switch up.  As far as I know there are no adverse effects from having a smaller MTU on the end hosts than the switch in the middle.  Nor are there any by enabling Jumbo Frames but not using them.  But there certainly are ill effects of enabling Jumbo Frames on end hosts but not on the switches and L2 links in between them.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1349059023965081762">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1349059023965081762" href="#1349059023965081762">29 June 2017 17:40</a>
              </span>
            </div>
            <div class="comment-content">You opened a huge can of worms:<br /><br />Defaults: It would make sense to make jumbo frames default on L2 switches in 2017, but having different platforms (or different software releases) with different defaults is a recipe for disaster.<br /><br />Enabling jumbo frames by default: it&#39;s one more parameter that has to be consistent across all devices, and is easy to miss. No problem if you automated configuration deployment. Hope you already did it. Many others didn&#39;t.<br /><br />Having jumbo frames in network, but not on end devices: what happens when a non-jumbo host OS receives a jumbo frame? Sometimes it depends on the protocol (v4 versus v6). Do you really want to know?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5554017396242876846">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06913023963950222205" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5554017396242876846" href="#5554017396242876846">29 June 2017 20:02</a>
              </span>
            </div>
            <div class="comment-content">There is a reddit thread on this very blog post. It reveals some strong consensus on the topic. https://www.reddit.com/r/networking/comments/6jvjk2/to_jumbo_or_not_to_jumbo/</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7599161285841983859">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09571175342652550804" rel="nofollow">DuaneO</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7599161285841983859" href="#7599161285841983859">10 July 2017 21:13</a>
              </span>
            </div>
            <div class="comment-content">I just had a multicast publisher pushing messages to a solar flare nic for packaging and the message size being pushed was causing ip fragments to be sent on wire. The fragmentation was causing reordering across some lags because the multicast hash used ip src-dst + src-dst-port if available and ip src-dst if not.<br /><br />Cranking up the MTU a bit solved this issue but we could have decreased the msg size at the cost of an increase in pps and fixed the reordering as well.  The box did NOT support changing the multicast hash.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
