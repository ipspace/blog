{
   "comments": [
      {
         "date": "04 November 2020 09:57",
         "html": "I have done real lab tests with IOS-XR based routers in the following scenario:<br />\n- ECMP links running IS-IS<br />\n- IS-IS + Segment Routing + TI-LFA<br />\n\n<p>Topology: 4 x ECMP link between two routers</p>\n\n<p>This is what happens:\nYou loose link #1: ECMP is still available: Minimal traffic loss (&lt;10ms) as traffic is simply re-distributed onto other ECMP links\nYou loose link #2: Same happens as for link #1\nYou loose link #3: Fast failover to last remaining link. BUT since ECMP is gone, TI-LFA will now calculate a backup path\nYou loose link #4: TI-LFA will provide fast failure, assuming there is another path available.</p>\n\n<p>I <em>think</em> the ECMP behavior is independent of the routing protocol, because it is implemented in CEF / hardware. But have not tested it properly with LDP / BGP etc. myself.</p>\n",
         "id": "201",
         "name": " Jan",
         "pub": "2020-11-04T09:57:33",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:13",
         "html": "<p>@Jan: Thanks for the data! </p>\n\n<p>However, 10 msec loss could still be caused by CPU reprogramming the ECMP buckets. Have you experienced any difference in how long the outage was based on whether you had pure ECMP or LFA on top of it?</p>\n\n<p>Thank you! Ivan</p>\n",
         "id": "202",
         "name": "Ivan Pepelnjak",
         "pub": "2020-11-04T11:13:06",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:38",
         "html": "<p>One thing to note is that in a pure spine-and-leaf network, you can, or perhaps even <em>will</em>, get loops when a link goes down, until the routing protocol has converged.</p>\n\n<p>Consider a network with two spines (S1,S2) and three leafs (L1, L2, L3).  A host connected to L1 send a packet to a host on L2.  L1 decides to send it to spine S1, but unbeknownst to the leafs, the link between S1 and L2 has gone down.  S1 has realized this, and reprogrammed itself, so when it receives the packet that needs to go to L2, it will try to send that to one of the other leafs, L1 or L3, and hope that they will send it to the <em>other</em> spine, S2, since it is the remaining path to L2.</p>\n\n<p>But, since L1 and L3 have not yet realized that S1 no longer has a link to L2, they may decide to send the packet back to S1.  In particular, L1 will almost certainly hash the packet the same way as when it got it the first time, and send it to S1 again.  Which will hash it the same way <em>it</em> did previously, and send it to L1.  Loop.</p>\n\n<p>Until the routing protocol converges a second or two later.</p>\n\n<p>This is inherent in a pure spine-and-leaf network, and the only way to avoid it is to have a less pure network design.  For example:</p>\n\n<ul>\n<li>Have multiple links between each spine and leaf.  If there had been two links between S1 and L2, the problem would not have happened, as S1 would just send the packet over the other direct link to L2.</li>\n<li>Connect the spines in a ring.  S1 would then have a shorter backup path towards L2 available (via S2) than by sending the packet back to some leaf.  (I personally like this design.)</li>\n</ul>\n\n<p>Whether you <em>need</em> that fast rerouting in your network, or if you can wait until the routing protocol converges, is of course a different question.  And likely depends on how <em>often</em> you have link failures. In the datacenter network I&#39;m managing, where links fail almost never, we would be OK with a convergence time of a minute or more; but our OSPF converges in a second or two, so no problem there.  (We still have the spines connected to each other, but for other reasons.)</p>\n",
         "id": "203",
         "name": " Bellman",
         "pub": "2020-11-04T11:38:18",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:44",
         "html": "<p>@Bellman: the behavior you describe depends on whether you designed a <a href=\"https://blog.ipspace.net/2018/09/valley-free-routing-in-data-center.html\">valley-free routing topology</a> (in which case you&#39;ll experience packet drops until the routing protocol does its job, and a <a href=\"https://blog.ipspace.net/2018/09/implications-of-valley-free-routing-in.html\">few other interesting things</a>) or not (in which case you&#39;ll experience path hunting and temporary loops).</p>\n\n<p>Even LFA wouldn&#39;t help in a leaf-and-spine fabric, you&#39;d need remote LFA to get to another spine switch.</p>\n",
         "id": "204",
         "name": "Ivan Pepelnjak",
         "pub": "2020-11-04T11:44:37",
         "type": "comment"
      },
      {
         "date": "04 November 2020 01:17",
         "html": "<p>@Jan: thanks for your contribution. It&#39;s interesting as we&#39;re seeing a different behaviour. In our IOSXR, by default, LFA is calculated for ECMP&#39;ed prefixes and thus it kicks in even if just one link fails (e.g. your link #1). What&#39;s your configuration ? Have you excluded somehow via CLI the LFA computation for ECMP&#39;ed prefixes ? </p>\n",
         "id": "205",
         "name": "Andrea Di Donato",
         "pub": "2020-11-04T13:17:32",
         "type": "comment"
      },
      {
         "date": "04 November 2020 04:07",
         "html": "Ivan:<br />\n\n<p>Yes, you can certainly configure your network so the spines drop packages instead of trying to route around the broken link.</p>\n\n<p>However, will you be <em>happier</em> because the spine is now dropping 100% of the traffic it receives destined for L2, instead of looping (and then dropping) a fraction of that same traffic?  This is in the context of wanting very quick failovers, faster than the routing protocol can react, so presumably you you want as small outages as possible.</p>\n\n<p>(And if you [generic you, not specifically you, Ivan] prefer valley-free routing, remember to consider what happens if the broken link is the one to the leaf where your monitoring and/or management stations are connected.  Suddenly you have no way of managing and monitoring that spine...  Unless those stations are dual-homed to two leaf routers.  A physically separate management network just moves the problem to that network; or are valley-free proponents OK with a valleyed management network?)</p>\n\n<p>&gt; Even LFA wouldn&#39;t help in a leaf-and-spine fabric, you&#39;d need\n&gt; remote LFA to get to another spine switch.</p>\n\n<p>Yes, that was exactly the point I was trying to make!  But I could obviously have been clearer about that.  A pure leaf-and-spine network inherently does not <em>have</em> a Loop-Free Alternative from the spines.  You need to break the topology somehow.  Tunnels to the other spines are one way of doing that, physical links to neighbouring spines another (the one I personally like best), and so on.</p>\n\n<p>(This was all a bit of a side-note to your main post.  I hope I have not derailed the discussion too much.)</p>\n",
         "id": "206",
         "name": "Bellman",
         "pub": "2020-11-04T16:07:01",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:44",
         "html": "<p>I have addressed some of it in one of &ldquo;between 0x2 nerds&rdquo; webinars.\nIn general - you are comparing 2 different techniques - IP FRR vs fast-rehash. IP FRR (xLFA) relies on pre-computed backup next-hop that is loop-free (could be ECMP) and is a control plane function (eventually end-result is downloaded into HW), it could take into consideration some additional data - SRLGs, interface load, etc.\nFast-rehash is a forwarding construct, where the next-hop (could be called differently) is not a single entry but an array of entries (ECMP bundle as downloaded by the control plane).\nIf one of them becomes unavailable (BFD or LoS or interface down events) it is simply removed from the array and the hashing is updated accordingly, since the name.\nUsually - you&rsquo;d see LFA implemented on a high end routers, it is much more intelligent/complex and provides non connected bypass (rLFA/TI-LFA).\nFast-rehash on contrary protects only connected links and doesn&rsquo;t require any additional computation (ECMP alternatives are per definition loop-free). Usually implemented in DC environments. Hope this explains it.\nIP FRR RFCs are produced by IETF RTGWG</p>\n",
         "id": "207",
         "name": " JeffT",
         "pub": "2020-11-04T23:44:14",
         "type": "comment"
      },
      {
         "date": "05 November 2020 12:20",
         "html": "<p>How about Open EIGRP for it&#39;s use of Successor/Feasible successors. LOL</p>\n",
         "id": "208",
         "name": "Jeff Sicuranza",
         "pub": "2020-11-05T00:20:27",
         "type": "comment"
      },
      {
         "date": "05 November 2020 03:42",
         "html": "<p>@Ivan: I double checked my notes and here is what I learned during lab testing, also checking with our engineering colleagues.</p>\n\n<p>Disclaimer:\n- I work for Cisco&#39;s CX organization.\n- Testing was done on Broadcom based XR platforms (NCS 5500). Might actually be different on other XR platforms, since the HW implementation DOES play a role.</p>\n\n<p>Hardware protection / programming for ECMP links is in fact (as written above) only activated when also activating a FRR feature (LFA, rLFA or TI-LFA).\nWithout it, no backup path is programmed in hardware and link down notifies IGP to start (re)convergence. </p>\n\n<p>With FRR enabled, it is pure hardware protection. Official convergence target is &lt;50ms, but might be quicker.\nIn testing, you can also see that not every traffic stream is affected. Most don&#39;t see any traffic loss, but a few packets where in the wrong NPU at the wrong time ;)</p>\n\n<p>@Andrea: I only tested with TI-LFA. I know that the LFA / rLFA implementation is quite different from TI-LFA so it could well be that they behave differently.</p>\n",
         "id": "209",
         "name": " Jan",
         "pub": "2020-11-05T15:42:30",
         "type": "comment"
      },
      {
         "date": "06 November 2020 01:35",
         "html": "<p>@JeffT: Thanks for your contribution &ndash; much appreciated. Will definitely go and watch &ldquo;between 0x2 nerds&rdquo; webinar asap. </p>\n\n<p>My objection is that in a link-protection only scenario the LFA solution is suboptimal if compared to ECMP/fast-rehashing. </p>\n\n<p>I&rsquo;ll give you an example and will provide you with some associated mumbling/maths. Give me a shout if it doesn&#39;t make any sense as am hungry for conjecture&rsquo;s confirmation.</p>\n\n<p>The way we think IOSXR behaves in the below scenario is the following: </p>\n\n<p>=====================================================================================\nSay there are D prefixes X1,...,Xd with 5 ECMP NH (i.e. 1,2,3,4,5) and 15 buckets</p>\n\n<p>At regime we have ECMP and therefore the hash bucket allocation is the following 123451234512345</p>\n\n<p>If link3 fails then the following should happen with IOSXR</p>\n\n<p>for pfx X1, link 3 is protected by link 4 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,4,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,4,4,5,1,2,4,4,5,1,2,4,4,5</p>\n\n<p>for pfx X2, link 3 is protected by link 5 and thus the NH pointer moves from the NH_GROUP [1,2,3,4,5] to NH_GROUP [1,2,5,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,5,4,5,1,2,5,4,5,1,2,5,4,5</p>\n\n<p>for pfx X3, link 3 is protected by link 1 and thus the NH pointer moves from the NH_GROUP [1,2,3,4,5] to NH_GROUP [1,2,1,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,1,4,5,1,2,1,4,5,1,2,1,4,5</p>\n\n<p>for pfx X4, link 3 is protected by link 2 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,2,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,2,4,5,1,2,2,4,5,1,2,2,4,5</p>\n\n<p>for pfx X5, link 3 is protected by link 4 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,4,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,4,4,5,1,2,4,4,5,1,2,4,4,5</p>\n\n<p>...</p>\n\n<p>for pfx Xd, link 3 is protected by link 2 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,2,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,2,4,5,1,2,2,4,5,1,2,2,4,5</p>\n\n<p>================================================================================\nNow, if am not mistaken, calling H the number of the ECMP NHs and thus generalising what just shown but without delving into the maths (I can but I&rsquo;d avoid this for now), we can observe that in the LFA scenario a link, say link 4, during link 3 protection has a load in terms of number of flows equivalent to the flows of one particular prefix group (out of the H-1 prefix groups)  with weight 2/H plus the load of all of the other H-2 prefix groups&rsquo; flows with weight 1/H. In an ECMP scenario instead that very same link 4  during protection has a load in terms of number of flows equivalent to the total number of flows across all of the prefix groups and with weight 1/(H-1). \nComparing the two values you can tell that the LFA protection is not robust (risking link saturation and/or qos thresholds crossing) against a disproportion in terms of number of flows per destination/prefix amongst the (H-1) groups of ECMP&rsquo;ed prefixes (e.g.. in an IP tunneling environments where all of the VxLAN, GTP, GRE destinations happen to be in the very same prefix group out of the H-1 groups of prefixes that has a weight of 2/H over link 4). \nHaving said that, one argument that I heard around is that this could be mitigated/counter-measured if these additional number of flows had a relatively low per-flow bw if compared with non-tunneled destinations&rsquo; flows. Should this (at all) happen though, it would mean that the (ECMP) per-flow load-balancing @ regime (with no fault) would be already crap as some flows (those not tunneled) have a much higher bw than the tunneled ones which is a contradiction in terms to me. We&rsquo;re here assuming by the way that flow-based load balancing within these tunnels  (e.g. GTP, GRE,VxLAN,.. ) is supported by the chipsets. <br />\nLast but not least, we should mention that LFA, as opposed to ECMP, is not resilient to a double fault as if the link and its backup link failed then LFA would actually provide&hellip; fast-discard ??</p>\n\n<p>Cheers</p>\n\n<p>Andrea</p>\n",
         "id": "218",
         "name": "Andrea Di Donato",
         "pub": "2020-11-06T13:35:26",
         "type": "comment"
      },
      {
         "date": "09 November 2020 11:04",
         "html": "Let me try to address in order:<br />\n@Ivan - 10ms includes failure detection, not just reprogramming HW, so it depends on how fast the failure is detected/propagated to HW.<br />\nIn previous life, implementing IPFRR on E/// SSR (EZ), we got HW performance tuned to 2K NH changes per 1ms. Jericho (to my memory) would be using SuperFEC structure and should be much faster.<br />\n@Andrea - I don&#39;t work for Cisco and can&#39;t comment on internals of their hashing. It would also be a reasonable assumption that most chipsets (COTS) do 5 tuple hashing (not looking into GTP TEID or VXLAN VNI).<br />\nIn most implementations, there&#39;s no load-based rebalancing within ECMP group (sticky), and ECMP is the only key to build the group (really basic grouping). LFA on contrary is computed by the control plane and can take a number of additional points into computation, SRLGs, common line cards, load on the link, etc.<br />\nThe intent is often - give me a node protecting LFA and if there isn&#39;t, link protecting would do. Also note that implementation of any none directly connected protection schemes (TI-LFA) is an order of magnitude more complex than basic LFA.<br />\nMy point is - this is not an apple to apple comparison, there&#39;s more to it.<br />\n\n\n",
         "id": "221",
         "name": "Jeff Tantsura",
         "pub": "2020-11-09T23:04:49",
         "type": "comment"
      },
      {
         "date": "09 November 2020 11:12",
         "html": "<p>@Jan\nLFA is indeed very different than xLFA.\nLFA - protect interface A(prefix optionally) by interface B (given it is loop-free within the constrains), when A goes down flip a pointer.\nTI-LFA - if not local protection available (e.g LFA), compute a tunnel that terminates at the PQ node, protect interface A by sending traffic into the tunnel that is preinstantiated in HW</p>\n",
         "id": "222",
         "name": "Jeff Tantsura",
         "pub": "2020-11-09T23:12:45",
         "type": "comment"
      },
      {
         "date": "09 November 2020 11:25",
         "html": "<p>@Andrea - &quot;Last but not least, we should mention that LFA, as opposed to ECMP, is not resilient to a double fault as if the link and its backup link failed then LFA would actually provide&hellip; fast-discard ??&quot;</p>\n\n<p>Neither technology is resilient to a double fault, control plane convergence plays fundamental role in recovery from the single failure, recomputing ECMP bundles or new LFAs after the failure has happened. Fast-rehash reacts on interface state change, not routing, however, eventually control plane converges, updates RIB and downloads updated routes to FIB (depending on implementation either bottom part of RIB (flattening NHs) or top part of FIB :-) will regroup. Further optimizations are possible, I&#39;m trying to look from a generic prospective (and being a coupe of years away from building routers :))</p>\n",
         "id": "223",
         "name": "Jeff Tantsura",
         "pub": "2020-11-09T23:25:11",
         "type": "comment"
      },
      {
         "date": "10 November 2020 05:40",
         "html": "<p>@Ivan, re the 3rd bullet point that your reader asked:</p>\n\n<p>&quot;L3 LAGs and/or amongst L2 links (e.g. LAG&rsquo;s individual links).&quot;</p>\n\n<p>Cisco had a detailed write-up about the breakdown of the CEF load-balancing structure here:</p>\n\n<p>https://community.cisco.com/t5/service-providers-documents/asr9000-xr-load-balancing-architecture-and-characteristics/ta-p/3124809</p>\n\n<p>So looks like for hierarchical FIB, 3 hardware lookup tables are needed for a non-LAG lookup: a FIB, which points to a RDLI/NRDLI structure, which in turn points to adjaceny table, which in turn points either to physical OIF or yet another indirection table -- the 4th one, the LAG, which finally points to OIF.</p>\n\n<p>That means a max of 4 lookups in hardware for the destination, for a &#39;LAG in ECMP&#39; entry. That&#39;s just destination lookup, not further processing like ACL, QOS... which means no line-rate forwarding, at least not for smaller packets. Clearly they have to trade-off between performance and high-availability here. </p>\n\n<p>I&#39;m not trying to nit-pick on Cisco, I&#39;m pretty sure other vendors can&#39;t do much better (or if they can match Cisco) -- Juniper structure for ex, is just as lengthy. But if vendors start to claim things line-rate performance, that&#39;s when we need to be suspicious.</p>\n",
         "id": "224",
         "name": " Minh Ha",
         "pub": "2020-11-10T05:40:34",
         "type": "comment"
      },
      {
         "date": "10 November 2020 06:25",
         "html": "<p>@Jeff, thx a lot for sharing all this info, esp. the details of your platform!! Wrt Andrea&#39;s comment,  &quot;LFA, as opposed to ECMP, is not resilient to a double fault as if the link and its backup link failed then LFA would actually provide&hellip; fast-discard&quot;, I don&#39;t think he meant double-fault as in the often-used meaning of the term. Here he likely meant when both the protected link and the LFA link went down, then LFA would provide fast discard. </p>\n\n<p>I agree with Andrea&#39;s observation, because by default, both IOSXR and Junos&#39;s LFA tiebreaking algorithms reduce the list of eligible LFAs to one. Junos&#39; process: 1)prefer  ECMP next hops, then prefer backup NH that provides node protection, then prefer NH that provides link protection, followed by NH closest to destination, followed by NH closest to PLR, and finally when all else fails, prefer NH with lowest system ID.</p>\n\n<p>IOS-XR by defaults also prefers direct ECMP next hop, followed by lowest total metric NH, disjoint linecard, and node protection. </p>\n\n<p>The only place that Cisco public doccos document the default selection algorithm is here: </p>\n\n<p>https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst9500/software/release/17-3/command_reference/b_173_9500_cr/ip_routing_commands.html#wp2703610168</p>\n\n<p>This is consistent with Jan&#39;s first comment, his test result, which results in ECMP protection, and when all ECMP fails, LFA kicks in, because by default LFA&#39;s first selection would be ECMP paths. I&#39;m wondering what kind of configuration Andrea had, because in his first comment he said LFA kicks in even after 1 of his ECMP fails? Would be good if we can see the detailed config or output. </p>\n\n<p>From reading Jan&#39;s 2nd comment, I get the impression that he only tested ECMP with xLFA though. It would be good if someone who has access to IOS-XR equipment to run vanilla IGP with ECMP, disable all xLFA, and run show cef internal to look at the FIB structure, including the hashbuckets, then enable LFA, and look at the hash buckets again. That way we can tell exactly what&#39;s going on, and it&#39;s pretty simple setup. </p>\n\n<p>Granted, what we see with cef internal is the software-FIB, but that software FIB is the input to create the more simplified hardware FIB, so what we see is the real thing happening on the hardware level.</p>\n",
         "id": "225",
         "name": " Minh Ha",
         "pub": "2020-11-10T06:25:23",
         "type": "comment"
      },
      {
         "date": "10 November 2020 06:52",
         "html": "<p>Also Jeff, re xLFA being control-plane function and ECMP being data-plane construct -- your 1st comment, I don&#39;t think that&#39;s the case. LFA was meant from the beginning as a data-plane short-term quick relief, a firefighting workaround to ease the forwarding latency pressure, until the IGP can converge and provide the permanent solution again. That&#39;s why Andrea&#39;s 2nd comment re its possible cause of instability condition, its lack of resilience... was very much on point, because xLFA was never meant to be a long-term answer.</p>\n\n<p>Re the control-plane vs data-plane thingy, after all, we can argue, that ALL features within a router HAVE their roots in the control plane. After all, it&#39;s the control plane that sets up everything at the beginning. But in this case, xLFA is considered a data-plane construct because it was pushed into the FIB and activated the instant an error condition was detected, with no involvement from the software aka control plane. I think we&#39;re basically saying the same thing in different ways here btw :)).</p>\n\n<p>Also, while xLFA, esp. rLFA and TI-LFA are indeed more intelligent than stateless ECMP, it&#39;s intelligence that accompanies a great deal of complexity, is painful, and with consequences. X25 was intelligent, ATM was very intelligent, but both were probably too intelligent for their own good.</p>\n\n<p>Wrt to xLFA, when one looks at all the P-node, Q-node, PQ-node, the targeted LDP sessions, the tie-breaking algo, the amount of calculation... then it can quickly overwhelm one&#39;s mind. And all this complexity will scale quickly with the size of the network. With complexity comes fragility, esp. when one has to troubleshoot it or takes over a large network that has it implemented. All of this, for a temporary backup feature. </p>\n\n<p>I think that&#39;s why they want to reduce the complexity and the flow disruption with TI-LFA in segment routing, by having the routers calculate the post-convergence backup LFA route(s), but still, a lot of the complexity is still there. EVPN multi-homing with ESI is complex, seamless MPLS is complex, but xLFA trumps both IMO. I&#39;m not complaining or hating on xLFA; I just mean if one strikes for simplicity, ECMP might be good enough in a lot of cases ;) . </p>\n",
         "id": "226",
         "name": " Minh Ha",
         "pub": "2020-11-10T06:52:29",
         "type": "comment"
      },
      {
         "date": "11 November 2020 11:45",
         "html": "Referring to the Huawei part in the original post:<br />\nHuawei DC switches can be configured to use &quot;consistent hashing&quot; where no re-hashing happens if one of the ECMP links fail. This may lead to less optimal load distribution on the remaining ECMP links.<br />\nHowever re-hashing still occurs if a new link is added to the ECMP or the failed link recovers.<br />\nECMP and FRR can be used at the same time and FRR kicks in only when all of the ECMP links fail.<br />\n\n\n",
         "id": "229",
         "name": "Oezguer",
         "pub": "2020-11-11T11:45:40",
         "type": "comment"
      },
      {
         "date": "13 November 2020 07:17",
         "html": "<p>@JeffT re. comment of 09 November 2020 11:04 <br />\nI agree with you Jeff that TI-LFA and LFA is much more than ECMP as it leverages the sophisticated control plane LFA computation but my point is that in a link-protection only scenario ECMP fast-rehash is way better than LFA or TI-LFA in terms of the degree of flows-spraying.&nbsp;   </p>\n\n<p>@JeffT re. comment of @ 09 November 2020 11:25 <br />\nAs Minh Ha pointed out, I only wanted to highlight that in a link-protection only scenario it is paradoxical that with LFA or TI-LFA if both the protected link and the LFA-backup link went down, then LFA would actually provide &#39;fast discard&#39; while ECMP fast-rehash would correctly still provide fast protection.  </p>\n\n<p>@ Minh HA re. comment @ 10 November 2020 06:25 <br />\nThe link you provided is for IOSXE actually - so am not sure it is relevant for IOSXR but it contains to me a pretty interesting info: It looks like that by default EIGRP prefers using ECMP fast-rehash as opposed to LFA while OSPF doesn&#39;t as it always enforces LFA. There is in fact for EIGRP  only a command to disable the ECMP fast-rehash behaviour in favour of LFA when LFA is configured.  Look at this link   https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst9500/software/release/17-3/command_reference/b_173_9500_cr/ip_routing_commands.html#wp1624013282 <br />\nThis is my understanding of that command at least -  it would help greatly if someone from Cisco could shed some light on this command.  </p>\n\n<p>Regarding the IOSXR config, it&#39;s just as simple as what follows: <br />\nrouter ospf BLA <br />\n fast-reroute per-prefix  </p>\n\n<p>Regarding the IOSXR show command, here&#39;s an output <br />\n<pre>\r\n&lt;BLA&gt;#show cef IP/32\r\nIP/32, version 649067, internal 0x1000001 0x0 (ptr 0x788eb2b0) [3], 0x0 (0x7d02bfd8), 0xa20 (0x797ffc38)\r\n Updated Jul 13 10:29:00.380 \r\n remote adjacency to HundredGigE1\r\n Prefix Len 32, traffic index 0, precedence n/a, priority 3\r\n   via IP#1/32, HundredGigE1, 10 dependencies, weight 0, class 0, protected, backup (Local-LFA) [flags 0x600]\r\n    path-idx 0 bkup-idx 3 NHID 0x0 [0x78662698 0x0], parent-ifh 0x4000140\r\n    next hop IP#1/32\r\n     local label 29621      labels imposed &#123;None&#125;\r\n   via IP#2/32, HundredGigE2, 10 dependencies, weight 0, class 0, protected [flags 0x400]\r\n    path-idx 1 bkup-idx 5 NHID 0x0 [0x786636a0 0x0], parent-ifh 0x40000c0\r\n    next hop IP#2/32\r\n     local label 29621      labels imposed &#123;None&#125;\r\n   via IP#3/32, HundredGigE3 dependencies, weight 0, class 0, protected, backup (Local-LFA) [flags 0x600]\r\n    path-idx 2 bkup-idx 5 NHID 0x0 [0x78663778 0x0], parent-ifh 0x4000440\r\n    next hop IP#3/32\r\n     local label 29621      labels imposed &#123;None&#125;\r\n   via IP#4/32, HundredGigE4, 10 dependencies, weight 0, class 0, protected, backup (Local-LFA) [flags 0x600]\r\n    path-idx 3 bkup-idx 0 NHID 0x0 [0x78660b98 0x0], parent-ifh 0x180000c0\r\n    next hop IP#4/32\r\n     local label 29621      labels imposed &#123;None&#125;\r\n   via IP#5/32, HundredGigE5, 9 dependencies, weight 0, class 0, protected [flags 0x400]\r\n    path-idx 4 bkup-idx 2 NHID 0x0 [0x78663850 0x0], parent-ifh 0x180003c0\r\n    next hop IP#5/32\r\n     local label 29621      labels imposed &#123;None&#125;\r\n   via IP#6/32, HundredGigE6, 9 dependencies, weight 0, class 0, protected, backup (Local-LFA) [flags 0x600]\r\n    path-idx 5 bkup-idx 2 NHID 0x0 [0x78663928 0x0], parent-ifh 0x180002c0\r\n    next hop IP#6/32\r\n   local label 29621      labels imposed &#123;None&#125;\r\n</pre>\n\n<p>Totally with you when you say: &quot;It would be good if someone who has access to IOS-XR equipment to run vanilla IGP with ECMP, disable all xLFA, and run show cef internal to look at the FIB structure, including the hashbuckets, then enable LFA, and look at the hash buckets again. That way we can tell exactly what&#39;s going on, and it&#39;s pretty simple setup.&quot; <br />\nIOSXR behaviour is actually also in line with this IETF draft (https://tools.ietf.org/html/draft-ietf-rtgwg-bgp-pic-12#section-6.1),  written by Cisco too, where it says that if LFA is on and the failure is local (e.g. interface down) then the LFA-protection  for that ECMP link down is triggered as opposed&nbsp; to the fast-rehashing of the ECMP set been triggered. The&nbsp; example in the draft is about PIC-CORE but at the end of the day the BGP-NH underneath is reachable via a set of&nbsp;  ECMP&#39;ed IGP NHs which, to me, provides no loss of generality as such. </p>\n\n<p>Cheers <br />\nAndrea</p>\n",
         "id": "242",
         "name": "Andrea Di Donato",
         "pub": "2020-11-13T19:17:25",
         "type": "comment"
      }
   ],
   "count": 18,
   "type": "post",
   "url": "2020/11/fast-failover-without-lfa-frr.html"
}
