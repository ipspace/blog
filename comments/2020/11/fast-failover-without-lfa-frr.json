{
   "comments": [
      {
         "date": "04 November 2020 09:57",
         "html": "I have done real lab tests with IOS-XR based routers in the following scenario:<br />\n- ECMP links running IS-IS<br />\n- IS-IS + Segment Routing + TI-LFA<br />\n\n<p>Topology: 4 x ECMP link between two routers</p>\n\n<p>This is what happens:\nYou loose link #1: ECMP is still available: Minimal traffic loss (&lt;10ms) as traffic is simply re-distributed onto other ECMP links\nYou loose link #2: Same happens as for link #1\nYou loose link #3: Fast failover to last remaining link. BUT since ECMP is gone, TI-LFA will now calculate a backup path\nYou loose link #4: TI-LFA will provide fast failure, assuming there is another path available.</p>\n\n<p>I <em>think</em> the ECMP behavior is independent of the routing protocol, because it is implemented in CEF / hardware. But have not tested it properly with LDP / BGP etc. myself.</p>\n",
         "id": "201",
         "name": " Jan",
         "pub": "2020-11-04T09:57:33",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:13",
         "html": "<p>@Jan: Thanks for the data! </p>\n\n<p>However, 10 msec loss could still be caused by CPU reprogramming the ECMP buckets. Have you experienced any difference in how long the outage was based on whether you had pure ECMP or LFA on top of it?</p>\n\n<p>Thank you! Ivan</p>\n",
         "id": "202",
         "name": "Ivan Pepelnjak",
         "pub": "2020-11-04T11:13:06",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:38",
         "html": "<p>One thing to note is that in a pure spine-and-leaf network, you can, or perhaps even <em>will</em>, get loops when a link goes down, until the routing protocol has converged.</p>\n\n<p>Consider a network with two spines (S1,S2) and three leafs (L1, L2, L3).  A host connected to L1 send a packet to a host on L2.  L1 decides to send it to spine S1, but unbeknownst to the leafs, the link between S1 and L2 has gone down.  S1 has realized this, and reprogrammed itself, so when it receives the packet that needs to go to L2, it will try to send that to one of the other leafs, L1 or L3, and hope that they will send it to the <em>other</em> spine, S2, since it is the remaining path to L2.</p>\n\n<p>But, since L1 and L3 have not yet realized that S1 no longer has a link to L2, they may decide to send the packet back to S1.  In particular, L1 will almost certainly hash the packet the same way as when it got it the first time, and send it to S1 again.  Which will hash it the same way <em>it</em> did previously, and send it to L1.  Loop.</p>\n\n<p>Until the routing protocol converges a second or two later.</p>\n\n<p>This is inherent in a pure spine-and-leaf network, and the only way to avoid it is to have a less pure network design.  For example:</p>\n\n<ul>\n<li>Have multiple links between each spine and leaf.  If there had been two links between S1 and L2, the problem would not have happened, as S1 would just send the packet over the other direct link to L2.</li>\n<li>Connect the spines in a ring.  S1 would then have a shorter backup path towards L2 available (via S2) than by sending the packet back to some leaf.  (I personally like this design.)</li>\n</ul>\n\n<p>Whether you <em>need</em> that fast rerouting in your network, or if you can wait until the routing protocol converges, is of course a different question.  And likely depends on how <em>often</em> you have link failures. In the datacenter network I&#39;m managing, where links fail almost never, we would be OK with a convergence time of a minute or more; but our OSPF converges in a second or two, so no problem there.  (We still have the spines connected to each other, but for other reasons.)</p>\n",
         "id": "203",
         "name": " Bellman",
         "pub": "2020-11-04T11:38:18",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:44",
         "html": "<p>@Bellman: the behavior you describe depends on whether you designed a <a href=\"https://blog.ipspace.net/2018/09/valley-free-routing-in-data-center.html\">valley-free routing topology</a> (in which case you&#39;ll experience packet drops until the routing protocol does its job, and a <a href=\"https://blog.ipspace.net/2018/09/implications-of-valley-free-routing-in.html\">few other interesting things</a>) or not (in which case you&#39;ll experience path hunting and temporary loops).</p>\n\n<p>Even LFA wouldn&#39;t help in a leaf-and-spine fabric, you&#39;d need remote LFA to get to another spine switch.</p>\n",
         "id": "204",
         "name": "Ivan Pepelnjak",
         "pub": "2020-11-04T11:44:37",
         "type": "comment"
      },
      {
         "date": "04 November 2020 01:17",
         "html": "<p>@Jan: thanks for your contribution. It&#39;s interesting as we&#39;re seeing a different behaviour. In our IOSXR, by default, LFA is calculated for ECMP&#39;ed prefixes and thus it kicks in even if just one link fails (e.g. your link #1). What&#39;s your configuration ? Have you excluded somehow via CLI the LFA computation for ECMP&#39;ed prefixes ? </p>\n",
         "id": "205",
         "name": "Andrea Di Donato",
         "pub": "2020-11-04T13:17:32",
         "type": "comment"
      },
      {
         "date": "04 November 2020 04:07",
         "html": "Ivan:<br />\n\n<p>Yes, you can certainly configure your network so the spines drop packages instead of trying to route around the broken link.</p>\n\n<p>However, will you be <em>happier</em> because the spine is now dropping 100% of the traffic it receives destined for L2, instead of looping (and then dropping) a fraction of that same traffic?  This is in the context of wanting very quick failovers, faster than the routing protocol can react, so presumably you you want as small outages as possible.</p>\n\n<p>(And if you [generic you, not specifically you, Ivan] prefer valley-free routing, remember to consider what happens if the broken link is the one to the leaf where your monitoring and/or management stations are connected.  Suddenly you have no way of managing and monitoring that spine...  Unless those stations are dual-homed to two leaf routers.  A physically separate management network just moves the problem to that network; or are valley-free proponents OK with a valleyed management network?)</p>\n\n<p>&gt; Even LFA wouldn&#39;t help in a leaf-and-spine fabric, you&#39;d need\n&gt; remote LFA to get to another spine switch.</p>\n\n<p>Yes, that was exactly the point I was trying to make!  But I could obviously have been clearer about that.  A pure leaf-and-spine network inherently does not <em>have</em> a Loop-Free Alternative from the spines.  You need to break the topology somehow.  Tunnels to the other spines are one way of doing that, physical links to neighbouring spines another (the one I personally like best), and so on.</p>\n\n<p>(This was all a bit of a side-note to your main post.  I hope I have not derailed the discussion too much.)</p>\n",
         "id": "206",
         "name": "Bellman",
         "pub": "2020-11-04T16:07:01",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:44",
         "html": "<p>I have addressed some of it in one of &ldquo;between 0x2 nerds&rdquo; webinars.\nIn general - you are comparing 2 different techniques - IP FRR vs fast-rehash. IP FRR (xLFA) relies on pre-computed backup next-hop that is loop-free (could be ECMP) and is a control plane function (eventually end-result is downloaded into HW), it could take into consideration some additional data - SRLGs, interface load, etc.\nFast-rehash is a forwarding construct, where the next-hop (could be called differently) is not a single entry but an array of entries (ECMP bundle as downloaded by the control plane).\nIf one of them becomes unavailable (BFD or LoS or interface down events) it is simply removed from the array and the hashing is updated accordingly, since the name.\nUsually - you&rsquo;d see LFA implemented on a high end routers, it is much more intelligent/complex and provides non connected bypass (rLFA/TI-LFA).\nFast-rehash on contrary protects only connected links and doesn&rsquo;t require any additional computation (ECMP alternatives are per definition loop-free). Usually implemented in DC environments. Hope this explains it.\nIP FRR RFCs are produced by IETF RTGWG</p>\n",
         "id": "207",
         "name": " JeffT",
         "pub": "2020-11-04T23:44:14",
         "type": "comment"
      },
      {
         "date": "05 November 2020 12:20",
         "html": "<p>How about Open EIGRP for it&#39;s use of Successor/Feasible successors. LOL</p>\n",
         "id": "208",
         "name": "Jeff Sicuranza",
         "pub": "2020-11-05T00:20:27",
         "type": "comment"
      },
      {
         "date": "05 November 2020 03:42",
         "html": "<p>@Ivan: I double checked my notes and here is what I learned during lab testing, also checking with our engineering colleagues.</p>\n\n<p>Disclaimer:\n- I work for Cisco&#39;s CX organization.\n- Testing was done on Broadcom based XR platforms (NCS 5500). Might actually be different on other XR platforms, since the HW implementation DOES play a role.</p>\n\n<p>Hardware protection / programming for ECMP links is in fact (as written above) only activated when also activating a FRR feature (LFA, rLFA or TI-LFA).\nWithout it, no backup path is programmed in hardware and link down notifies IGP to start (re)convergence. </p>\n\n<p>With FRR enabled, it is pure hardware protection. Official convergence target is &lt;50ms, but might be quicker.\nIn testing, you can also see that not every traffic stream is affected. Most don&#39;t see any traffic loss, but a few packets where in the wrong NPU at the wrong time ;)</p>\n\n<p>@Andrea: I only tested with TI-LFA. I know that the LFA / rLFA implementation is quite different from TI-LFA so it could well be that they behave differently.</p>\n",
         "id": "209",
         "name": " Jan",
         "pub": "2020-11-05T15:42:30",
         "type": "comment"
      },
      {
         "date": "06 November 2020 01:35",
         "html": "<p>@JeffT: Thanks for your contribution &ndash; much appreciated. Will definitely go and watch &ldquo;between 0x2 nerds&rdquo; webinar asap. </p>\n\n<p>My objection is that in a link-protection only scenario the LFA solution is suboptimal if compared to ECMP/fast-rehashing. </p>\n\n<p>I&rsquo;ll give you an example and will provide you with some associated mumbling/maths. Give me a shout if it doesn&#39;t make any sense as am hungry for conjecture&rsquo;s confirmation.</p>\n\n<p>The way we think IOSXR behaves in the below scenario is the following: </p>\n\n<p>=====================================================================================\nSay there are D prefixes X1,...,Xd with 5 ECMP NH (i.e. 1,2,3,4,5) and 15 buckets</p>\n\n<p>At regime we have ECMP and therefore the hash bucket allocation is the following 123451234512345</p>\n\n<p>If link3 fails then the following should happen with IOSXR</p>\n\n<p>for pfx X1, link 3 is protected by link 4 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,4,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,4,4,5,1,2,4,4,5,1,2,4,4,5</p>\n\n<p>for pfx X2, link 3 is protected by link 5 and thus the NH pointer moves from the NH_GROUP [1,2,3,4,5] to NH_GROUP [1,2,5,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,5,4,5,1,2,5,4,5,1,2,5,4,5</p>\n\n<p>for pfx X3, link 3 is protected by link 1 and thus the NH pointer moves from the NH_GROUP [1,2,3,4,5] to NH_GROUP [1,2,1,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,1,4,5,1,2,1,4,5,1,2,1,4,5</p>\n\n<p>for pfx X4, link 3 is protected by link 2 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,2,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,2,4,5,1,2,2,4,5,1,2,2,4,5</p>\n\n<p>for pfx X5, link 3 is protected by link 4 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,4,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,4,4,5,1,2,4,4,5,1,2,4,4,5</p>\n\n<p>...</p>\n\n<p>for pfx Xd, link 3 is protected by link 2 and thus the NH pointer moves from the NH GROUP [1,2,3,4,5] to NH_GROUP [1,2,2,4,5]  and the pre-programmed hash bucket re-allocation must therefore provide the following 1,2,2,4,5,1,2,2,4,5,1,2,2,4,5</p>\n\n<p>================================================================================\nNow, if am not mistaken, calling H the number of the ECMP NHs and thus generalising what just shown but without delving into the maths (I can but I&rsquo;d avoid this for now), we can observe that in the LFA scenario a link, say link 4, during link 3 protection has a load in terms of number of flows equivalent to the flows of one particular prefix group (out of the H-1 prefix groups)  with weight 2/H plus the load of all of the other H-2 prefix groups&rsquo; flows with weight 1/H. In an ECMP scenario instead that very same link 4  during protection has a load in terms of number of flows equivalent to the total number of flows across all of the prefix groups and with weight 1/(H-1). \nComparing the two values you can tell that the LFA protection is not robust (risking link saturation and/or qos thresholds crossing) against a disproportion in terms of number of flows per destination/prefix amongst the (H-1) groups of ECMP&rsquo;ed prefixes (e.g.. in an IP tunneling environments where all of the VxLAN, GTP, GRE destinations happen to be in the very same prefix group out of the H-1 groups of prefixes that has a weight of 2/H over link 4). \nHaving said that, one argument that I heard around is that this could be mitigated/counter-measured if these additional number of flows had a relatively low per-flow bw if compared with non-tunneled destinations&rsquo; flows. Should this (at all) happen though, it would mean that the (ECMP) per-flow load-balancing @ regime (with no fault) would be already crap as some flows (those not tunneled) have a much higher bw than the tunneled ones which is a contradiction in terms to me. We&rsquo;re here assuming by the way that flow-based load balancing within these tunnels  (e.g. GTP, GRE,VxLAN,.. ) is supported by the chipsets. <br />\nLast but not least, we should mention that LFA, as opposed to ECMP, is not resilient to a double fault as if the link and its backup link failed then LFA would actually provide&hellip; fast-discard ??</p>\n\n<p>Cheers</p>\n\n<p>Andrea</p>\n",
         "id": "218",
         "name": "Andrea Di Donato",
         "pub": "2020-11-06T13:35:26",
         "type": "comment"
      },
      {
         "date": "09 November 2020 11:04",
         "html": "Let me try to address in order:<br />\n@Ivan - 10ms includes failure detection, not just reprogramming HW, so it depends on how fast the failure is detected/propagated to HW.<br />\nIn previous life, implementing IPFRR on E/// SSR (EZ), we got HW performance tuned to 2K NH changes per 1ms. Jericho (to my memory) would be using SuperFEC structure and should be much faster.<br />\n@Andrea - I don&#39;t work for Cisco and can&#39;t comment on internals of their hashing. It would also be a reasonable assumption that most chipsets (COTS) do 5 tuple hashing (not looking into GTP TEID or VXLAN VNI).<br />\nIn most implementations, there&#39;s no load-based rebalancing within ECMP group (sticky), and ECMP is the only key to build the group (really basic grouping). LFA on contrary is computed by the control plane and can take a number of additional points into computation, SRLGs, common line cards, load on the link, etc.<br />\nThe intent is often - give me a node protecting LFA and if there isn&#39;t, link protecting would do. Also note that implementation of any none directly connected protection schemes (TI-LFA) is an order of magnitude more complex than basic LFA.<br />\nMy point is - this is not an apple to apple comparison, there&#39;s more to it.<br />\n\n\n",
         "id": "221",
         "name": "Jeff Tantsura",
         "pub": "2020-11-09T23:04:49",
         "type": "comment"
      },
      {
         "date": "09 November 2020 11:12",
         "html": "<p>@Jan\nLFA is indeed very different than xLFA.\nLFA - protect interface A(prefix optionally) by interface B (given it is loop-free within the constrains), when A goes down flip a pointer.\nTI-LFA - if not local protection available (e.g LFA), compute a tunnel that terminates at the PQ node, protect interface A by sending traffic into the tunnel that is preinstantiated in HW</p>\n",
         "id": "222",
         "name": "Jeff Tantsura",
         "pub": "2020-11-09T23:12:45",
         "type": "comment"
      },
      {
         "date": "09 November 2020 11:25",
         "html": "<p>@Andrea - &quot;Last but not least, we should mention that LFA, as opposed to ECMP, is not resilient to a double fault as if the link and its backup link failed then LFA would actually provide&hellip; fast-discard ??&quot;</p>\n\n<p>Neither technology is resilient to a double fault, control plane convergence plays fundamental role in recovery from the single failure, recomputing ECMP bundles or new LFAs after the failure has happened. Fast-rehash reacts on interface state change, not routing, however, eventually control plane converges, updates RIB and downloads updated routes to FIB (depending on implementation either bottom part of RIB (flattening NHs) or top part of FIB :-) will regroup. Further optimizations are possible, I&#39;m trying to look from a generic prospective (and being a coupe of years away from building routers :))</p>\n",
         "id": "223",
         "name": "Jeff Tantsura",
         "pub": "2020-11-09T23:25:11",
         "type": "comment"
      }
   ],
   "count": 13,
   "type": "post",
   "url": "2020/11/fast-failover-without-lfa-frr.html"
}
