{
   "comments": [
      {
         "date": "04 November 2020 09:57",
         "html": "I have done real lab tests with IOS-XR based routers in the following scenario:<br />\n- ECMP links running IS-IS<br />\n- IS-IS + Segment Routing + TI-LFA<br />\n\n<p>Topology: 4 x ECMP link between two routers</p>\n\n<p>This is what happens:\nYou loose link #1: ECMP is still available: Minimal traffic loss (&lt;10ms) as traffic is simply re-distributed onto other ECMP links\nYou loose link #2: Same happens as for link #1\nYou loose link #3: Fast failover to last remaining link. BUT since ECMP is gone, TI-LFA will now calculate a backup path\nYou loose link #4: TI-LFA will provide fast failure, assuming there is another path available.</p>\n\n<p>I <em>think</em> the ECMP behavior is independent of the routing protocol, because it is implemented in CEF / hardware. But have not tested it properly with LDP / BGP etc. myself.</p>\n",
         "id": "201",
         "name": " Jan",
         "pub": "2020-11-04T09:57:33",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:13",
         "html": "<p>@Jan: Thanks for the data! </p>\n\n<p>However, 10 msec loss could still be caused by CPU reprogramming the ECMP buckets. Have you experienced any difference in how long the outage was based on whether you had pure ECMP or LFA on top of it?</p>\n\n<p>Thank you! Ivan</p>\n",
         "id": "202",
         "name": "Ivan Pepelnjak",
         "pub": "2020-11-04T11:13:06",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:38",
         "html": "<p>One thing to note is that in a pure spine-and-leaf network, you can, or perhaps even <em>will</em>, get loops when a link goes down, until the routing protocol has converged.</p>\n\n<p>Consider a network with two spines (S1,S2) and three leafs (L1, L2, L3).  A host connected to L1 send a packet to a host on L2.  L1 decides to send it to spine S1, but unbeknownst to the leafs, the link between S1 and L2 has gone down.  S1 has realized this, and reprogrammed itself, so when it receives the packet that needs to go to L2, it will try to send that to one of the other leafs, L1 or L3, and hope that they will send it to the <em>other</em> spine, S2, since it is the remaining path to L2.</p>\n\n<p>But, since L1 and L3 have not yet realized that S1 no longer has a link to L2, they may decide to send the packet back to S1.  In particular, L1 will almost certainly hash the packet the same way as when it got it the first time, and send it to S1 again.  Which will hash it the same way <em>it</em> did previously, and send it to L1.  Loop.</p>\n\n<p>Until the routing protocol converges a second or two later.</p>\n\n<p>This is inherent in a pure spine-and-leaf network, and the only way to avoid it is to have a less pure network design.  For example:</p>\n\n<ul>\n<li>Have multiple links between each spine and leaf.  If there had been two links between S1 and L2, the problem would not have happened, as S1 would just send the packet over the other direct link to L2.</li>\n<li>Connect the spines in a ring.  S1 would then have a shorter backup path towards L2 available (via S2) than by sending the packet back to some leaf.  (I personally like this design.)</li>\n</ul>\n\n<p>Whether you <em>need</em> that fast rerouting in your network, or if you can wait until the routing protocol converges, is of course a different question.  And likely depends on how <em>often</em> you have link failures. In the datacenter network I&#39;m managing, where links fail almost never, we would be OK with a convergence time of a minute or more; but our OSPF converges in a second or two, so no problem there.  (We still have the spines connected to each other, but for other reasons.)</p>\n",
         "id": "203",
         "name": " Bellman",
         "pub": "2020-11-04T11:38:18",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:44",
         "html": "<p>@Bellman: the behavior you describe depends on whether you designed a <a href=\"https://blog.ipspace.net/2018/09/valley-free-routing-in-data-center.html\">valley-free routing topology</a> (in which case you&#39;ll experience packet drops until the routing protocol does its job, and a <a href=\"https://blog.ipspace.net/2018/09/implications-of-valley-free-routing-in.html\">few other interesting things</a>) or not (in which case you&#39;ll experience path hunting and temporary loops).</p>\n\n<p>Even LFA wouldn&#39;t help in a leaf-and-spine fabric, you&#39;d need remote LFA to get to another spine switch.</p>\n",
         "id": "204",
         "name": "Ivan Pepelnjak",
         "pub": "2020-11-04T11:44:37",
         "type": "comment"
      },
      {
         "date": "04 November 2020 01:17",
         "html": "<p>@Jan: thanks for your contribution. It&#39;s interesting as we&#39;re seeing a different behaviour. In our IOSXR, by default, LFA is calculated for ECMP&#39;ed prefixes and thus it kicks in even if just one link fails (e.g. your link #1). What&#39;s your configuration ? Have you excluded somehow via CLI the LFA computation for ECMP&#39;ed prefixes ? </p>\n",
         "id": "205",
         "name": "Andrea Di Donato",
         "pub": "2020-11-04T13:17:32",
         "type": "comment"
      },
      {
         "date": "04 November 2020 04:07",
         "html": "Ivan:<br />\n\n<p>Yes, you can certainly configure your network so the spines drop packages instead of trying to route around the broken link.</p>\n\n<p>However, will you be <em>happier</em> because the spine is now dropping 100% of the traffic it receives destined for L2, instead of looping (and then dropping) a fraction of that same traffic?  This is in the context of wanting very quick failovers, faster than the routing protocol can react, so presumably you you want as small outages as possible.</p>\n\n<p>(And if you [generic you, not specifically you, Ivan] prefer valley-free routing, remember to consider what happens if the broken link is the one to the leaf where your monitoring and/or management stations are connected.  Suddenly you have no way of managing and monitoring that spine...  Unless those stations are dual-homed to two leaf routers.  A physically separate management network just moves the problem to that network; or are valley-free proponents OK with a valleyed management network?)</p>\n\n<p>&gt; Even LFA wouldn&#39;t help in a leaf-and-spine fabric, you&#39;d need\n&gt; remote LFA to get to another spine switch.</p>\n\n<p>Yes, that was exactly the point I was trying to make!  But I could obviously have been clearer about that.  A pure leaf-and-spine network inherently does not <em>have</em> a Loop-Free Alternative from the spines.  You need to break the topology somehow.  Tunnels to the other spines are one way of doing that, physical links to neighbouring spines another (the one I personally like best), and so on.</p>\n\n<p>(This was all a bit of a side-note to your main post.  I hope I have not derailed the discussion too much.)</p>\n",
         "id": "206",
         "name": "Bellman",
         "pub": "2020-11-04T16:07:01",
         "type": "comment"
      },
      {
         "date": "04 November 2020 11:44",
         "html": "<p>I have addressed some of it in one of &ldquo;between 0x2 nerds&rdquo; webinars.\nIn general - you are comparing 2 different techniques - IP FRR vs fast-rehash. IP FRR (xLFA) relies on pre-computed backup next-hop that is loop-free (could be ECMP) and is a control plane function (eventually end-result is downloaded into HW), it could take into consideration some additional data - SRLGs, interface load, etc.\nFast-rehash is a forwarding construct, where the next-hop (could be called differently) is not a single entry but an array of entries (ECMP bundle as downloaded by the control plane).\nIf one of them becomes unavailable (BFD or LoS or interface down events) it is simply removed from the array and the hashing is updated accordingly, since the name.\nUsually - you&rsquo;d see LFA implemented on a high end routers, it is much more intelligent/complex and provides non connected bypass (rLFA/TI-LFA).\nFast-rehash on contrary protects only connected links and doesn&rsquo;t require any additional computation (ECMP alternatives are per definition loop-free). Usually implemented in DC environments. Hope this explains it.\nIP FRR RFCs are produced by IETF RTGWG</p>\n",
         "id": "207",
         "name": " JeffT",
         "pub": "2020-11-04T23:44:14",
         "type": "comment"
      },
      {
         "date": "05 November 2020 12:20",
         "html": "<p>How about Open EIGRP for it&#39;s use of Successor/Feasible successors. LOL</p>\n",
         "id": "208",
         "name": "Jeff Sicuranza",
         "pub": "2020-11-05T00:20:27",
         "type": "comment"
      }
   ],
   "count": 8,
   "type": "post",
   "url": "2020/11/fast-failover-without-lfa-frr.html"
}
