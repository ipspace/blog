<div class="comments post" id="comments">
  <h4>5 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="281">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> andrea di donato</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c281" href="#281">04 December 2020 10:16</a>
              </span>
            </div>
            <div class="comment-content"><p>Fantastic wrap-up Ivan/Jeff!</p>

<p>I just wanted to add what we already discussed on this other previous blog @ https://blog.ipspace.net/2020/11/fast-failover-without-lfa-frr.html regarding the comparison between LFA/TI-LFA and Fast-rehash of ECMP-paths:</p>

<p>TI-LFA and LFA are much more than Fast-rehash of ECMP-paths as LFA techniques leverage a more sophisticated control-plane computation but in a link-protection only scenario ECMP fast-rehash is way better than LFA or TI-LFA in terms of the degree of flows-spraying especially if the number of flows per destination varies a lot (e.g. either systemic and/or in an IP tunnelling environment such as GTP, VxLAN and so fort). In this kind of environment in fact the high-end router could carry on spraying the tunnels&rsquo; destinations flows over the N-1 links left (I am assuming an high-end router chipset is capable of performing per-flow loadbalancing of mpls-encapsulated GTP or VxLAN traffic otherwise it is not an high-end router to me ) with fast-rehash rather than sending all of the flows of a tunnel destination over the single LFA/TI-LFA backup NH in order to provide fast link-protection.</p>

<p>Moreover, if both the protected link and the LFA-backup link went down, then LFA would actually and paradoxically provide &#39;fast discard&#39; while ECMP fast-rehash would correctly still provide fast link-protection over the N-2 ECMP links left. </p>

<p>I&#39;d be important if on high-end routers you could disable (with a knob rather than with complex LFA policies) LFA/TI-LFA in favour of ECMP fast-rehash when you require just fast link-protection in an ECMP scenario. </p>

<p>Andrea</p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="283">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Prabhu raj</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c283" href="#283">04 December 2020 09:44</a>
              </span>
            </div>
            <div class="comment-content"><p>Some questions</p>

<p>*Does IP FRR work across inter ospf domains where the source is located in OSPF area X and destination is located in OSPF area Y.</p>

<p>*Does IP FRR work across multiple IGP protocols where the source is located in a OSPF region and the destination is located inside a IS-IS region.</p>

<p>*How BFD can utilitized by existing control plane protocols to detect link and node failures and recalculate. </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="284">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c284" href="#284">05 December 2020 03:42</a>
              </span>
            </div>
            <div class="comment-content"><p>@Andrea,</p>

<p>Re your last paragraph, ECMP is independent of xLFA and just works if you don&#39;t enable any of the xLFA features. ECMP was in existence way before LFA was even conceived. So let&#39;s say you just unbox a bunch of routers with zero config, then build your physical topology in an ECMP manner, you can easily achieve (U|E)CMP with a maximum-paths command. Seriously I don&#39;t understand why that reader in Ivan&#39;s original post asked that question. I strongly believe he misunderstood what his vendor rep told him, that or the vendor rep himself was misguided. </p>

<p>This is the config guide for configuring UCMP for NCS 5500 running IOS-XR, no mention of xLFA whatsoever:</p>

<p>https://www.cisco.com/c/en/us/td/docs/iosxr/ncs5500/routing/63x/b-routing-cg-ncs5500-63x/b-routing-cg-ncs5500-63x_chapter_01001.html</p>

<p>And this is how ECMP works on the data-plane level for ASR9k, again with no bullshit xLFAs noise:</p>

<p>https://community.cisco.com/t5/service-providers-documents/asr9000-xr-load-balancing-architecture-and-characteristics/ta-p/3124809/page/9 </p>

<p>Obviously, since Broadcom high-end chipsets, some of which are used by ASR9k and NCS5500, support ECMP, ECMP failover is done on the hardware/data-plane level. Tomahawk 3&#39;s ECMP&#39;s max no of groups + hardware table size can be viewed here:</p>

<p>https://docs.broadcom.com/doc/56980-DS</p>

<p>And yes, you&#39;re absolutely right about ECMP being superior to xLFA in terms of flow spraying, because that&#39;s what ECMP was built to do. xLFAs, with all of their hideous complexities -- if they&#39;re already so complex at this high-level view, imagine how much more complex and buggy the code would be -- are never meant to be anything but a quick relief while global repair aka control-plane IGP convergence and FIB update, takes place. They&#39;re never intended to be a delicate balancing act.</p>

<p>Also, I just saw your last comment on the original post. Yes, I&#39;m aware that I posted LFA&#39;s selection criteria for IOS-XE, as it&#39;s hard to locate one for IOS-XR at the time, but it was close. I managed to locate one for IOS-XR in the meantime, here:</p>

<p>https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2016/pdf/BRKRST-3020.pdf</p>

<p>For IOS-XR IS-IS, the default tie-breakers are indeed in the order that I commented in the original post. The Juniper order I got were documented by their own engineers, so it was correct too. It&#39;s weird Cisco didn&#39;t mention default tie-breakers for OSPF, so I assume it&#39;s like IS-IS as they&#39;re both LS IGP.  In any case, it doesn&#39;t matter as this is configurable :) .</p>

<p>&quot;by default EIGRP prefers using ECMP fast-rehash as opposed to LFA while OSPF doesn&#39;t as it always enforces LFA.&quot;</p>

<p>As I mentioned above, LFA only gets enforced when it&#39;s enabled/configured. Without LFA enabled, U|ECMP works just fine. In your config, you enabled LFA, so LFA took over and its first choice of backup path would be the ECMP paths if it used the default tie-breakers.</p>

<p>Your command output is very interesting. So among 6 paths (equal cost I believe), there are 2 groups of 3, within each there&#39;s one protected and 2 back-up. WHat I notice is the first 3 interfaces have similar &quot;parent-ifh&quot; index, all starting with 0x4000, while the rest 3 are 0x18000. What&#39;s their physical or logical relation? That might shed a light on their LFA relationship. </p>

<p>As for PIC and LFA, they&#39;re not related. PIC is only relevant for BGP and is intended to speed up BGP data-plane convergence after IGP and BGP re-convergence, while LFA was used as a temporary relief while waiting for IGP re-convergence. I read that PIC link you provided, but it&#39;s only applicable to PIC, PIC-CORE to be exact. </p>

<p>Cisco has some restrictions between PIC and LFA. If you enable PIC and LFA, you cannot use them to protect the same prefix:</p>

<p>https://www.cisco.com/c/en/us/td/docs/routers/asr9000/software/asr9k-r6-5/routing/configuration/guide/b-routing-cg-asr9000-65x/b-routing-cg-asr9000-65x_chapter_01000.html</p>

<p>In the end, one just has to ask this question: if one cares so much about high availability, why not just increase the degree of ECMP? And to make it stronger, make each ECMP path a port channel. Granted, you trade-off performance for HA that way, but if HA is your goal, it might be worthwhile. And I&#39;m glad I&#39;m not the only one to think that, as Lukas Krattiger has similar view, that &quot;Most scalable approach is still classic ECMP with PIC Core.&quot;</p>

<p>ECMP was originally used for load balancing, but thanks to the redundancy provided, it can provide protection as well. LFA is purely for protection, and temporary protection at that, while ECMP is permanent from the start. So why not kill 2 birds with one stone, actually 3, because with ECMP, you gain simplicity (and sanity) as well. TI-LFA in particular, looks to me like just an attempt to boost the image of Segment Routing, which itself seems like a resurrection of ATM LANE. Centralized-controller paradigm works well at small scale, and sucks big time at large scale, just like shared-memory vs crossbar fabrics. </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="289">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Andrea Di Donato</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c289" href="#289">05 December 2020 10:44</a>
              </span>
            </div>
            <div class="comment-content"><p>@ Minha:    </p>

<p>First of all thanks for your always priceless contributions!      </p>

<p>Regarding your statement: <br />
&ldquo;As I mentioned above, LFA only gets enforced when it&#39;s enabled/configured. Without LFA enabled, U|ECMP works just fine. In your config, you enabled LFA, so LFA took over and its first choice of backup path would be the ECMP paths if it used the default tie-breakers. <br />
&ldquo; <br />
The issue I have Minha is that on the very same router I have some prefixes that are ECMP&rsquo;ed while others aren&rsquo;t and I therefore need to have LFA for the non ECMP&rsquo;ed prefixes and at the very same time ECMP fast-rehash for the ECMP&rsquo;ed prefixes. <br />
Regarding the tie-breakers , what I am seeing on IOSXR at least is that the first choice being an ECMP path means that by default one out of the N x ECNP-NHs is chosen as the LFA-backup-NH which is different from  spraying the flows over the N-1 ECMP NHs left which is what fast-rehash does.  </p>

<p>=======================================  </p>

<p>Regarding your statement: <br />
&ldquo; <br />
What&#39;s their physical or logical relation? That might shed a light on their LFA relationship. <br />
&ldquo; <br />
Well spotted Minha !! What I am seeing in production is that IOSXR provides node-protection by default since on a per-prefix basis, every NH on box A is protected with a NH on Box B. The scenario is that of prefixes having N x ECMP-NHs, N/2 on box A and N/2 on box B. LFA is just powered on with no specific config.  </p>

<p>=======================================   </p>

<p>Regarding your statement: <br />
&ldquo; <br />
As for PIC and LFA, they&#39;re not related. PIC is only relevant for BGP <br />
&ldquo; <br />
I just wanted to add here that BGP PIC-CORE implementation is, to me, very much fast-rehashing but at a higher indirection level of the hierarchical FIB as it applies to service/BGP prefixes in a BGP multipath scenario. What I found here is that in some implementations the trigger for the fast-rehashing at that higher level can be both an interface-down or a BFD session-down event but in other implementations it&rsquo;s the BFD session-down event only that triggers the fast-rehashing. This is important to know if you need to design a high-performance routing/network solution as in that latter case you might not want to enable BGP multipath for traffic requiring fast-protection via fast-rehashing. Having said that, to make things even more complex, I also found that some implementations make use of the central CPU to scale BFD up while others use the CPU on the line card. At the end of the day you do need to lab these solutions with the actual production HW and the actual production routing scenario and measure the convergence time which is not always easy when you have Terabytes of complex traffic flowing through your box&hellip;. &#x2639;  </p>

<p>ciao <br />
Andrea  </p>
</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="294">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Minh Ha</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c294" href="#294">07 December 2020 05:31</a>
              </span>
            </div>
            <div class="comment-content"><p>@Andrea,</p>

<p>Thanks for your valuable input! It&#39;s always great to have more insight from ground zero :) . And we owe all these great discussions to Ivan who have created and coordinated this ipspace platform as a medium of knowledge exchange. Ivan&#39;s contribution to the community over the years, is just immense and goes way beyond simple education. He should receive some kind of award for all this tireless work and inspiration. Thx a million Ivan and pls keep up the great work!!</p>

<p>Yes, unfortunately if you have LFA configured on ECMP interface(s) then LFA logic will take over. This is exactly why I hate complexity. As vendors keep piling features on top of features, suddenly there&#39;s unexpected interaction or corner case not accounted for, and even vendor documentation cannot cover all possible scenarios anymore, and we have to rely on field experience to identify possible issues, like these :( . That&#39;s why nothing beats operational experience &amp; maturity. </p>

<p>Cisco IOS-XR indeed prefers node-protection and tries to provide it whenever possible. In Junos configuration is more explicit and you can choose to enable node-link-protection or just link protection. </p>

<p>So from your output and your explanation of it, I can see that IOS-XR can provide more than 1 ECMP backup NH for LFA, as among the 6 paths, there&#39;re 2 backups for each primary.  </p>

<p>&quot;The scenario is that of prefixes having N x ECMP-NHs, N/2 on box A and N/2 on box B. &quot;</p>

<p>I don&#39;t get this part. Can you elaborate on the setup? I thought all ECMP interfaces would have to be on the same router/box, for it to be ECMP :) .  How&#39;s that possible?</p>

<p>In Junos, and only with TI-LFA for Segment Routing, there&#39;s a cmd to enable more than 1 ECMP backup path, this one here:</p>

<p>https://www.juniper.net/documentation/en_US/junos/topics/reference/configuration-statement/protocols-isis-backup-spf-options-use-post-convergence-lfa.html</p>

<p>So this cmd helps restore load balancing for ECMP paths if you have to enable TI-LFA. Too bad it&#39;s not applicable to LFA and RLFA. I wonder if Cisco can be requested to provide something similar?</p>

<p>In the end, I still fail to understand how even RLFA and TI-LFA are superior to ECMP. In case of double-link failure for link-only protection setup, both RLFA anf TI-LFA risk permanent loop until global repair finishes its reconvergence, so they would be useless, while ECMP can still provide some hit if it has more than 2 paths. That&#39;s why IOS and Junos prefer node protection.</p>

<p>And both RLFA &amp; TI LFA fail to provide 100% coverage, the latter despite its name. For RLFA, it&#39;s obvious when there&#39;s no PQ node, you&#39;re screwed. For TI-LFA, when some leaf networks can only be accessed off the primary/protected node, and that node goes down, bad luck. You just can&#39;t find any other backup path in the network because the only path to those leaves is via the node that&#39;s RIP. So there&#39;s no such thing as TI.  </p>

<p>Also, for TI-LFA, there&#39;s something murky re the amount of calculation required. Say the local repair needs to remove the primary/protected host in order to calculate the post convergence topology in case that host goes down -- node protection scenario. By default for RLFA/TI-LFA, in order to remove non-scalable massive overhead, people try to take shortcut by calculating a reverse SPF from the primary/protected node&#39;s perspective. That cuts down the number of SPF calculation. But by removing that node, where would be your new anchor point to do reverse SPF? This doesn&#39;t seem to be documented anywhere, but this is crucial for LFA viability. Trying to perform rSPF on every node in the routing area, would be suicidal for very large-scale IGP deployments having say 1k routers or more. For vanilla LFA, the number of SPF to be done is proportional to the node degree, so it&#39;s definitely non-trivial in very large networks as well, though to a way less extent and still viable. Also, vanilla LFA performs weakly (low protection coverage) in large networks with low node degree.</p>

<p>So all in all, despite their sophistication, I still don&#39;t see how they&#39;re better than ECMP. So yes, if one needs LFA because there&#39;s no choice, then surely have it on, but otherwise I&#39;d say ECMP works well for most cases. For LFA, if you tweak timer to be too low, you run the risk of false positives as well, triggering the backup path while the primary path is still OK. And don&#39;t forget, after every global convergence, LFA algorithm is run again, so it&#39;s pretty resource-intensive if your network is both large and quick-changing or experiencing instability. </p>

<p>Re your point about PIC, yes, when PIC is combined with BGP multipaths, it&#39;s essentially ECMP from the data-plane&#39;s perspective. </p>

<p>As for BFD, you said &quot;I also found that some implementations make use of the central CPU to scale BFD up while others use the CPU on the line card&quot;. I don&#39;t understand how centralizing BFD provides any advantage, what&#39;s the scale-up for? BFD&#39;s purpose is to quickly inform of failures and trigger reconvergence and LFA or ECMP rehash. So isn&#39;t it easier to have the LC CPU handle this and then notify the control plane to do reconvergence via IPC channels? Sending it to the central CPU and bypassing the LC CPU means the data plane cannot react quickly to failure, seem to defeat the purpose of doing LFA and ECMP in hardware?</p>
</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
