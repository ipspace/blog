{
   "comments": [
      {
         "date": "04 December 2020 10:16",
         "html": "<p>Fantastic wrap-up Ivan/Jeff!</p>\n\n<p>I just wanted to add what we already discussed on this other previous blog @ https://blog.ipspace.net/2020/11/fast-failover-without-lfa-frr.html regarding the comparison between LFA/TI-LFA and Fast-rehash of ECMP-paths:</p>\n\n<p>TI-LFA and LFA are much more than Fast-rehash of ECMP-paths as LFA techniques leverage a more sophisticated control-plane computation but in a link-protection only scenario ECMP fast-rehash is way better than LFA or TI-LFA in terms of the degree of flows-spraying especially if the number of flows per destination varies a lot (e.g. either systemic and/or in an IP tunnelling environment such as GTP, VxLAN and so fort). In this kind of environment in fact the high-end router could carry on spraying the tunnels&rsquo; destinations flows over the N-1 links left (I am assuming an high-end router chipset is capable of performing per-flow loadbalancing of mpls-encapsulated GTP or VxLAN traffic otherwise it is not an high-end router to me ) with fast-rehash rather than sending all of the flows of a tunnel destination over the single LFA/TI-LFA backup NH in order to provide fast link-protection.</p>\n\n<p>Moreover, if both the protected link and the LFA-backup link went down, then LFA would actually and paradoxically provide &#39;fast discard&#39; while ECMP fast-rehash would correctly still provide fast link-protection over the N-2 ECMP links left. </p>\n\n<p>I&#39;d be important if on high-end routers you could disable (with a knob rather than with complex LFA policies) LFA/TI-LFA in favour of ECMP fast-rehash when you require just fast link-protection in an ECMP scenario. </p>\n\n<p>Andrea</p>\n",
         "id": "281",
         "name": " andrea di donato",
         "pub": "2020-12-04T10:16:12",
         "type": "comment"
      },
      {
         "date": "04 December 2020 09:44",
         "html": "<p>Some questions</p>\n\n<p>*Does IP FRR work across inter ospf domains where the source is located in OSPF area X and destination is located in OSPF area Y.</p>\n\n<p>*Does IP FRR work across multiple IGP protocols where the source is located in a OSPF region and the destination is located inside a IS-IS region.</p>\n\n<p>*How BFD can utilitized by existing control plane protocols to detect link and node failures and recalculate. </p>\n",
         "id": "283",
         "name": " Prabhu raj",
         "pub": "2020-12-04T21:44:52",
         "type": "comment"
      },
      {
         "date": "05 December 2020 03:42",
         "html": "<p>@Andrea,</p>\n\n<p>Re your last paragraph, ECMP is independent of xLFA and just works if you don&#39;t enable any of the xLFA features. ECMP was in existence way before LFA was even conceived. So let&#39;s say you just unbox a bunch of routers with zero config, then build your physical topology in an ECMP manner, you can easily achieve (U|E)CMP with a maximum-paths command. Seriously I don&#39;t understand why that reader in Ivan&#39;s original post asked that question. I strongly believe he misunderstood what his vendor rep told him, that or the vendor rep himself was misguided. </p>\n\n<p>This is the config guide for configuring UCMP for NCS 5500 running IOS-XR, no mention of xLFA whatsoever:</p>\n\n<p>https://www.cisco.com/c/en/us/td/docs/iosxr/ncs5500/routing/63x/b-routing-cg-ncs5500-63x/b-routing-cg-ncs5500-63x_chapter_01001.html</p>\n\n<p>And this is how ECMP works on the data-plane level for ASR9k, again with no bullshit xLFAs noise:</p>\n\n<p>https://community.cisco.com/t5/service-providers-documents/asr9000-xr-load-balancing-architecture-and-characteristics/ta-p/3124809/page/9 </p>\n\n<p>Obviously, since Broadcom high-end chipsets, some of which are used by ASR9k and NCS5500, support ECMP, ECMP failover is done on the hardware/data-plane level. Tomahawk 3&#39;s ECMP&#39;s max no of groups + hardware table size can be viewed here:</p>\n\n<p>https://docs.broadcom.com/doc/56980-DS</p>\n\n<p>And yes, you&#39;re absolutely right about ECMP being superior to xLFA in terms of flow spraying, because that&#39;s what ECMP was built to do. xLFAs, with all of their hideous complexities -- if they&#39;re already so complex at this high-level view, imagine how much more complex and buggy the code would be -- are never meant to be anything but a quick relief while global repair aka control-plane IGP convergence and FIB update, takes place. They&#39;re never intended to be a delicate balancing act.</p>\n\n<p>Also, I just saw your last comment on the original post. Yes, I&#39;m aware that I posted LFA&#39;s selection criteria for IOS-XE, as it&#39;s hard to locate one for IOS-XR at the time, but it was close. I managed to locate one for IOS-XR in the meantime, here:</p>\n\n<p>https://www.ciscolive.com/c/dam/r/ciscolive/us/docs/2016/pdf/BRKRST-3020.pdf</p>\n\n<p>For IOS-XR IS-IS, the default tie-breakers are indeed in the order that I commented in the original post. The Juniper order I got were documented by their own engineers, so it was correct too. It&#39;s weird Cisco didn&#39;t mention default tie-breakers for OSPF, so I assume it&#39;s like IS-IS as they&#39;re both LS IGP.  In any case, it doesn&#39;t matter as this is configurable :) .</p>\n\n<p>&quot;by default EIGRP prefers using ECMP fast-rehash as opposed to LFA while OSPF doesn&#39;t as it always enforces LFA.&quot;</p>\n\n<p>As I mentioned above, LFA only gets enforced when it&#39;s enabled/configured. Without LFA enabled, U|ECMP works just fine. In your config, you enabled LFA, so LFA took over and its first choice of backup path would be the ECMP paths if it used the default tie-breakers.</p>\n\n<p>Your command output is very interesting. So among 6 paths (equal cost I believe), there are 2 groups of 3, within each there&#39;s one protected and 2 back-up. WHat I notice is the first 3 interfaces have similar &quot;parent-ifh&quot; index, all starting with 0x4000, while the rest 3 are 0x18000. What&#39;s their physical or logical relation? That might shed a light on their LFA relationship. </p>\n\n<p>As for PIC and LFA, they&#39;re not related. PIC is only relevant for BGP and is intended to speed up BGP data-plane convergence after IGP and BGP re-convergence, while LFA was used as a temporary relief while waiting for IGP re-convergence. I read that PIC link you provided, but it&#39;s only applicable to PIC, PIC-CORE to be exact. </p>\n\n<p>Cisco has some restrictions between PIC and LFA. If you enable PIC and LFA, you cannot use them to protect the same prefix:</p>\n\n<p>https://www.cisco.com/c/en/us/td/docs/routers/asr9000/software/asr9k-r6-5/routing/configuration/guide/b-routing-cg-asr9000-65x/b-routing-cg-asr9000-65x_chapter_01000.html</p>\n\n<p>In the end, one just has to ask this question: if one cares so much about high availability, why not just increase the degree of ECMP? And to make it stronger, make each ECMP path a port channel. Granted, you trade-off performance for HA that way, but if HA is your goal, it might be worthwhile. And I&#39;m glad I&#39;m not the only one to think that, as Lukas Krattiger has similar view, that &quot;Most scalable approach is still classic ECMP with PIC Core.&quot;</p>\n\n<p>ECMP was originally used for load balancing, but thanks to the redundancy provided, it can provide protection as well. LFA is purely for protection, and temporary protection at that, while ECMP is permanent from the start. So why not kill 2 birds with one stone, actually 3, because with ECMP, you gain simplicity (and sanity) as well. TI-LFA in particular, looks to me like just an attempt to boost the image of Segment Routing, which itself seems like a resurrection of ATM LANE. Centralized-controller paradigm works well at small scale, and sucks big time at large scale, just like shared-memory vs crossbar fabrics. </p>\n",
         "id": "284",
         "name": " Minh Ha",
         "pub": "2020-12-05T03:42:49",
         "type": "comment"
      },
      {
         "date": "05 December 2020 10:44",
         "html": "<p>@ Minha:    </p>\n\n<p>First of all thanks for your always priceless contributions!      </p>\n\n<p>Regarding your statement: <br />\n&ldquo;As I mentioned above, LFA only gets enforced when it&#39;s enabled/configured. Without LFA enabled, U|ECMP works just fine. In your config, you enabled LFA, so LFA took over and its first choice of backup path would be the ECMP paths if it used the default tie-breakers. <br />\n&ldquo; <br />\nThe issue I have Minha is that on the very same router I have some prefixes that are ECMP&rsquo;ed while others aren&rsquo;t and I therefore need to have LFA for the non ECMP&rsquo;ed prefixes and at the very same time ECMP fast-rehash for the ECMP&rsquo;ed prefixes. <br />\nRegarding the tie-breakers , what I am seeing on IOSXR at least is that the first choice being an ECMP path means that by default one out of the N x ECNP-NHs is chosen as the LFA-backup-NH which is different from  spraying the flows over the N-1 ECMP NHs left which is what fast-rehash does.  </p>\n\n<p>=======================================  </p>\n\n<p>Regarding your statement: <br />\n&ldquo; <br />\nWhat&#39;s their physical or logical relation? That might shed a light on their LFA relationship. <br />\n&ldquo; <br />\nWell spotted Minha !! What I am seeing in production is that IOSXR provides node-protection by default since on a per-prefix basis, every NH on box A is protected with a NH on Box B. The scenario is that of prefixes having N x ECMP-NHs, N/2 on box A and N/2 on box B. LFA is just powered on with no specific config.  </p>\n\n<p>=======================================   </p>\n\n<p>Regarding your statement: <br />\n&ldquo; <br />\nAs for PIC and LFA, they&#39;re not related. PIC is only relevant for BGP <br />\n&ldquo; <br />\nI just wanted to add here that BGP PIC-CORE implementation is, to me, very much fast-rehashing but at a higher indirection level of the hierarchical FIB as it applies to service/BGP prefixes in a BGP multipath scenario. What I found here is that in some implementations the trigger for the fast-rehashing at that higher level can be both an interface-down or a BFD session-down event but in other implementations it&rsquo;s the BFD session-down event only that triggers the fast-rehashing. This is important to know if you need to design a high-performance routing/network solution as in that latter case you might not want to enable BGP multipath for traffic requiring fast-protection via fast-rehashing. Having said that, to make things even more complex, I also found that some implementations make use of the central CPU to scale BFD up while others use the CPU on the line card. At the end of the day you do need to lab these solutions with the actual production HW and the actual production routing scenario and measure the convergence time which is not always easy when you have Terabytes of complex traffic flowing through your box&hellip;. &#x2639;  </p>\n\n<p>ciao <br />\nAndrea  </p>\n",
         "id": "289",
         "name": "Andrea Di Donato",
         "pub": "2020-12-05T10:44:37",
         "type": "comment"
      }
   ],
   "count": 4,
   "type": "post",
   "url": "2020/12/fast-failover-techniques.html"
}
