<div class="comments post" id="comments">
  <h4>10 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2749094095560731630">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2749094095560731630" href="#2749094095560731630">27 November 2008 14:53</a>
              </span>
            </div>
            <div class="comment-content">you missed to mention that not only shouldn't you trust "independent experts" neither schouldn't you trust "journalists" in general and especially not technology "journalists".<BR/><BR/>by the way the nss press release which the networkworldstory refers to is from Nov. 2007!?<BR/><BR/>"Using IPS in your router can turn a 60G router into a 5G one or even a 100M bit/sec device" - the only cisco routers i know about which support IOS-IPS are 800 to 73xx, none of them is not even near "2G". all other cisco IPS solutions are either appliance or module based. And if you put a 500Mbps labeled IPS module in your 60G router, not a good idea anyway.<BR/><BR/>And by the way if you start to use gravity in your routers then all routers  turn into a "1G".</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6686887165315274107">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6686887165315274107" href="#6686887165315274107">28 November 2008 21:35</a>
              </span>
            </div>
            <div class="comment-content">You refer "[configuring] the network in a way that effectively disables the hardware." What are some non-obvious examples of this besides the one you named? as I didn't know that multiple GRE tunnels terminating at one loopback was bad design.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5674519476644697351">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5674519476644697351" href="#5674519476644697351">28 November 2008 22:56</a>
              </span>
            </div>
            <div class="comment-content">@Anonymous: terminating multiple GRE tunnels on one loopback interface causes problems if the ASICs cannot do a lookup on the source IP address, which is the case with some hardware.<BR/><BR/>To figure out the exact limitations of your particular combination of hardware+software, it's best to talk to your Cisco SE or (if you're an end user) your Professional Services partner.<BR/><BR/>@Michael: You forgot to mention that you can temporarily turn 1G gravity routers into 10G boxes if you drop them.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6268306880009088132">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6268306880009088132" href="#6268306880009088132">05 December 2008 03:08</a>
              </span>
            </div>
            <div class="comment-content">To Ivan Pepelnjak,<BR/><BR/>"terminating multiple GRE tunnels on one loopback interface causes problems if the ASICs cannot do a lookup on the source IP address, which is the case with some hardware."<BR/><BR/>Could you please give me the idea which model of cisco router got the problem about ASICs?<BR/><BR/>Thank you very much!</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4876650220188541307">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11980981309011002583" rel="nofollow">Eric</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4876650220188541307" href="#4876650220188541307">16 December 2008 18:37</a>
              </span>
            </div>
            <div class="comment-content">The 7600 in particular will warn you if you attempt to terminate multiple tunnels on a single loopback or interface.<BR/><BR/>It provides a warning message about traffic will now be software switched.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5355317173365034004">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5355317173365034004" href="#5355317173365034004">25 February 2009 17:53</a>
              </span>
            </div>
            <div class="comment-content">Ivan,<BR/>Can you point me in the direction where you got your stats for the FWSM being a bottle neck?<BR/>Thanks.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7791680385903629831">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7791680385903629831" href="#7791680385903629831">25 February 2009 20:04</a>
              </span>
            </div>
            <div class="comment-content">@anonymous: Very simple - the first bottleneck is the bus bandwidth between FWSM and the backplane. I've probably got the figures from the FWSM book published by Cisco Press (I was reading it at approximately that time).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1276721740409245488">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1276721740409245488" href="#1276721740409245488">25 February 2009 21:27</a>
              </span>
            </div>
            <div class="comment-content">Thanks.  Are you referring to the 6-Gbps dot1q EtherChannel connection to the backplane?  If so I am wondering how this is a bottleneck when the specs for code &lt;3.2 all state 5Gbps max throughput.  Starting with 4.0 code you can send some flows up to the SUP for a max throughput of 20Gbps.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5640172094829167886">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5640172094829167886" href="#5640172094829167886">08 March 2009 14:00</a>
              </span>
            </div>
            <div class="comment-content">The FWSM-6500/7600 EtherChannel interconnect has an obvious limitation because a single IP flow will be glued to one EC 1Gbps member due to src-dst XOR balancing, maxing out your VPN tunnel at 1 Gbps each. <BR/><BR/>The 4.0 code can indeed alleviate that problem by pushing forwarding to the PFC, but you should only do this for stateless flows (UDP, ESP,...), as you lose all stateful checks after doing so.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7814261003137840345">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://www.goanime.com" rel="nofollow">Chad Myers</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7814261003137840345" href="#7814261003137840345">15 May 2009 02:21</a>
              </span>
            </div>
            <div class="comment-content">The loopback / single source restriction for hardware acceleration is a restriction of the supervisor.  If using the ipsec spa it works perfectly fine.  Originally the documentation had you create a dummy tunnel against the loopback first in order to ensure that the rest of the tunnels would get passed on to the spa.<br /><br />That restriction is also annoying.  When setting up a head-end for N remote sites who is going to create N different loopbacks, each with a unique address, simply to create the tunnel?  It's not something that is expected and goes against how it would normally be deployed.  A single tunnel to manage the device, fine, but not if it's going to be some sort of tunnel aggregator.  And that restriction is very, very poorly documented.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
