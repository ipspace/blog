{
  "comments": [
    {
      "comments": [
        {
          "date": "07 December 2016 14:01",
          "html": "The only major data center switching vendor with decent MPLS support on reasonably-priced switches is Juniper. Arista has no real MPLS control plane, Cisco has MPLS on Nexus 7000...",
          "id": "8960008061100785246",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-12-07T14:01:26.597+01:00",
          "ref": "3762837035398554554",
          "type": "comment"
        },
        {
          "date": "07 December 2016 15:48",
          "html": "Actually also HPE 5900 supports it, and seems to be very cheap.<br /><br />Also they have no license requirement for any feature that is supported on the box.<br /><br />Disclaimer: I&#39;ve used many of them, but never configured MPLS/VPLS. From the docs, it works and is supported.",
          "id": "4313189299303893292",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Andrea",
          "profile": null,
          "pub": "2016-12-07T15:48:54.717+01:00",
          "ref": "3762837035398554554",
          "type": "comment"
        }
      ],
      "date": "07 December 2016 11:12",
      "html": "Just for completeness, if one is keen to a little more additional complexity (but not that much if compared with VXLAN/EVPN) you can use VPLS/VPWS over MPLS.<br />This is a fair standard and should be available on most platform, almost on any vendor.<br />",
      "id": "3762837035398554554",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Andrea",
      "profile": null,
      "pub": "2016-12-07T11:12:43.187+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "07 December 2016 14:02",
          "html": "OMG, we&#39;re getting old... Haven&#39;t realized it&#39;s been THAT long.<br /><br />As for L2 transport, as long as virtual switches keep pretending the earth is flat (I&#39;m looking at VMware ;), we&#39;ll be asked to provide it.",
          "id": "1984915150871635748",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-12-07T14:02:37.292+01:00",
          "ref": "5841848252049146222",
          "type": "comment"
        }
      ],
      "date": "07 December 2016 12:40",
      "html": "&quot;Our systems integrator partner expressed a view that VXLAN is still very new&quot; - my personal advice, change immediately your system integrator. VXLAN has been around for 4-5 years, it is not &quot;very new&quot;. Ivan wrote the first post on VXLAN in August 2011 ! But I have a more philosophical question, why would you need an L2 transport ? Isn&#39;t much better to build an L3-only Data Center, using well known and well proven standards, and getting rid of all terrible L2 stuff ? OpenContrail operating in L3-only mode is my dream, but it needs a better support, unfortunately with current support it is not realistic to run it in production networks !!!",
      "id": "5841848252049146222",
      "image": "https://1.bp.blogspot.com/-CrF2hgzSoTQ/WoyqrG4y49I/AAAAAAAAH8M/KLGgfrBE1R0H8T0kc5SxwuThLaoxr6mlACK4BGAYYCw/s32/Pizzo%2BCamarda.JPG",
      "name": "Ammiraglio Tofonoto",
      "profile": "https://www.blogger.com/profile/04524005885569437211",
      "pub": "2016-12-07T12:40:11.209+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "date": "07 December 2016 15:02",
      "html": "More than right Ivan, but just think about how much the world without MAC addresses and L2 switches would be easier (no ARP, no broadcast storms, no STP, and so forth) !!! Networks would be much simpler to design and operate (but probably our friends in Juniper and Cisco would not be so happy ...).",
      "id": "5793139904021683769",
      "image": "https://1.bp.blogspot.com/-CrF2hgzSoTQ/WoyqrG4y49I/AAAAAAAAH8M/KLGgfrBE1R0H8T0kc5SxwuThLaoxr6mlACK4BGAYYCw/s32/Pizzo%2BCamarda.JPG",
      "name": "Ammiraglio Tofonoto",
      "profile": "https://www.blogger.com/profile/04524005885569437211",
      "pub": "2016-12-07T15:02:21.256+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "date": "07 December 2016 15:22",
      "html": "HPE 5940 now too supports VXLAN/EVPN ;)",
      "id": "5216018885977205688",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2016-12-07T15:22:00.130+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "09 December 2016 21:44",
          "html": "I tell people that Cisco VXLAN/EVPN and ACI fabrics cost and perform virtually the same. It&#39;s the same HW after all.<br /><br />VXLAN/EVPN gives you a thousand manual config points, each of which can crash your network in wondrous and unpredictable ways. You&#39;ll get really good at troubleshooting VXLAN/EVPN, and will master things like &quot;symmetric IRB&quot; and &quot;bud nodes&quot; and BGP type-2 advertisements. And, you will learn the hard way how not to engineer and secure the underlay, how to not do code upgrades, what not to do with VPC, etc. (Cisco highly recommends you buy Nexus Fabric Manager software to help smooth over some of that complexity)<br /><br />In contrast, ACI is the &quot;easy button.&quot; An optimized L2/L3 fabric for dozens or hundreds of leaf switches, all centrally managed, usable as fast as you can rack and cable the leaves. For L2 VLANs and L3 SVI&#39;s, ACI is goofproof. vCenter integration is easy, free, and immediately useful. It&#39;s only when you get into service graphs and contracts that ACI gives you enough rope to hang yourself.<br /><br />ACI has a learning curve, but it&#39;s at a high level, it encourages consistency, and it yields immediate value in terms of DC-wide visibility.   VXLAN/EVPN&#39;s learning curve is all about the nuts-and-bolts of the forwarding technology -- MAC, ARP, VNIs-to-VLAN mapping, uplink versus downlink ports, BGP RR&#39;s, etc. With VXLAN/EVPN, it&#39;s easy for the undisciplined engineer to get lost, lose site of the high level, and build an unsupportable monster.",
          "id": "2698056489373334957",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "lcw",
          "profile": "https://www.blogger.com/profile/10562240567667879482",
          "pub": "2016-12-09T21:44:48.335+01:00",
          "ref": "4827780952325648349",
          "type": "comment"
        }
      ],
      "date": "07 December 2016 17:00",
      "html": "My $0.02 on ACI:<br /><br />The learning curve is no different than anything else - if you haven&#39;t done EVPN, then you&#39;d have to learn that too, just in ACI it can be admittedly frustrating since you&#39;re trying to figure out a GUI that should be &quot;simple&quot; instead of a new &quot;complex&quot; technology. (FWIW I think its a decent - not great - GUI, but I&#39;m used to it at this point)<br /><br />I also think ACI is just like anything else -- there are no rails put up to keep you from doing stupid shit! You can do lots of dumb things with  PBR and VACLs and stupid things like that and ACI is no different. On the whole though I think (especially from 1.2+) its a very solid platform that if nothing else handles firmware management and fault reporting quite nicely (I think its great for lots more than that, but at a minimum I think those are two nice aspects).",
      "id": "4827780952325648349",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Carl",
      "profile": "https://comeroutewithme.com",
      "pub": "2016-12-07T17:00:41.178+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "09 December 2016 09:07",
          "html": "While I totally agree with your take on the subject, unfortunately many engineers don&#39;t have the luxury of starting the discussion at that point, and once you&#39;re faced with &quot;build us a L2 fabric&quot; decision made without even involving the networking team in the process, you have to find a solution that will do the least damage ;)",
          "id": "838749833604917996",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-12-09T09:07:46.141+01:00",
          "ref": "4785000346078343366",
          "type": "comment"
        }
      ],
      "date": "08 December 2016 16:09",
      "html": "Well as a Network Engineer or Architect I believe the right question should be How L2 vs L3 fabric differs and what are Pros and Cons of each approach. <br /><br />People have been running Layer 2 only DCs for ages, some implementations were broken and others were done little cleaner.<br /><br />We should also remember that sometimes even the most clean solution on paper or technically also fails at some point, so there are always known unknowns and unknown unknowns :)<br /><br />Last week encountered a OTV bug causing Data Centre meltdown.<br /><br />Another important question would be how you migrate your current fat DC to New DC fabric with minimal or no disruption.<br /><br />Here is my take around some of considerations when it comes to DC fabrics and don&#39;t think mentioned VxLAN or L2 vs L3 fabric any where :)<br /><br />http://deepakarora1984.blogspot.in/2016/12/data-centre-fabric-design-considerations.html<br /><br />HTH...<br />Evil CCIE",
      "id": "4785000346078343366",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "A Network Artist",
      "profile": "https://www.blogger.com/profile/06314916176190119200",
      "pub": "2016-12-08T16:09:29.437+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "11 December 2016 16:46",
          "html": "I did a design almost exactly like this with a customer a few months ago. In the end we decided to go for VXLAN between data centers.",
          "id": "4333631650582611750",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-12-11T16:46:15.673+01:00",
          "ref": "8761806827747828363",
          "type": "comment"
        },
        {
          "date": "11 December 2016 19:22",
          "html": "Thanks for the quick response Ivan! What is the advantage of using VXLAN vs LACP between DCs in that scenario?",
          "id": "4012327572744076116",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Dennis",
          "profile": null,
          "pub": "2016-12-11T19:22:16.542+01:00",
          "ref": "8761806827747828363",
          "type": "comment"
        },
        {
          "date": "11 December 2016 20:15",
          "html": "You don&#39;t pretend two boxes are a single device (and thus are less likely to hit &quot;interesting&quot; bugs).",
          "id": "2925104782945772435",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-12-11T20:15:43.466+01:00",
          "ref": "8761806827747828363",
          "type": "comment"
        }
      ],
      "date": "11 December 2016 13:16",
      "html": "As Ivan wrote MLAG seems dated, however what is the opinion for a very small deployment?<br />I&#39;m talking about 4 meshed core switches (10g/40g), 2 per DC with private redundant fiber between DCs.<br />ESX hosts will be directly connected and there will be a few L2 only switches for physical hosts. Those connections should be redundant towards both of the core switches per DC.<br /><br />As this is based on Juniper gear what would be the advantage of using VXLAN + EVPN vs. MC-LAG? I think for using BGP there is an extra license required and it seems a little more complicated to setup and operate than going with MC-LAG between the two core switches in each DC. Also note there is no NSX license available so an overlay would have to be terminated on the core switches.",
      "id": "8761806827747828363",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dennis",
      "profile": null,
      "pub": "2016-12-11T13:16:50.524+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "14 December 2016 15:43",
          "html": "Hi Enrique,<br /><br />Thanks for the comment. I&#39;m positive there are tons of niche environments with special requirements (yours is obviously one of them); I&#39;m just continuously upset that we get &quot;need large L2 domains&quot; by default because someone didn&#39;t even consider the impact of what they want to have.<br /><br />What you&#39;re proposing definitely makes sense in your particular environment, but I don&#39;t expect it to be implemented anytime soon (not sure whether you could do it yourself using Switch Light OS as the baseline). It might be better to work with a more traditional Linux-based switch and control the forwarding rules (MAC / TCAM tables) from your own userspace agent.",
          "id": "1647918213488473415",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-12-14T15:43:34.846+01:00",
          "ref": "1455377292938335838",
          "type": "comment"
        },
        {
          "date": "14 December 2016 18:01",
          "html": "That&#39;s a nice idea, depending on its response time it could be feasible. We will explore it, thanks!",
          "id": "7312455514624719667",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Enrique Vallejo",
          "profile": "http://personales.unican.es/vallejoe/",
          "pub": "2016-12-14T18:01:03.444+01:00",
          "ref": "1455377292938335838",
          "type": "comment"
        }
      ],
      "date": "14 December 2016 11:53",
      "html": "As part of a research project we studied the use of Ethernet in large HPC systems. In addition to the typical VM mobility, we found a couple of requirements for large L2 domains in such environments: transport protocols that don&#39;t run on top of IP (such as RoCEv1 or Open-MX) and service announcement mechanisms using L2 broadcast (again, such as Open-MX). I guess you would label them as &quot;wrongly designed stacks&quot;, but the thing is that they are used.<br /><br />In our system we considered low-diameter topologies to reduce power consumption, requiring per-packet non-minimal adaptive routing. This is not supported by any of the four &quot;commodity&quot; alternatives that you detail, so we proposed an extension to OpenFlow switches to be able to react in micro-second time (as you argue in other post, apart from being dead, OpenFlow does not react quickly). If you have any interest, we have a more detailed discussion <a href=\"http://personales.unican.es/vallejoe/Publications/Benito%20-%20HiPC&#39;15%20-%20On%20the%20Use%20of%20Commodity%20Ethernet%20Technology%20in%20Exascale%20HPC%20Systems.pdf\" rel=\"nofollow\">in the paper</a>. <br />",
      "id": "1455377292938335838",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Enrique Vallejo",
      "profile": "http://personales.unican.es/vallejoe/",
      "pub": "2016-12-14T11:53:43.471+01:00",
      "ref": "9090669448465741670",
      "type": "comment"
    }
  ],
  "count": 18,
  "id": "9090669448465741670",
  "type": "post",
  "url": "2016/12/q-building-layer-2-data-center-fabric.html"
}