comments:
- comments:
  - date: 06 April 2016 00:24
    html: I totally agree with you, it all starts and sounds &quot;open&quot; at the
      beginning or on the marketing front.. but every vendor has to include their
      own reincarnation or special sauce of SDN. One vendor&#39;s Openflow implementation
      may not be compatible with another.. one vendor requires their hardware to be
      purchased (ACI/Cisco), another may work as an overlay and not care about the
      underlying infrastructure/gear (Vmware/NSX, Microsoft HNV). However, we still
      need to pay for licenses whether it is a NFV or hardware appliance. I really
      love the idea that networking is (finally) becoming more abstracted away from
      the hardware, accessible via an API.. I will admit that there are a number of
      solid VM &quot;appliances&quot; which are free and may fit the task at hand
      (Vyos, Quagga, SecurityOnion, Endian UTM, NGINX, etc) even better would be if/when
      these appliances took advantage of DPDK, offering much better performance.
    id: '5840052654415976818'
    image: https://resources.blogblog.com/img/blank.gif
    name: Mario
    profile: null
    pub: '2016-04-06T00:24:09.930+02:00'
    ref: '1215102374002719282'
    type: comment
  date: 05 April 2016 21:33
  html: "Is it only my that doesn\xB4t want to listen any longer to what vendors say\
    \ about &quot;their&quot; SDN solutions?<br />It was once said that you can do\
    \ what you want with your low cost white box switches. But you have to pay an\
    \ arm and a leg to get all licenses together. <br />Of course you can do everything\
    \ yourself.<br />In both cases you need power, may it be the power of money or\
    \ men. In the latter case you gain at least know-how instead of the first one,\
    \ where you inherit dependency."
  id: '1215102374002719282'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Michael67
  profile: https://www.blogger.com/profile/03014282355010119539
  pub: '2016-04-05T21:33:42.023+02:00'
  ref: '35446961487536709'
  type: comment
- date: 08 April 2016 16:36
  html: 'Hello Ivan,<br /><br />I have studied ACI and NSX in pretty good detail and
    have worked on PoC/LAB implementation of both platforms. I used to think that
    NSX having VTEPS that could do routing on locally on the host for VMs in different
    subnets was a pretty awesome feature. Later, as you also mentioned in this presentation,
    Cisco added the VTEP feature through AVS to the ACI platform which allowed the
    same function.<br /><br />Then, one of my very intelligent colleagues pointed
    out to me that being able to route in between subnets on the same host does not
    achieve much at all. An example will illustrate this idea well: Assuming an application
    uses six VMs, 3 DB, 2 APP and 1 Web server. Also assume DRS is used (which it
    is in most production environments I&#39;ve seen) and there are 20, 30 or 50 ESXi
    hosts. Statistically, especially if this application&#39;s VMs are resource intensive,
    they will all end up on different hosts. Then, being able to route within the
    hosts becomes a moot point. This results in no architectural difference between
    ACI and NSX assuming AVS/VTEP with ACI is not utilized for 90 or 95%+ of the east-west
    traffic.<br /><br />My immediate takeaway is that to avoid the complexity added
    by VTEPs on every ESXi host (and yes, it does add a little bit of complexity),
    I&#39;d rather forgo the slight performance gain that I might get on ESXi hosts
    routing locally for a very small portion of the traffic and have all the routing
    be handled by the physical fabric.<br /><br />Even with NSX, in the same scenario,
    with 6 VMs spread across 20, 30 or 50 ESXi hosts, most of the east-west traffic
    will end travelling encapsulated over the physical network anyways - and I&#39;d
    rather have the flexibility, visibility and troubleshooting capability that I&#39;d
    be achieve through the ACI physical fabric than the logical NSX overlay. With
    ACI normalization of traffic between VLANs and VXLAN, virtual and physical, I
    can troubleshoot traffic in between virtual-virtual, virtual-physical and physical-physical
    all in the same manner. With NSX, the troubleshooting has improved significantly
    since version 6.2, but, there is probably still a long way to go to get the type
    of visibility you get in ACI.<br /><br />What is your take on this situation?
    <br /><br />Thanks.'
  id: '235305677950427694'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Salman Naqvi
  profile: https://www.blogger.com/profile/11529916018516958539
  pub: '2016-04-08T16:36:14.702+02:00'
  ref: '35446961487536709'
  type: comment
count: 3
id: '35446961487536709'
type: post
url: 2016/04/real-life-software-defined-security.html
