<div class="comments post" id="comments">
  <h4>2 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="3018331344361027738">
          <!--
          <div class="avatar-image-container">
            <img src="https://mcnicholas.smugmug.com/photos/83566062-M.jpg">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08195400942656152609" rel="nofollow">The Silent Rider</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3018331344361027738" href="#3018331344361027738">08 March 2016 16:07</a>
              </span>
            </div>
            <div class="comment-content">As we got denser, we began to push the limits of UCS oversubscription, but mostly due to the fiber-channel traffic. So we had to increase southbound chassis links from 2 cables to 4 cables, and increase the number of uplinks in the SAN port-channels. Other than that, its hard to outrun the 6200 series fabric interconnects.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="804143518601136211">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09899058943394399795" rel="nofollow">Eric Singer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c804143518601136211" href="#804143518601136211">08 March 2016 19:21</a>
              </span>
            </div>
            <div class="comment-content">@silent rider:<br /><br />Looks like you&#39;re hitting a common limitation of blade architectures. In 2016, I just don’t see the point to blade servers.  Unless you’re running a heavy slant of physical to virtual hosts, it just seems like more complexity, lesser scalability / performance, for little gain.  Even if you’re using UCS for rackmounts, there’s still little win IMO.  It adds a ton of complexity under the guise of simplicity, and it costs an arm and a leg on top of it.  Good old fashion rackmounts are just easier, faster, and more cost effective when it comes to virtualization.  The only selling point I’ve heard for UCS is “what happens if your server crashes?” which is a good point for physical servers, but not so much for virtual ones. I would say the one exception might be if you&#39;re running a 100+ virtual hosts.  At that point, the automation capability you get with UCS might start paying off.<br /><br />I run the exact setup Ivan has been mentioning for a while now (4 years) and it works great.  I have two 5596’s, and all our Vmhosts (32 of them) up link into those switches.  We have a number of hosts that have well over 100 VM’s per and they’re not even breaking a sweat.  <br /></div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
