comments:
- comments:
  - date: 22 March 2016 23:22
    html: There are a lot of great things like this that can be done, that are easy
      for people with some networking experience but can be a disaster if this falls
      in wrong hands. Although the title claims about sysadmins, I guess it should
      say &quot;people with no networking knowledge&quot;. I&#39;m willing to bet
      that  this setup will  become in a host becoming a transit AS , because someone
      will try to peer against different TOR with different ASes and the host become
      a transit AS :).
    id: '5183153374282006526'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Antonio Ojea
    profile: https://www.blogger.com/profile/14615081241229714383
    pub: '2016-03-22T23:22:40.361+01:00'
    ref: '2105756400092121126'
    type: comment
  - date: 23 March 2016 07:59
    html: Well, of course that would happen. That&#39;s why you&#39;d be running BGP,
      not OSPF, so you can filter on AS-path on ToR switches and only accept /32 (or
      /128) prefixes with empty AS-path.
    id: '7777330978533690772'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2016-03-23T07:59:40.147+01:00'
    ref: '2105756400092121126'
    type: comment
  - date: 23 March 2016 08:42
    html: That&#39;s the point of your article, you know it, i know it, and everybody
      that have used bgp before knows it, but devs , sysadmins that haven&#39;t used
      bgp before doesn&#39;t know this. (Pd. This was a real case)
    id: '336567830042589310'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Antonio Ojea
    profile: https://www.blogger.com/profile/14615081241229714383
    pub: '2016-03-23T08:42:28.480+01:00'
    ref: '2105756400092121126'
    type: comment
  - date: 23 March 2016 08:59
    html: There are simple safety mechanisms you can implement as Ivan mentioned.
      Reserve a /24 for the data enter and accept only /32 from that prefix. However,
      the point here is that you should provide a way for developers to consume the
      network as a service. They want to announce their service, they couldn&#39;t
      care less if that&#39;s done by BGP or voodoo. So build an abstraction they
      can consume and keep control of the how for yourself.
    id: '5843177102593797621'
    image: https://1.bp.blogspot.com/-LMLfK3vIwBg/VtcDJHiLQzI/AAAAAAAABik/TMb1dNS7AP8/s32/IMG_0919.png
    name: David Barroso
    profile: https://www.blogger.com/profile/08333059712411851393
    pub: '2016-03-23T08:59:35.285+01:00'
    ref: '2105756400092121126'
    type: comment
  date: 22 March 2016 09:50
  html: Running BGP on an application server has a lot of advantages and it&#39;s
    very easy to do. For example, you could achieve [legacy] application mobility
    by just configuring RFC1918 addresses between your ToR and your servers, assigning
    an application IP to a loopback interface and announcing it. You want to move
    the application to another rack/pod/dc? Just start announcing that IP address
    from another host. And the application guys don&#39;t even have to know that BGP
    is involved in there. That&#39;s what puppet classes are for. Just ask them to
    tag their host with the required class that will take care of the loopback interface
    and the BGP configuration and off you go ;)<br /><br />And don&#39;t forget other
    use cases like anycast services or routing to a hypervisor. Think how simple your
    network might turn out as you could get rid of overlay protocols and just use
    BGP to ensure end to end connectivity and app mobility. <br />
  id: '2105756400092121126'
  image: https://1.bp.blogspot.com/-LMLfK3vIwBg/VtcDJHiLQzI/AAAAAAAABik/TMb1dNS7AP8/s32/IMG_0919.png
  name: David Barroso
  profile: https://www.blogger.com/profile/08333059712411851393
  pub: '2016-03-22T09:50:31.795+01:00'
  ref: '8211505654029451921'
  type: comment
- comments:
  - date: 22 March 2016 16:38
    html: Has anyone actually had the stones to use it?
    id: '4652140462523365672'
    image: https://resources.blogblog.com/img/blank.gif
    name: Matt
    profile: null
    pub: '2016-03-22T16:38:31.152+01:00'
    ref: '2985969361041786364'
    type: comment
  - date: 22 March 2016 16:54
    html: In lab only :) But seems a good-enough BGP implementation...
    id: '8908867979413965794'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2016-03-22T16:54:22.403+01:00'
    ref: '2985969361041786364'
    type: comment
  - date: 25 March 2016 18:16
    html: I would say Hyper-V in general is a viable server/network virtualization
      solution. As Ivan pointed out several years ago (http://blog.ipspace.net/2012/12/hyper-v-network-virtualization-wnvnvgre.html)
      Hyper-V Server 2012 is when Microsoft really started to tackle network virtualization
      and scalability issues. 2012 R2 added some much needed BGP features accessible
      via powershell..I believe OSPF support has also been removed, as it most likely
      was never really used. Server 2016 (nano) added even more nice features (https://technet.microsoft.com/en-ca/library/dn890699.aspx)
      namely access to a programmable network controller/switch, VXLAN support and
      software LB. I&#39;ve been running Hyper-V on production since 2008 release
      without any issues, I am really looking forward to the Hyper-V 2016 and beyond.
      I would still not want to sprawl any VM&#39;s across to another DC, same rules
      applies, keep the failure domains small, here is a nice write-up from Daniel&#39;s
      blog - https://danielhertzberg.wordpress.com/2015/10/11/333/<br />
    id: '6480528350022853579'
    image: https://resources.blogblog.com/img/blank.gif
    name: Mario
    profile: null
    pub: '2016-03-25T18:16:21.227+01:00'
    ref: '2985969361041786364'
    type: comment
  date: 22 March 2016 10:29
  html: Windows Server 2012 R2 supports BGP and it&#39;s easy to configure via powershell
    as BGP Route Health Injection via loopback address.
  id: '2985969361041786364'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2016-03-22T10:29:40.953+01:00'
  ref: '8211505654029451921'
  type: comment
- comments:
  - date: 23 March 2016 09:09
    html: 'What do you mean at scale? Any ToR can do at least a few tens of thousands
      of host routes and recent Broadcom asics can do several hundreds of thousands. '
    id: '8965859984584346640'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2016-03-23T09:09:18.438+01:00'
    ref: '1099449811835210140'
    type: comment
  - date: 23 March 2016 10:19
    html: Depends on the use case. What if we are taking about first hops belonging
      to a national BGP backbone with no aggregation?<br /><br />The FIB relation
      is huge, but not a problem if we just limit server route injection to the endpoints
      that can really need it. Front-end web servers behind a loadbalancer just don&#39;t
      need more than current simple routing. That gives the extra FIB capacity to
      a limited amount of back-end clusters that profit from multihoming and anycast
      RHI.
    id: '3977913513938123091'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2016-03-23T10:19:56.389+01:00'
    ref: '1099449811835210140'
    type: comment
  - date: 23 March 2016 17:17
    html: We talk about interesting stuff and the next moment someone says &quot;let&#39;s
      solve IP address mobility across data centers with this&quot; ;))<br /><br />That&#39;s
      not how things are done, and you&#39;re _NOT_ supposed to leak full Internet
      table into your data center or your host routes into the Internet.<br /><br
      />Anyway, if you have more than ~50K host routes (which means you&#39;re BIG)
      _and_ you plan to propagate them further than a single DC (which means you have
      the Enterprise Craplication mindset), you&#39;re doing something fundamentally
      wrong.
    id: '69343149094727092'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2016-03-23T17:17:24.976+01:00'
    ref: '1099449811835210140'
    type: comment
  - date: 23 March 2016 18:45
    html: In that use case, the DC first L3 hop were high-end PEs of a national BGP/MPLS
      backbone, so even in case of intraDC multihoming or anycasting between infrastructures
      (L2 failure domains), the /32 routes were visible nationally. Not a very cost-effective
      architecture, but with some advantages...<br /><br />But my point was that host
      routing could be feasible for just a minority of endpoints that justify it.
      Just not for every server. So maybe a few hundreds or thousands, and little
      FIB scalability impact. It&#39;s not an all/nothing decission. Could be as small
      as 2/3 nodes of a backend cluster anycasting... Some dozens, etc...<br /><br
      />Regarding the original topic, edge BGP for service announcement on a host
      can be so standarized (also with as much security and protection as wanted at
      the network side) that the server team just doesn&#39;t have to touch the configurations
      further than configuring the loopback and interface addresses and the gateways,
      as they do today.
    id: '2706459577523498873'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2016-03-23T18:45:36.558+01:00'
    ref: '1099449811835210140'
    type: comment
  date: 22 March 2016 18:38
  html: 'Running BGP on servers is feasible, but challenging at big scale. Moving
    from /24 subnets to /32 host routes is difficult for recent ToR switches with
    small FIBs, unless aggregating on a leaf pair (which kills awesome value add services
    such as distant Anycast RHI or Mobility DHCP+RHI). Fortunately, moving from ARP
    on a /24 connected subnet to hundreds of BGP peerings can be solved via intermediary
    Route Servers (IXP-like) specially if we are talking about virtualized environments,
    but /32 host routing is definitely an scalability limitation in large deployments.<br
    /><br />The real solution is always the same: solve the problems with simple networking
    at the application layer. Maybe leaving Server Route Injection for a small number
    of critical clusters that really benefit from Anycasting and Multihoming.<br /><br
    />P.S. The server/networking silos dilemma can be solved with some work on standardization
    of BGP server configurations. BIRD is well suited for that. The server team ends
    up just configuring host /32 addresses and gateways (BGP neighbors) in an standalone
    text file.'
  id: '1099449811835210140'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2016-03-22T18:38:41.530+01:00'
  ref: '8211505654029451921'
  type: comment
- comments:
  - date: 07 September 2016 17:23
    html: Petr&#39;s draft doesn&#39;t touch on edge BGP/IP mobility at all, but it&#39;s
      still an excellent read.
    id: '2403720938151071619'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: steve
    profile: https://www.blogger.com/profile/11424695747492099992
    pub: '2016-09-07T17:23:23.032+02:00'
    ref: '6556760384711376365'
    type: comment
  date: 27 March 2016 21:23
  html: I&#39;ll just leave this here... https://datatracker.ietf.org/doc/draft-ietf-rtgwg-bgp-routing-large-dc/
  id: '6556760384711376365'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Lochii Connectivity
  profile: https://www.blogger.com/profile/16149469473695878840
  pub: '2016-03-27T21:23:24.640+02:00'
  ref: '8211505654029451921'
  type: comment
- date: 08 August 2016 05:55
  html: When using BGP on a VM for mobility, what is the best way to establish a peer
    relationship with a new TOR switch after a live migration? The VM won&#39;t inherently
    know the peer address or the ASN. Cumulus quagga has bgp unnumbered and remote-as
    external, but what about other vendors? Is live vm mobility with bgp on servers
    only possible with cumulus? Maybe derive peer address and ASN from LLDP or CDP?
  id: '6316656933100441409'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: k
  profile: https://www.blogger.com/profile/11230115511504237242
  pub: '2016-08-08T05:55:34.326+02:00'
  ref: '8211505654029451921'
  type: comment
count: 17
id: '8211505654029451921'
type: post
url: 2016/03/sysadmins-shouldnt-be-involved-with.html
