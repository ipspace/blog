<div class="comments post" id="comments">
  <h4>17 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2105756400092121126">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08333059712411851393" rel="nofollow">David Barroso</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2105756400092121126" href="#2105756400092121126">22 March 2016 09:50</a>
              </span>
            </div>
            <div class="comment-content">Running BGP on an application server has a lot of advantages and it&#39;s very easy to do. For example, you could achieve [legacy] application mobility by just configuring RFC1918 addresses between your ToR and your servers, assigning an application IP to a loopback interface and announcing it. You want to move the application to another rack/pod/dc? Just start announcing that IP address from another host. And the application guys don&#39;t even have to know that BGP is involved in there. That&#39;s what puppet classes are for. Just ask them to tag their host with the required class that will take care of the loopback interface and the BGP configuration and off you go ;)<br /><br />And don&#39;t forget other use cases like anycast services or routing to a hypervisor. Think how simple your network might turn out as you could get rid of overlay protocols and just use BGP to ensure end to end connectivity and app mobility. <br /></div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5183153374282006526">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14615081241229714383" rel="nofollow">Antonio Ojea</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5183153374282006526" href="#5183153374282006526">22 March 2016 23:22</a>
              </span>
            </div>
            <div class="comment-content">There are a lot of great things like this that can be done, that are easy for people with some networking experience but can be a disaster if this falls in wrong hands. Although the title claims about sysadmins, I guess it should say &quot;people with no networking knowledge&quot;. I&#39;m willing to bet that  this setup will  become in a host becoming a transit AS , because someone will try to peer against different TOR with different ASes and the host become a transit AS :).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7777330978533690772">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7777330978533690772" href="#7777330978533690772">23 March 2016 07:59</a>
              </span>
            </div>
            <div class="comment-content">Well, of course that would happen. That&#39;s why you&#39;d be running BGP, not OSPF, so you can filter on AS-path on ToR switches and only accept /32 (or /128) prefixes with empty AS-path.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="336567830042589310">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14615081241229714383" rel="nofollow">Antonio Ojea</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c336567830042589310" href="#336567830042589310">23 March 2016 08:42</a>
              </span>
            </div>
            <div class="comment-content">That&#39;s the point of your article, you know it, i know it, and everybody that have used bgp before knows it, but devs , sysadmins that haven&#39;t used bgp before doesn&#39;t know this. (Pd. This was a real case)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5843177102593797621">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08333059712411851393" rel="nofollow">David Barroso</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5843177102593797621" href="#5843177102593797621">23 March 2016 08:59</a>
              </span>
            </div>
            <div class="comment-content">There are simple safety mechanisms you can implement as Ivan mentioned. Reserve a /24 for the data enter and accept only /32 from that prefix. However, the point here is that you should provide a way for developers to consume the network as a service. They want to announce their service, they couldn&#39;t care less if that&#39;s done by BGP or voodoo. So build an abstraction they can consume and keep control of the how for yourself.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2985969361041786364">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2985969361041786364" href="#2985969361041786364">22 March 2016 10:29</a>
              </span>
            </div>
            <div class="comment-content">Windows Server 2012 R2 supports BGP and it&#39;s easy to configure via powershell as BGP Route Health Injection via loopback address.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4652140462523365672">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Matt</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4652140462523365672" href="#4652140462523365672">22 March 2016 16:38</a>
              </span>
            </div>
            <div class="comment-content">Has anyone actually had the stones to use it?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8908867979413965794">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8908867979413965794" href="#8908867979413965794">22 March 2016 16:54</a>
              </span>
            </div>
            <div class="comment-content">In lab only :) But seems a good-enough BGP implementation...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6480528350022853579">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Mario</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6480528350022853579" href="#6480528350022853579">25 March 2016 18:16</a>
              </span>
            </div>
            <div class="comment-content">I would say Hyper-V in general is a viable server/network virtualization solution. As Ivan pointed out several years ago (http://blog.ipspace.net/2012/12/hyper-v-network-virtualization-wnvnvgre.html) Hyper-V Server 2012 is when Microsoft really started to tackle network virtualization and scalability issues. 2012 R2 added some much needed BGP features accessible via powershell..I believe OSPF support has also been removed, as it most likely was never really used. Server 2016 (nano) added even more nice features (https://technet.microsoft.com/en-ca/library/dn890699.aspx) namely access to a programmable network controller/switch, VXLAN support and software LB. I&#39;ve been running Hyper-V on production since 2008 release without any issues, I am really looking forward to the Hyper-V 2016 and beyond. I would still not want to sprawl any VM&#39;s across to another DC, same rules applies, keep the failure domains small, here is a nice write-up from Daniel&#39;s blog - https://danielhertzberg.wordpress.com/2015/10/11/333/<br /></div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1099449811835210140">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1099449811835210140" href="#1099449811835210140">22 March 2016 18:38</a>
              </span>
            </div>
            <div class="comment-content">Running BGP on servers is feasible, but challenging at big scale. Moving from /24 subnets to /32 host routes is difficult for recent ToR switches with small FIBs, unless aggregating on a leaf pair (which kills awesome value add services such as distant Anycast RHI or Mobility DHCP+RHI). Fortunately, moving from ARP on a /24 connected subnet to hundreds of BGP peerings can be solved via intermediary Route Servers (IXP-like) specially if we are talking about virtualized environments, but /32 host routing is definitely an scalability limitation in large deployments.<br /><br />The real solution is always the same: solve the problems with simple networking at the application layer. Maybe leaving Server Route Injection for a small number of critical clusters that really benefit from Anycasting and Multihoming.<br /><br />P.S. The server/networking silos dilemma can be solved with some work on standardization of BGP server configurations. BIRD is well suited for that. The server team ends up just configuring host /32 addresses and gateways (BGP neighbors) in an standalone text file.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="8965859984584346640">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8965859984584346640" href="#8965859984584346640">23 March 2016 09:09</a>
              </span>
            </div>
            <div class="comment-content">What do you mean at scale? Any ToR can do at least a few tens of thousands of host routes and recent Broadcom asics can do several hundreds of thousands. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3977913513938123091">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3977913513938123091" href="#3977913513938123091">23 March 2016 10:19</a>
              </span>
            </div>
            <div class="comment-content">Depends on the use case. What if we are taking about first hops belonging to a national BGP backbone with no aggregation?<br /><br />The FIB relation is huge, but not a problem if we just limit server route injection to the endpoints that can really need it. Front-end web servers behind a loadbalancer just don&#39;t need more than current simple routing. That gives the extra FIB capacity to a limited amount of back-end clusters that profit from multihoming and anycast RHI.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="69343149094727092">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c69343149094727092" href="#69343149094727092">23 March 2016 17:17</a>
              </span>
            </div>
            <div class="comment-content">We talk about interesting stuff and the next moment someone says &quot;let&#39;s solve IP address mobility across data centers with this&quot; ;))<br /><br />That&#39;s not how things are done, and you&#39;re _NOT_ supposed to leak full Internet table into your data center or your host routes into the Internet.<br /><br />Anyway, if you have more than ~50K host routes (which means you&#39;re BIG) _and_ you plan to propagate them further than a single DC (which means you have the Enterprise Craplication mindset), you&#39;re doing something fundamentally wrong.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2706459577523498873">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2706459577523498873" href="#2706459577523498873">23 March 2016 18:45</a>
              </span>
            </div>
            <div class="comment-content">In that use case, the DC first L3 hop were high-end PEs of a national BGP/MPLS backbone, so even in case of intraDC multihoming or anycasting between infrastructures (L2 failure domains), the /32 routes were visible nationally. Not a very cost-effective architecture, but with some advantages...<br /><br />But my point was that host routing could be feasible for just a minority of endpoints that justify it. Just not for every server. So maybe a few hundreds or thousands, and little FIB scalability impact. It&#39;s not an all/nothing decission. Could be as small as 2/3 nodes of a backend cluster anycasting... Some dozens, etc...<br /><br />Regarding the original topic, edge BGP for service announcement on a host can be so standarized (also with as much security and protection as wanted at the network side) that the server team just doesn&#39;t have to touch the configurations further than configuring the loopback and interface addresses and the gateways, as they do today.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6556760384711376365">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/16149469473695878840" rel="nofollow">Lochii Connectivity</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6556760384711376365" href="#6556760384711376365">27 March 2016 21:23</a>
              </span>
            </div>
            <div class="comment-content">I&#39;ll just leave this here... https://datatracker.ietf.org/doc/draft-ietf-rtgwg-bgp-routing-large-dc/</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="2403720938151071619">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11424695747492099992" rel="nofollow">steve</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2403720938151071619" href="#2403720938151071619">07 September 2016 17:23</a>
              </span>
            </div>
            <div class="comment-content">Petr&#39;s draft doesn&#39;t touch on edge BGP/IP mobility at all, but it&#39;s still an excellent read.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6316656933100441409">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11230115511504237242" rel="nofollow">k</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6316656933100441409" href="#6316656933100441409">08 August 2016 05:55</a>
              </span>
            </div>
            <div class="comment-content">When using BGP on a VM for mobility, what is the best way to establish a peer relationship with a new TOR switch after a live migration? The VM won&#39;t inherently know the peer address or the ASN. Cumulus quagga has bgp unnumbered and remote-as external, but what about other vendors? Is live vm mobility with bgp on servers only possible with cumulus? Maybe derive peer address and ASN from LLDP or CDP?</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
