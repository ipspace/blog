comments:
- date: 09 February 2016 09:17
  html: I totally agree with you Ivan. Also I believe that with a reliable design
    plus what you called automation magic, there won&#39;t be any complexity.<br />This
    even might be helpful when connecting the DC block to the Core and having data
    transfer with WAN. Besides, nowadays more Enterprises are becoming SPs and using
    BGP as the Core routing protocol.<br />The level of granularity and traffic engineering
    is incredible.<br />The only drawback I see is the staff the knowledge in BGP
    area; though they can be educated in that too, hence at the end of the day they
    want to be Network engineers!<br /><br />And of course we know that it&#39;s not
    for everywhere and business needs should be taken into account.<br /><br />Looking
    forward to the webinar today.
  id: '2112031300180004149'
  image: https://lh4.googleusercontent.com/-ZiU4mPQaSAc/AAAAAAAAAAI/AAAAAAAABSU/twai0RLqo44/s32-c/photo.jpg
  name: Mohammad Moghaddas
  profile: https://www.blogger.com/profile/15452464039180688994
  pub: '2016-02-09T09:17:04.465+01:00'
  ref: '8010770320418191090'
  type: comment
- date: 09 February 2016 10:25
  html: 'One more example: Cisco relies on BGP as routing protocol for its &quot;Dynamic
    Fabric Automation&quot; (DFA).'
  id: '4116132709415870605'
  image: https://resources.blogblog.com/img/blank.gif
  name: Martin
  profile: null
  pub: '2016-02-09T10:25:57.680+01:00'
  ref: '8010770320418191090'
  type: comment
- date: 09 February 2016 15:50
  html: 'This shouldn&#39;t come as a surprise to anyone. Petr Lapukhov had a NANOG
    presentation about BGP as the DC IGP a few years back - they implemented it at
    MS Bing. <br /><br />Brocade will be rolling out an IP fabric leveraging BGP (EVPN)
    in March.<br /><br />Personally, I&#39;d like to see more literature on BGP timer
    tuning. I think that&#39;s often misunderstood, by me at least. '
  id: '2891280148997577384'
  image: https://resources.blogblog.com/img/blank.gif
  name: Steve
  profile: null
  pub: '2016-02-09T15:50:03.706+01:00'
  ref: '8010770320418191090'
  type: comment
- date: 09 February 2016 18:28
  html: I think the question is if we need to use link-state or distance vector in
    DC&#39;s today. Link state probably made sense when fabric is asymmetric interms
    of different ecmps, link-speeds etc. With symmetric fabric where all things equal
    with leaf-spine arch, does it make sense for a node to know every bit of fabric
    info or just reach-ability is suffice ?.<br /><br />As far as tooling is concerned,
    BGP/OSPF/ISIS have been developed organically based on their role in the network,
    so can we can always achieve what one is not intended to be.
  id: '7711023460231292939'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Bhargav
  profile: https://www.blogger.com/profile/03803244374816795623
  pub: '2016-02-09T18:28:14.509+01:00'
  ref: '8010770320418191090'
  type: comment
- date: 10 February 2016 05:04
  html: BGP was considered a WAN technology and slowly got in to DC&#39;s for sometime
    now.<br /><br />It is starting to penetrate in to servers as well. What are your
    thoughts on having BGP running from the servers itself ?
  id: '71127082762953557'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2016-02-10T05:04:29.240+01:00'
  ref: '8010770320418191090'
  type: comment
- date: 10 February 2016 09:10
  html: 'Good article Ivan,<br /><br />personally I&#39;m a big fan of simple (KISS)
    and using BGP as the only routing protocol makes sense to me (even in deployments
    not fitting the &quot;large DC&quot; description in Petr&#39;s draft).<br /><br
    />Especially if we consider that products on the systems-side (think chassis&#39;
    running NSX or Azure-stack) support BGP as a means of interconnecting to the fabric
    (whether injecting host-routes from the chassis to allow for VM-mobility is worth
    considering is probably another discussion but at least BGP has proven itself
    to be able to scale to support this).<br /><br />regards,<br /><br />Alan Wijntje '
  id: '8033647838872620394'
  image: https://resources.blogblog.com/img/blank.gif
  name: Alan Wijntje
  profile: http://www.routz.nl
  pub: '2016-02-10T09:10:28.047+01:00'
  ref: '8010770320418191090'
  type: comment
- comments:
  - date: 19 February 2016 19:29
    html: Of course you can do that - it would work on (almost?) any DC switch. However,
      you&#39;d have to configure different BGP neighbors on different servers.
    id: '4258851963486747206'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2016-02-19T19:29:28.672+01:00'
    ref: '5124011749312586203'
    type: comment
  - date: 23 February 2016 09:39
    html: Hi, is it true that you can&#39;t use unnumbered and BFD simultaneously?
    id: '1680339105197100439'
    image: https://lh6.googleusercontent.com/-sfbs3Zz-00E/AAAAAAAAAAI/AAAAAAAAC2o/U5OLsq6g9v4/s32-c/photo.jpg
    name: Brendan Franklin
    profile: https://www.blogger.com/profile/08303880674892766910
    pub: '2016-02-23T09:39:55.320+01:00'
    ref: '5124011749312586203'
    type: comment
  - date: 23 February 2016 09:54
    html: 'While I remember seeing something on BFD-over-MAC, I can&#39;t find a related
      Internet Draft, and I haven&#39;t seen anyone implementing something along these
      lines.<br /><br />You _might_ use multihop BFD between loopbacks (again, not
      sure whether anyone implemented it), but there&#39;s a simpler option used by
      Cumulus implementation: use BFD between IPv6 LLA.'
    id: '4976184263960601759'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2016-02-23T09:54:36.752+01:00'
    ref: '5124011749312586203'
    type: comment
  date: 19 February 2016 12:03
  html: "Hi Ivan,<br /><br />Due to the limitation of BGP not being able to use unnumbered\
    \ interfaces we must use tricks. It is an interesting trick using the IPv6 Link\
    \ local address for peering proposed by Cumulus.<br /><br />My question is: having\
    \ Nexus switches as spine and leaf, N9K, why we couldn\u2019t use a single SVI\
    \ on each switch where all the SVIs are in the same subnet and the fabric ports\
    \ being used as access switchports for the SVI vlan. BGP peerings will be done\
    \ through the SVI IP addresses and only 1 IPv4 address per device would be used.<br\
    \ />Why this would not be a suitable solution as a Nexus switch can act as a layer\
    \ 3 and layer 2 interface on the same port?<br /><br />Thanks.<br />"
  id: '5124011749312586203'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2016-02-19T12:03:04.172+01:00'
  ref: '8010770320418191090'
  type: comment
count: 10
id: '8010770320418191090'
type: post
url: 2016/02/using-bgp-in-data-center-fabrics.html
