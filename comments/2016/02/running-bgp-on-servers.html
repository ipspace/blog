<div class="comments post" id="comments">
  <h4>35 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="8590995317155939082">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01346466217359422202" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8590995317155939082" href="#8590995317155939082">17 February 2016 13:07</a>
              </span>
            </div>
            <div class="comment-content">As with many design decisions, it&#39;s not automatically bad all the time.  It&#39;s about the overall design design goals, and *you&#39;re ability to validate the design.*  </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6036397790791736460">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://www.salmannaqvi.com" rel="nofollow">Salman Naqvi</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6036397790791736460" href="#6036397790791736460">17 February 2016 13:52</a>
              </span>
            </div>
            <div class="comment-content">From my experience (being one of the principle network engineers at six data centers), unless you control the server, and understand the software that runs BGP, OSPF or another routing protocol, DO NOT run routing protocols directly on end hosts. My prior counterparts thought running OSPF on Mainframes was a good idea. Then we had a routing blackhole due to misconfiguration on the server. Twice! The main issue was the Mainframe admins lack of networking/OSPF knowledge. In reality, there was no requirement that couldn&#39;t be met with a simple secondary route. We didn&#39;t even need anything special like vPC or MLAG. In short, stay away unless you control the server and know what you are doing.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4987600352033771024">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/00440193885039741934" rel="nofollow">JMcA</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4987600352033771024" href="#4987600352033771024">17 February 2016 14:07</a>
              </span>
            </div>
            <div class="comment-content">I think you&#39;re taking a specific failure and turning into too general of a rule to fix it.<br /><br />OSPF has essentially no routing policy control (within an area, anyway, only on area or AS boundaries). Which means it&#39;s hard to stop an end node from becoming a transit link, and increases danger of blackholing failures such as what you apparently experienced. You also end up with the server holding a full set of OSPF routes in its RIB/FIB, which is....inelegant.<br /><br />We (started with OSPF, but learned and switched to) BGP which does provide the routing policy to prevent exactly these sorts of failures.  Only pass a default down to the server, restrict what routes the network will accept from the server. Solves both problems, and works phenomenally well.<br /><br />There are all sorts of other capabilities that come along when you use a good setup for running routing protocols on servers that are beyond this scope.<br /><br />So I would caution against saying you had failures using OSPF, therefore you shouldn&#39;t run *any* routing protocols on servers. Switch to BGP and be happy.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7662477136121559184">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7662477136121559184" href="#7662477136121559184">17 February 2016 17:32</a>
              </span>
            </div>
            <div class="comment-content">Salman, as JMcA pointed out (and I wrote in one of the blog posts I linked to), running OSPF on nodes not under common administration is not a good idea. BGP (or heavily filtered RIP) is the only routing protocol I would run between servers and the network.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6366687533411505184">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://noiproute.wordpress.com" rel="nofollow">John</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6366687533411505184" href="#6366687533411505184">17 February 2016 13:54</a>
              </span>
            </div>
            <div class="comment-content">Ive never really understood why sysadmins don&#39;t run OSPF/RIP on their servers as stubs and source their traffic from a static IP on an installed loopback interface as a /32 (could run the OSPF/RIP interfaces with DHCP that connect to the outside environment.<br /><br />DC disaster recovery scenarios would be much more simple, move the VM, wait for OSPF/RIP to reconverge, and youre done, you wouldn&#39;t need L2 extensions to get it to work, and you could implement it with far less expensive equipment (would also work cross vendor with relative ease too), all you&#39;d need would be DHCP, RIP/OSPF/ISIS<br /><br />obviously you&#39;d have the challenge of having a large adjacency design as you&#39;d have to allow anything in the DHCP range to form an adjacency, and you&#39;d have to be careful that you never ended up with a host as a transit host, but still I think its an interesting design.<br /><br />Im sure if ive got this appalling wrong Ivan is going to shoot my argument full of holes, or some windows person is going to say sourcing the IP address from the loopback will cause windows to disintegrate...<br /></div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="3764911718092669854">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3764911718092669854" href="#3764911718092669854">17 February 2016 17:33</a>
              </span>
            </div>
            <div class="comment-content">No need to punch holes in your argument - we&#39;re perfectly aligned, apart from a &quot;minor&quot; detail - don&#39;t EVER use a link-state routing protocol between devices that are not under common administration. In this case, use BGP.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3008061051749490296">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06190251385478958186" rel="nofollow">Tore Anderson</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3008061051749490296" href="#3008061051749490296">17 February 2016 21:31</a>
              </span>
            </div>
            <div class="comment-content">What sucks about BGP in this regard is that the protocols (or maybe it&#39;s not the protocol, just every single implementation I&#39;ve seen) requires every single neighbour to be explicitly configured. That fits badly with &quot;cloudy&quot; environments where a customer might all of a sudden decide to spin up a dozen or two new VMs from which he wants to immediately begin advertising a set of service addresses. Having to wait for the network admin to configure BGP sessions to each of those new VMs is a non-starter.<br /><br />OSPF is also a non-starter due to the inability to filter out accidental/malicious advertisements, so we&#39;re using RIP, which works well enough. (Of course, we don&#39;t have a RIP *topology*, whatever the router picks up from RIP just gets exported to a more sensible routing protocol.)<br /><br />I&#39;ve always wondered why it&#39;s not possible to have an unspecified peer address in BGP (0.0.0.0/::). That would have solved the problem - I&#39;d just ask the customer to establish BGP sessions to the default router address on their server LAN. Do you know, Ivan?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3617401069915038593">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17383484355925378685" rel="nofollow">plumbis</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3617401069915038593" href="#3617401069915038593">17 February 2016 22:19</a>
              </span>
            </div>
            <div class="comment-content">What you are talking about is BGP Dynamic Peers or Cumulus&#39;s BGP unnumbered, where you can configure &quot;neighbor eth0 remote-as external&quot; and the peer just establishes. <br /><br />Disclaimer: I work for Cumulus</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1715709553031502155">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1715709553031502155" href="#1715709553031502155">18 February 2016 07:52</a>
              </span>
            </div>
            <div class="comment-content">Dynamic BGP peers are a generic solution (work even with a single VLAN/subnet per ToR switch), not sure how many ToR switches have that feature.<br /><br />BGP Unnumbered is a cool solution but requires a compatible BGP daemon on the other end (yeah, I know it&#39;s all based on RFCs and you can make it work with Junos and NX-OS, but it gets kludgy) and L3 interfaces toward servers instead of a single VLAN-per-ToR.<br /><br />So, how about adding dynamic BGP neighbors to Quagga? ;))</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7249728197049564041">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://cumulusnetworks.com" rel="nofollow">Donald Sharp</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7249728197049564041" href="#7249728197049564041">18 February 2016 15:20</a>
              </span>
            </div>
            <div class="comment-content">We do have it:<br /><br />bgp listen range address/mask peer-group <br /><br />The documentation is missing, I&#39;ll get it added.<br /><br />Obligatory Disclaimer: I work for Cumulus :)</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6308782041318247556">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11098363457090827699" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6308782041318247556" href="#6308782041318247556">17 February 2016 15:33</a>
              </span>
            </div>
            <div class="comment-content">If the hosts are stubs, a default gateway is all that is really needed.  A routing protocol is required for (alternate) path selection.<br /><br />Routing on the host went away some time ago because we could fix the dual-attachment/loop avoidance/ RSTP problem with your favorite flavor of MLAG and decrease convergence time, SPF hickups, etc by having less routers in your area / domain and keeping the hosts as &quot;leafs&quot; in the design.<br /><br />Not saying you could not have a rock steady DC with 510 routers for example (500 hosts + 10 routers / VRFs) but one thing is for sure, you are increasing your probability towards stability by lowering the complexity and just managing the 10 routers / VRFs.<br /><br />One of the main reason for the &quot;BGP on the host&quot; discussion lately is for egress path selection between DCs.  PBR, TE, LISP, etc all catches the traffic once it is out of the host and rely on some form of &quot;marker&quot; to apply the proper forwarding.  <br /><br />Now if you have overlay networks originating directly from the host, &quot;maybe&quot; you want to provide the information to the host so that the end to end connection is established over the appropriate egress point in the DC and over the WAN link of choice for this tenant or application.<br /><br />This makes &quot;some&quot; sense if you own both the DCs and control / own the WAN.  <br /><br />Past history has proven however, a number of times, that implementing more granlar controls is not for everyone...  actually few ever do (QoS has been around for ever and I&#39;m still surprised how little of it is actally implemented).<br /><br />BTW BGP has been in DLR &amp; ESG from the begining in NSX-v.<br /><br />My 2 cents :) <br /><br /><br /><br /></div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="2154170787412461394">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2154170787412461394" href="#2154170787412461394">17 February 2016 17:34</a>
              </span>
            </div>
            <div class="comment-content">I have a perfect counter-argument, but it wouldn&#39;t fit into a comment ;)) Just kidding, time to write another blog post.<br /><br />Thanks for the comment!<br />Ivan</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7010556715106993503">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11098363457090827699" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7010556715106993503" href="#7010556715106993503">17 February 2016 21:21</a>
              </span>
            </div>
            <div class="comment-content">It will be a pleasure to read you as always </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5801672567729717471">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17383484355925378685" rel="nofollow">plumbis</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5801672567729717471" href="#5801672567729717471">17 February 2016 22:22</a>
              </span>
            </div>
            <div class="comment-content">You&#39;re assuming mLAG is a good solution. If you don&#39;t need IP mobility, you can simply advertise a /24 per rack (or whatever appropriately sized aggregate) and a default route to the servers. This makes maintenance much easier and no risk of mLAG failover failures bringing down all attached hosts. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2800256748959619034">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/11098363457090827699" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2800256748959619034" href="#2800256748959619034">17 February 2016 22:55</a>
              </span>
            </div>
            <div class="comment-content">I am not assuming anything nor am I saying that implementing or not MLAG is a good thing.  Neither am I saying that running a routing BGP on the host for picking an egress path is a good thing.  Just a recap of what we did in the last 15+ years.<br /><br />sometimes we need to stop drinking our own kool-aid ;)<br /><br />BGP as a mean to solve world hunger has been a recurring topic over the years because we are networking people and we see the world thru our very limited lens.  For instance it is funny how Peer to Peer networking, which handles millions of endpoints with attributes (files, songs, etc) has been designed without the IETF and widely implemented without our help...  </div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3711423625659398849">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3711423625659398849" href="#3711423625659398849">17 February 2016 19:20</a>
              </span>
            </div>
            <div class="comment-content">To get to the presentation go here:<br />https://cumulusnetworks.com/webinars/<br /><br />Click on &#39;On Demand Webinars&#39;<br />Click on the &#39;Watch Now&#39; Under the Demystifying Networking heading.<br />Fill in some info and click on &#39;Watch Now&#39;<br /><br />Full Disclosure:  I work for Cumulus and am going to see if we can make this easier to find.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4099552782865140643">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4099552782865140643" href="#4099552782865140643">17 February 2016 19:53</a>
              </span>
            </div>
            <div class="comment-content">Added the link Cumulus marketing sent me. Thank you!</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6684584310644704157">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17383484355925378685" rel="nofollow">plumbis</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6684584310644704157" href="#6684584310644704157">17 February 2016 22:25</a>
              </span>
            </div>
            <div class="comment-content">Thanks for writing this Ivan. I&#39;ve had countless conversations with customers about routing on the host, using the Linux package, Docker containers and vRouters. The fact this is revolutionary, I believe, is a result of vendors selling L3 licenses (which increases the cost of the datacenter) and the fact they have no reason to encourage good design, since they can&#39;t monetize that server endpoint. Cisco&#39;s CSR1000v has taken a step in the right direction but it&#39;s a resource hog and too expensive, so customers avoid it. I hope more vendors see the value of a L3 only datacenter and build products to enable those customers.<br /><br />Disclaimer: I work at Cumulus</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="7328558544594669107">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7328558544594669107" href="#7328558544594669107">18 February 2016 13:18</a>
              </span>
            </div>
            <div class="comment-content">&gt; The fact this is revolutionary, I believe, is a result of vendors selling L3 licenses<br /><br />Exactly. We were trying OSPF on servers for L3 failover instead of usual L2 but idea died for several reasons, one of them was that you can&#39;t really control link-state protocol announcements all that well. We had some unfortunate more-specific announcements too. Or occasional and unexpected anycast after failover. It wasn&#39;t pretty.<br /><br />I thought about using BGP from the start but it would mean that every ToR switch would cost much more due to standard &quot;ISP tax&quot; license for BGP/IS-IS/MPLS (thank you Juniper, I know you do that because you love us). <br /><br />Other reason was that server team hated it because they didn&#39;t understand it so network team had to do everything network-related by themselves, creating even bigger administrative overhead instead of lowering it.<br /><br />Now it&#39;s back to stretched L2 between datacenters. Brown substance flows as it&#39;s supposed to and world is back to normal.<br /><br />End of story.<br /><br /><br />Funny thing: I actually used that scheme years ago with Solaris servers and Juniper M-series routers with huge success, but back then server admins were actually understanding how network works instead of just googling quick and dirty solutions.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1343782324679862982">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Andrea</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1343782324679862982" href="#1343782324679862982">17 February 2016 23:11</a>
              </span>
            </div>
            <div class="comment-content">Currently we run (for different part) both BGP and OSPF on servers (customer DNSes and shared cache / web blocking).<br /><br />Since servers&#39; teams have zero understanding on this, we (the network team) run them.<br />We employ quagga and bird for diversity, roughly 50/50 percent.<br />Not a single failure, patching really fast.. no dozen of teams to be involved, no offshoring and sudo and ... Unuseful stupid paperwork for a reboot.. I patched all of them (roughly 30) today for CVE-2015-7547 in almost 2h with no service interruption..<br /><br />But.. It&#39;s hard to cope with this stuff if you&#39;re the average point-and-click GUI guy.. Even unix teams are not the way they used to be 10 years ago.. Nobody knows the loadaverage meaning anymore.. How could you expect them to understand non transit routing protocol config?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6862399869311668757">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03803244374816795623" rel="nofollow">Bhargav</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6862399869311668757" href="#6862399869311668757">18 February 2016 00:56</a>
              </span>
            </div>
            <div class="comment-content">Another interesting article Ivan. <br /><br />Looks like there is no black &amp; white answer but the decision to run any routing protocols on servers would probably be based on combination of many factors.<br /><br />1) Operational issue: Extent of Silo between Server &amp; Network teams. Not sure if having just good sysadmin with strong networking knowledge will help for large deployments. <br /><br />2) Complexity: #session as pointed out in one of the comments. Is there enough data to prove more session is more complex ?. On a slightly different note, these days gossip protocol on hosts. <br /><br />3) Scale_of_deployment: If the scale of deployment is small (2-4 switches) then both operational and complexity can be managed better.<br /><br />4) Rate_of_change: #network updates is another factor. Constant network update is not desirable. On VM based deployment, #updates is far less when compared to microservices that have very short life.<br /><br />5) Where SDN: Is it from the host or from ToR ?. Marking en all makes seems fine from the host but there is growing complexity for #policy. Traditional systems usually had these policy at L2/L3 boundary which was simple and easy to manage.<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="9116836639815084536">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/12129459576108497137" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9116836639815084536" href="#9116836639815084536">18 February 2016 01:14</a>
              </span>
            </div>
            <div class="comment-content">&gt;one link failure away from IBM mainframe becoming the core router<br /><br />There&#39;s a great bsdnow blog that talks about a similar scenario.  Episode 103 at the 34:30 mark.  Great story.  Link is below.<br /><br />Thanks,<br /> Phil<br /><br />https://youtu.be/l6XQUciI-Sc?t=2072</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5494851688895337145">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08768566067706348989" rel="nofollow">dreamer</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5494851688895337145" href="#5494851688895337145">18 February 2016 08:26</a>
              </span>
            </div>
            <div class="comment-content">Perhaps secondary to the discussion, but aren&#39;t anycast services also  leveraging routing daemons on servers? Point being, certain sysadmins are well aware of networking concepts before the &quot;full-stack&quot; term became hip.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3069240902919059154">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06036116499201821433" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3069240902919059154" href="#3069240902919059154">19 February 2016 17:30</a>
              </span>
            </div>
            <div class="comment-content">Some anycast designs use that method yes.  I&#39;ve also seen others which use some kind of LB in front of the end servers and they take care of health monitoring and doing the NAT tricks to make it so the servers are just vanilla servers.   A lot of this comes down to skillset on the server side and who manages the network portion of the servers.  We&#39;ve diverged over time to silos.  <br /><br />I like BGP but running it to the servers in order to get dynamic redundancy is a bit overkill unless for instance it&#39;s a VR handling lots of VMs behind it.   It&#39;s just not scalable in a large datacenter to run BGP on the nodes.  That&#39;s why Contrail for instance uses XMPP for that exchange and translates it on a centralized node to BGP.   </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3678346591388781846">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3678346591388781846" href="#3678346591388781846">19 February 2016 19:07</a>
              </span>
            </div>
            <div class="comment-content">Any clever way for running bgp on the hosts in a environment where I wanted to advertise just a loopback ip on a vm on a host? It would be highly suboptimal to have every vm (let say there&#39;s 30) on a specified host all run bgp in order to advertise each of their respective /32 loopback ip&#39;s... static routing on at the host level and redistributing it into bgp obviously wouldn&#39;t scale nor accommodate vm migrations... any ideas on that?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5773986762939013014">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5773986762939013014" href="#5773986762939013014">20 February 2016 17:04</a>
              </span>
            </div>
            <div class="comment-content">Not exactly what you&#39;re looking for, but Project Calico does some of what you want to achieve.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1430159783069473797">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1430159783069473797" href="#1430159783069473797">21 February 2016 16:01</a>
              </span>
            </div>
            <div class="comment-content">Popular VM LB&#39;s run BGP from servers. If that is okay, why not a routing bgp stack from servers is a problem ?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="140515738386479052">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c140515738386479052" href="#140515738386479052">22 February 2016 17:02</a>
              </span>
            </div>
            <div class="comment-content">It&#39;s not a technology, but a people/skills problem. The load balancers are usually configured by people who understand (some) networking.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="199167097837159499">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17193851449183304827" rel="nofollow">Jeff Tantsura</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c199167097837159499" href="#199167097837159499">24 February 2016 02:43</a>
              </span>
            </div>
            <div class="comment-content">I&#39;ve been building an internal project, with Docker overlay over L3VPN (EVPN later), where every veth pair is represented as a PE-CE link. Control plane can be run locally, as an additional container or centrally, programming forwarding logic into OVS.<br />Every Docker host is represented by a single /32, AKA PE loopback, used as NH in BGP updates. Data plane - MPLSoVxLAN, going forward I envision using EVPN VxLAN control plane (draft-ietf-bess-evpn-overlay) <br />So far we have been redistributing connected within VRF so remote end of /31 is the container itself. We have also built libnetwork adapters.<br /><br />Obviously there&#39;s some magic on IP assignment,mapping into internal API&#39;s, etc <br /><br />At some point I will provide more details and perhaps a demo<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8632285392644530887">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/16141388907060749402" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8632285392644530887" href="#8632285392644530887">24 February 2016 18:50</a>
              </span>
            </div>
            <div class="comment-content">Great blog, great comments. Check out osrg/go-bgp on github. BGP as a distributed data store has some interesting properties. Management plane needs a datastore anyways. Only diff is if you want to consolidate things or stack up multiple clustering types. Opaque BGP from our friend Petr Lapukhov at Facebook is pretty awesome imo. I like it much more then forced data structs that I don&#39;t need. Let me set a TLV and encode pictures of cats if I want to. Cya round guys and great convo.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="8946197330183271845">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8946197330183271845" href="#8946197330183271845">24 February 2016 23:06</a>
              </span>
            </div>
            <div class="comment-content">Thanks for the info but how is this relevant to the topic here ?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4963666759388180583">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/16141388907060749402" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4963666759388180583" href="#4963666759388180583">24 February 2016 23:45</a>
              </span>
            </div>
            <div class="comment-content">Hi Anonymous, well we are talking about BGP on servers right?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6397547598523114303">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17193851449183304827" rel="nofollow">Jeff Tantsura</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6397547598523114303" href="#6397547598523114303">26 February 2016 00:38</a>
              </span>
            </div>
            <div class="comment-content">Brent - communities :)<br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5084584442215863336">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/09023765657674084072" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5084584442215863336" href="#5084584442215863336">28 February 2016 18:05</a>
              </span>
            </div>
            <div class="comment-content">@Jeff - alas, using communities (32-bit or 62-bit) to encode arbitrary data is pretty awkward - tried that. Not to mention these are just attributes that need to go with an NLRI (semantic overloading). Not sure why there was no generic opaque payload in BGP from day 2 (MP-BGP) - seemed pretty natural.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3888535207715767786">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/17193851449183304827" rel="nofollow">Jeff Tantsura</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3888535207715767786" href="#3888535207715767786">10 May 2016 09:40</a>
              </span>
            </div>
            <div class="comment-content">@Petr - really?<br />I recall there&#39;s draft-lapukhov-bgp-opaque-signaling :)<br /><br />See you tomorrow at Networking @Scale</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
