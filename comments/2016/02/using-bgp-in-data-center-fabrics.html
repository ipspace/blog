<div class="comments post" id="comments">
  <h4>10 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2112031300180004149">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/15452464039180688994" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2112031300180004149" href="#2112031300180004149">09 February 2016 09:17</a>
              </span>
            </div>
            <div class="comment-content">I totally agree with you Ivan. Also I believe that with a reliable design plus what you called automation magic, there won&#39;t be any complexity.<br />This even might be helpful when connecting the DC block to the Core and having data transfer with WAN. Besides, nowadays more Enterprises are becoming SPs and using BGP as the Core routing protocol.<br />The level of granularity and traffic engineering is incredible.<br />The only drawback I see is the staff the knowledge in BGP area; though they can be educated in that too, hence at the end of the day they want to be Network engineers!<br /><br />And of course we know that it&#39;s not for everywhere and business needs should be taken into account.<br /><br />Looking forward to the webinar today.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4116132709415870605">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Martin</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4116132709415870605" href="#4116132709415870605">09 February 2016 10:25</a>
              </span>
            </div>
            <div class="comment-content">One more example: Cisco relies on BGP as routing protocol for its &quot;Dynamic Fabric Automation&quot; (DFA).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2891280148997577384">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Steve</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2891280148997577384" href="#2891280148997577384">09 February 2016 15:50</a>
              </span>
            </div>
            <div class="comment-content">This shouldn&#39;t come as a surprise to anyone. Petr Lapukhov had a NANOG presentation about BGP as the DC IGP a few years back - they implemented it at MS Bing. <br /><br />Brocade will be rolling out an IP fabric leveraging BGP (EVPN) in March.<br /><br />Personally, I&#39;d like to see more literature on BGP timer tuning. I think that&#39;s often misunderstood, by me at least. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7711023460231292939">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/03803244374816795623" rel="nofollow">Bhargav</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7711023460231292939" href="#7711023460231292939">09 February 2016 18:28</a>
              </span>
            </div>
            <div class="comment-content">I think the question is if we need to use link-state or distance vector in DC&#39;s today. Link state probably made sense when fabric is asymmetric interms of different ecmps, link-speeds etc. With symmetric fabric where all things equal with leaf-spine arch, does it make sense for a node to know every bit of fabric info or just reach-ability is suffice ?.<br /><br />As far as tooling is concerned, BGP/OSPF/ISIS have been developed organically based on their role in the network, so can we can always achieve what one is not intended to be.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="71127082762953557">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c71127082762953557" href="#71127082762953557">10 February 2016 05:04</a>
              </span>
            </div>
            <div class="comment-content">BGP was considered a WAN technology and slowly got in to DC&#39;s for sometime now.<br /><br />It is starting to penetrate in to servers as well. What are your thoughts on having BGP running from the servers itself ?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8033647838872620394">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://www.routz.nl" rel="nofollow">Alan Wijntje</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8033647838872620394" href="#8033647838872620394">10 February 2016 09:10</a>
              </span>
            </div>
            <div class="comment-content">Good article Ivan,<br /><br />personally I&#39;m a big fan of simple (KISS) and using BGP as the only routing protocol makes sense to me (even in deployments not fitting the &quot;large DC&quot; description in Petr&#39;s draft).<br /><br />Especially if we consider that products on the systems-side (think chassis&#39; running NSX or Azure-stack) support BGP as a means of interconnecting to the fabric (whether injecting host-routes from the chassis to allow for VM-mobility is worth considering is probably another discussion but at least BGP has proven itself to be able to scale to support this).<br /><br />regards,<br /><br />Alan Wijntje </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5124011749312586203">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5124011749312586203" href="#5124011749312586203">19 February 2016 12:03</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan,<br /><br />Due to the limitation of BGP not being able to use unnumbered interfaces we must use tricks. It is an interesting trick using the IPv6 Link local address for peering proposed by Cumulus.<br /><br />My question is: having Nexus switches as spine and leaf, N9K, why we couldnâ€™t use a single SVI on each switch where all the SVIs are in the same subnet and the fabric ports being used as access switchports for the SVI vlan. BGP peerings will be done through the SVI IP addresses and only 1 IPv4 address per device would be used.<br />Why this would not be a suitable solution as a Nexus switch can act as a layer 3 and layer 2 interface on the same port?<br /><br />Thanks.<br /></div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="4258851963486747206">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4258851963486747206" href="#4258851963486747206">19 February 2016 19:29</a>
              </span>
            </div>
            <div class="comment-content">Of course you can do that - it would work on (almost?) any DC switch. However, you&#39;d have to configure different BGP neighbors on different servers.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1680339105197100439">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/08303880674892766910" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1680339105197100439" href="#1680339105197100439">23 February 2016 09:39</a>
              </span>
            </div>
            <div class="comment-content">Hi, is it true that you can&#39;t use unnumbered and BFD simultaneously?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4976184263960601759">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4976184263960601759" href="#4976184263960601759">23 February 2016 09:54</a>
              </span>
            </div>
            <div class="comment-content">While I remember seeing something on BFD-over-MAC, I can&#39;t find a related Internet Draft, and I haven&#39;t seen anyone implementing something along these lines.<br /><br />You _might_ use multihop BFD between loopbacks (again, not sure whether anyone implemented it), but there&#39;s a simpler option used by Cumulus implementation: use BFD between IPv6 LLA.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
