{
  "comments": [
    {
      "date": "10 September 2018 17:22",
      "html": "Quite interesting stuff as usual on your blogs. Albeit I agree with what you write I would qualify it as &quot;don&#39;t use _today&#39;s_ IGPs in this situation&quot;. Let&#39;s see however _what_ one would ideally want from the routing if it stretches to your server (I use the terms loosely, can be kubernetes address, can be real addresses, can be HV flavor). I do think it would be very desirable BTW compared to the &quot;out-of-band&quot; solutions like IBGP-RRs or &quot;leaking of addresses&quot; where each server is a &quot;domain&quot; but ultimately all those domains need to be synchronized by the main routing again so the servers see each other. The more &quot;degress of separation&quot; the more fragile and slow in convergence the solution always is. <br /><br />So I think req&#39;s list reads like this roughly: <br /><br />* contain blast radius, i.e. ideally a server failure shakes only the minimum necessary set of fabric: here IGPs, if one runs /32-routing (which one must often do in case of e.g. mobility and one doesn&#39;t want to run DHCP on each ToR with carefully controlled ranges [own set of problems]) will generate the &#39;one address shakes e&#39;one&#39; problem or &quot;whole fabric blast radius&quot;. So, instead of a flat IGP (where e&#39;one needs replicating the LSDB BTW and one can&#39;t really summarize easily) we could try areas but that causes blackholes (since summaries will generate the problem of area ingress/egress on link failures). One could claim that &quot;servers never fail&quot; but today&#39;s reality of rolling updates on fabrics runs contrary to this assertion. <br />* multi-homing and what I call &quot;true anycast&quot;. One would want multi-homing and  probably want to do it using two addresses. And ideally one would have the option to have even same address on multiple servers (service anycast) independent of ECMP really (i.e. anycast to multiple servers independent of metric)<br />* only default route (weighted) on the servers<br />* northbound metric balancing, i.e. the servers adjusting to failure of &quot;fat links&quot; and generally picking their ToRs with more capacity over less fat pipes. This is a coarse version of flow engineering but given the speed of flow changes and shifts in traffic patterns I am not a big believer in controllers being able to react to it anyway. A &quot;bandwidth broker&quot; on the server kernel is the best solution in a sense but a luxury few will be able to afford. <br />* scale: it is really easy to blow up address space with servers running HVs, VMs and so on so one has to respect that and build a solution that can cope.  <br />* server security: I drop that blackhole for the moment, it will come up ;-)<br /><br />Hastily typed before a meeting, excuse less than perfect English ;-)",
      "id": "898609224270602086",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Tony P",
      "profile": "https://www.blogger.com/profile/07732539097585801151",
      "pub": "2018-09-10T17:22:29.465+02:00",
      "ref": "5567288163004825454",
      "type": "comment"
    }
  ],
  "count": 1,
  "id": "5567288163004825454",
  "type": "post",
  "url": "2013/08/virtual-appliance-routing-network.html"
}