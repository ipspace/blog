{
   "comments": [
      {
         "comments": [
            {
               "date": "12 September 2013 16:16",
               "html": "I can&#39;t tell you how much I love your &quot;rant&quot; and how much I agree with you ;)",
               "id": "681455220860722795",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Ivan Pepelnjak",
               "profile": "https://www.blogger.com/profile/13457151406311272386",
               "pub": "2013-09-12T16:16:51.099+02:00",
               "ref": "6976506994331519960",
               "type": "comment"
            },
            {
               "date": "20 September 2013 18:02",
               "html": "AWESOME rant!",
               "id": "6627076199337741672",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Unknown",
               "profile": "https://www.blogger.com/profile/13721206915366692378",
               "pub": "2013-09-20T18:02:13.764+02:00",
               "ref": "6976506994331519960",
               "type": "comment"
            }
         ],
         "date": "12 September 2013 16:05",
         "html": "Thank you for this post... I agree that complexity is very often glossed over, and holding state is difficult in the network, particularly at scale. <br /><br />(Warning..RANT coming)<br /><br />I also think we impose some old world &#39;networking&#39; because we are comfortable with it. There are some things we should probably just let go of in our networking architectures which would make things much better for us in the network virtualization space. Some of the complexity is currently a self inflicted wound which we accepted because of the technology of yesterdays past (or more gear gets sold that way). <br /><br />Networks are really about deploying applications workloads, and not there just to be networks for networking sake. We all like our networking jobs, but when we forget that networking is a means to an end, we concentrate too much on the means.. and not the ends.<br /><br />You stated... &quot;there always will be tight coupling between all the hypervisors running VMs belonging to the same subnet&quot;. I guess for me it begs the question about how far paravirtualization should go up the stack.  If we have forced the guest instance OS to understand it was on a hypervisor for storage and for network performance, why don&#39;t we force them to understand the network a bit further and assist with the network virtualization process too. If you want to move an application instance, let the guest OS help. Assuming that it must be decoupled from network intelligence is an old world idea.<br /><br />There are many guest OSes which already demand an HVM model so they can have access to specific new compute functionality, why would we not expect to see networking follow in the same model. It breaks the OS virtualization model, but so what.  So do a ton of things these days. Guest agents are doing more and more.<br /><br />The &#39;subnet&#39; coupling is an artificial boundary we&#39;ve imposed because we still live with the sins of legacy networking support at the guest OSes..right? I am sure we will have that &#39;sin&#39; around for many, many years (heck.. OSes have IPv6 support and we all see how well the networking community has embraced that..). However, new kernel networking functionality and application stacks can go a long way to understanding the underlying networking and interact with it in a way that could remove the need for the complexity in the network virtualization market too.  <br /><br />To me, network virtualization needs to understand much more than the &#39;coupling of subnets&#39; to support advanced application workloads efficiently. For some of what it will take to have the network be workload aware, you will have to ask the guest OSes and even applications on them for help.<br /><br />There are lovely things in the networking world now which hold application state and can provide application level control that we&#39;ll use to get us over the hump. We should however start pushing the guest OSes to be smarter..not just the hypervisor and soft switching crowd. Let&#39;s start rants about advancing linux netstack to support the network virtualization functionality for smarter application stacks. We can start getting the world behind the effort and provide models to follow. If we are trying to move the complexity to the edge for control and stability... then move it to the edge.  The near edge approach is a great stepping stone. Our hypervisors and soft switches are great middleware. They don&#39;t comprise the whole application flow however. We should view it as such.<br /><br />Why stop SDN at the network virtualization hurdle.",
         "id": "6976506994331519960",
         "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
         "name": "Unknown",
         "profile": "https://www.blogger.com/profile/04982620456797894645",
         "pub": "2013-09-12T16:05:49.971+02:00",
         "ref": "4245351899037370190",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "12 September 2013 20:56",
               "html": "The Cisco 1000V configuration can get long and tedious and there&#39;s no really good way to keep track of the MAC configurations...  Also you really need to know what you&#39;re doing when you&#39;re configuring a 1000V as it&#39;s not very intuitive.  IMHO that&#39;s the hardest part and I feel it&#39;s only because that&#39;s the part I&#39;ve been doing the least amount of time.  I would like to see VMware and Cisco work together more and improve the 1000V and other things.<br /><br />The answer is always easy when you know it!",
               "id": "2514494184822068976",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Ciscoman",
               "profile": "https://www.blogger.com/profile/15215351197556121612",
               "pub": "2013-09-12T20:56:31.253+02:00",
               "ref": "7978776982543661953",
               "type": "comment"
            }
         ],
         "date": "12 September 2013 20:50",
         "html": "I have been running VRE/VRF on Cisco routers for about 10 years now.  It is not really that hard and you can do some really advanced things with it.  Especially in large DC environments.<br /><br />I think people are trying to make network virtualization too hard and push it into a PC or VMware.  Why not virtualize multiple routers in a single Cisco router?  Why not let Cisco Nexus Switches virtualize switching, and even integrate virtual switches into VMware?<br /><br />Cisco&#39;s &quot;FlexPod&quot; design is now starting to become a &#39;buzz-word&#39; and the sales teams are jumping on with Cisco Nexus but it&#39;s still underutilized IMHO.  What it can do today probably wont be fully realized for another 5+ years.  =[  Virtualization is the future and this is just the beginning!",
         "id": "7978776982543661953",
         "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
         "name": "Ciscoman",
         "profile": "https://www.blogger.com/profile/15215351197556121612",
         "pub": "2013-09-12T20:50:27.224+02:00",
         "ref": "4245351899037370190",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "16 September 2013 08:35",
               "html": "Hi Balaji!<br /><br />If you read this article as NX1KV versus NSX comparison, you totally absolutely missed the point. I just used them as examples illustrating why total decoupling scales better than tight(er) vSwitch-pSwitch coupling. I also used the latest publicly-available documentation or scalability results for both platforms (NVP 3.2 in case of NSX).<br /><br />I&#39;m positive Cisco is working on Nexus 1000V improvements, and once they ship, I&#39;ll be more than happy to write about them, like I wrote about unicast VXLAN and improved scalability (64 ==&gt; 128 hosts). In the meantime, the numbers speak for themselves.<br /><br />Finally, if you happen to be working for one of the vendors mentioned in this blog post, it would be fair to disclose that.<br /><br />Best,<br />Ivan",
               "id": "5345194514500369708",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Ivan Pepelnjak",
               "profile": "https://www.blogger.com/profile/13457151406311272386",
               "pub": "2013-09-16T08:35:51.344+02:00",
               "ref": "5861382880449240167",
               "type": "comment"
            }
         ],
         "date": "15 September 2013 21:30",
         "html": "This article is comparing the Nexus 1000V release GA&#39;ed 4 years ago vs. the NSX release still yet to be released.  We are happy to do a techtalk to go over the state-of-the-art of Nexus 1000V and explains what we have learned in the past four plus years of production deployment in many customers.   Making sure the centralized VSM is not in the way for scale and availability of the network is a key effort we have made a lot of progress over the years.  The architecture of Nexus 1000V is not standing still and we are refreshing the user experience and scalability of the product every year. Having said that, we believe in physical + virtual being managed together and the infrastructure being application aware is the overall right direction to solve real problems that operators face everyday deploying applications.",
         "id": "5861382880449240167",
         "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
         "name": "Balaji",
         "profile": "https://www.blogger.com/profile/01533211595932321121",
         "pub": "2013-09-15T21:30:54.119+02:00",
         "ref": "4245351899037370190",
         "type": "comment"
      },
      {
         "comments": [
            {
               "date": "22 September 2013 09:11",
               "html": "Hi Art!<br /><br />Thanks for the comment. I think you&#39;re underestimating the difference between server and network virtualization - it&#39;s (fundamentally) way easier to implement a system of many isolated components than a system of tightly coupled components. The complexity of one versus the other has nothing to do with how new they are.<br /><br />Also, don&#39;t blame it on Cisco:<br /><br />* VMware wasn&#39;t the only virtualization vendor using VLAN-based virtual networking approach - nobody had a clue. The only ones that had the guts to invent something radically new (and scalable) were the engineers designing Amazon&#39;s VPC.<br /><br />* Networking industry is full of MacGyvers who want to solve everything within the network. It started (at least) with the invention of transparent bridge that &quot;solved&quot; the problem of two broken protocols (LAT and MOP) and continues to these days with all sorts of kludges (LISP and MIP come to mind) that try to bypass brokenness of TCP stack.<br /><br />Did Cisco encourage this behavior? Sure. But so did every other vendor in the networking industry. When was the last time you&#39;ve seen a vendor telling their customer &quot;this is not how it&#39;s done&quot;? It was probably IBM sometime in the &#39;80s.",
               "id": "2461670688388866835",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Ivan Pepelnjak",
               "profile": "https://www.blogger.com/profile/13457151406311272386",
               "pub": "2013-09-22T09:11:21.506+02:00",
               "ref": "4665117972226116604",
               "type": "comment"
            },
            {
               "date": "23 September 2013 16:12",
               "html": "All very valid points Ivan and I cant disagree with you. But at the same time industry politics have and will always impact technological progression, both for good and bad. Much more than I have blame for any specific vendor, I think market forces dictate any vendors behavior in many ways that are fairly predictable, so I think marketplace diversity will have a far greater impact on a healthy market than the lack or presence of any specific brand ... they all hire from the same pool :) And hopefully we will see a much more diverse market emerge :)",
               "id": "3011362198470919139",
               "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
               "name": "Unknown",
               "profile": "https://www.blogger.com/profile/13721206915366692378",
               "pub": "2013-09-23T16:12:39.905+02:00",
               "ref": "4665117972226116604",
               "type": "comment"
            }
         ],
         "date": "20 September 2013 23:23",
         "html": "Great post as always Ivan. I tend to think feelings about network virtualization being hard largely come from it being new. Today we have to manage both old and new but wont be long before we are primarily managing new (dont need DLSW+ much anymore). All new innovations need time to become well understood, and once well understood I think network virtualization will be seen as a simplifying technology. Not that the end result is necessarily simpler, but if you consider the desired attributes of the future network those can be accomplished more simply using this abstraction. So its too simplistic to say its more simple or more complex, but it will be providing more value added functions.  <br /><br />I think a good related question is why it is new - the vswitch has been around for over a decade, why are we finally doing something with it now? And part of that may have to do with newer technologies becoming available, but it is mostly because, imho,  VMware didnt want to piss off Cisco, and Cisco had to try to figure out a way to do it without disrupting or  their margin machine and preserving the value-added features they wanted their hardware to be providing - thus a decade after virtualization became popular we are just now finally figuring out the network problems that have been present for virtualized environments from the very beginning. <br /><br />Not that they didnt try, SONA and/or modern switch API&#39;s was probably a very fitting thing for networks circa 2005, but it was plagued, imho, because of Cisco&#39;s desire to make everything proprietary and thus inability to build a meaningful ecosystem. That and a completely network-centric view of technology that tried to position the network as the answer to all problems. Then the more recent attempts have been aimed at locking down vm&#39;s to specific hardware which Cisco loves but VMware has been against. Glad that today progress is finally being made.   ",
         "id": "4665117972226116604",
         "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
         "name": "Unknown",
         "profile": "https://www.blogger.com/profile/13721206915366692378",
         "pub": "2013-09-20T23:23:23.362+02:00",
         "ref": "4245351899037370190",
         "type": "comment"
      }
   ],
   "count": 4,
   "id": "4245351899037370190",
   "type": "post",
   "url": "2013/09/why-is-network-virtualization-so-hard.html"
}
