comments:
- date: 18 April 2013 14:29
  html: Will be intersting to see the scalability numbers of the multicast-less/control-planed
    VXLAN (http://blogs.cisco.com/datacenter/cisco-vxlan-innovations-overcoming-ip-multicast-challenges/).<br
    />
  id: '1960494173117879327'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-04-18T14:29:07.677+02:00'
  ref: '7488918989472478682'
  type: comment
- comments:
  - date: 18 April 2013 17:51
    html: Hi!<br /><br />v2 of this post - I&#39;m anxiously waiting to see what Cisco
      did for non-multicast VXLAN ;)<br /><br />Larger-scale vMotion - while vMotion
      outside of a DRS cluster is not automatic (you have to trigger it manually),
      people use it for coarse-grained resource allocation (if a cluster becomes overloaded,
      it&#39;s pretty easy to move a whole app stack somewhere else) or prior to large-scale
      maintenance activities ... and then there&#39;s the long-distance unicorn-riding
      variant ;)<br /><br />As for &quot;what most customers need&quot; - 80+% of
      them are probably fine with a single cluster or two. That&#39;s 60 servers;
      if you buy high-end gear, you could pack few thousand VMs onto them. More than
      enough in many cases, unless you&#39;re going down the full-VDI route.<br /><br
      />Multiple NX1KV instances per DC is obviously a good idea, but keep in mind
      that<br /><br />A) you cannot vMotion a running VM across them;<br />B) Configuration
      changes made in one vDS are not propagated to another one, so you need an automation
      layer on top (could be of the cut-and-paste variant :D ).<br /><br />Ivan
    id: '4541038643683295148'
    image: https://lh3.googleusercontent.com/-ZIhwz6bLuK0/AAAAAAAAAAI/AAAAAAAAFtg/mLtCQ3p4_0E/s32-c/photo.jpg
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2013-04-18T17:51:08.092+02:00'
    ref: '8184372840172372167'
    type: comment
  date: 18 April 2013 14:30
  html: I look forward to v2 of this post in a few months when multicast isn&#39;t
    being used and packet replication is being performed somewhere else - vswitch
    or node in control cluster, etc.<br /><br />Many customers limit vMotion to 32
    physical hosts since that is the largest size of a cluster.  It has been validated
    you can vMotion between clusters under a given set of conditions, but do [large]
    customers do this?   I wonder myself about the holy grail.  What&#39;s your take?  What
    are you seeing?<br /><br />As Cisco continues to increase scale for the 1000V
    to catch up to VMware, is it needed for *most* customers?  Rather if vMotion is
    contained to cluster size, would it not be advantageous to maintain 2, 3, or even
    4 VSMs on the 1000V to reduce single points of failure for the virtual network?  Once
    could argue either way, but what is your take?<br /><br />Thanks,<br />Jason (@jedelman8)<br
    /><br />
  id: '8184372840172372167'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jason Edelman (@jedelman8)
  profile: http://jedelman.com
  pub: '2013-04-18T14:30:21.367+02:00'
  ref: '7488918989472478682'
  type: comment
- date: 19 April 2013 15:36
  html: What did you say about manual? :)  Is that the holy grail?  For a customer
    of that size, I&#39;d imagine they can write some scripts using the vSphere SDK/APIs.<br
    /><br />With talking to Cisco a few weeks back, they were recommending to use
    DCNM actually and a Master VSM interestingly enough.  Per them, &quot;Create Master
    VSM with all the needed profiles and network configuration. Use the running config
    to create exact config across all other Nexus 1000V VSMs. Changes made to master
    VSM can be replicated to all other VSMs.&quot;  <br /><br />Replicated might mean
    manual scripting as you call the cut-and-paste variant without DCNM.<br /><br
    />-Jason (@jedelman8)
  id: '4850796922078959672'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jason Edelman (@jedelman8)
  profile: http://jedelman.com
  pub: '2013-04-19T15:36:34.730+02:00'
  ref: '7488918989472478682'
  type: comment
- date: 20 April 2013 21:59
  html: vShield (vCloud Network and Security) Manager can configure VXLAN not only
    on vSphere Distributed Switch but also on Nexus 1000V thanks to Network Segmentation
    Manager API.<br />Besides 60 host Nexus 1000V limitation, the other painful limit
    is 2048 ports per switch which means less than 34 VMs per host - hardly cloud
    scale.<br />vMotion across distributed virtual switches is (currently) not possible
    but you also need shared storage which you most likely will not have across many
    clusters. That means you can deploy VM (but not migrate) anywhere in the datacenter.
  id: '5126239924939449749'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-04-20T21:59:10.441+02:00'
  ref: '7488918989472478682'
  type: comment
- date: 18 May 2013 01:57
  html: '&gt;That means you can deploy VM (but not migrate) anywhere in the datacenter.<br
    /><br />Which I believe is an even more interesting use case for large customer
    with scattered free resources here and there but very unflexible vlan assignments
    to those islands (&quot;oh that cluster is pretty idle why don&#39;t you deploy
    your new VM there?&quot;, &quot;yeah but the VLAN that I need is only available
    in this overloaded cluster&quot;)-ish. <br /><br />Massimo. <br /><br />'
  id: '509932930510072435'
  image: https://resources.blogblog.com/img/blank.gif
  name: Massimo Re Ferre'
  profile: http://it20.info
  pub: '2013-05-18T01:57:04.149+02:00'
  ref: '7488918989472478682'
  type: comment
- date: 09 July 2013 15:12
  html: If they do not have the vlans located &quot;everywhere&quot; - how are end
    users going to get to them ?<br />
  id: '7289786236071110217'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-07-09T15:12:46.684+02:00'
  ref: '7488918989472478682'
  type: comment
count: 7
id: '7488918989472478682'
type: post
url: 2013/04/vxlan-scalability-challenges.html
