{
  "comments": [
    {
      "date": "30 January 2013 16:42",
      "html": "This is a common problem in world of IT today. The people have less or no more knowledge and trust in people who think they have knowledge. ;) With a well implemented VMware SRM solution you could solve the problem very comfortable. SRM is even able to run scripts e.g. to change a switch configuration or it waits for an user interaction before continuing the desaster recovery process. You don&#39;t need L2 links and VMotion. Replicate the storage, build and test an SRM Action Plan. That&#39;s it&#39;s... Why make life complicate? ;) ",
      "id": "8932426604167096082",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Tschokko",
      "profile": "http://www.tschokko.de",
      "pub": "2013-01-30T16:42:19.813+01:00",
      "ref": "7960341891125018904",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "31 January 2013 10:18",
          "html": "My VMware expert told me, that changing the IP address of a VM is not neccessary. The action plan can run scripts that prepares the infrastructure on the backup site and it&#39;s able to wait for user interaction. I think it&#39;s good choice, because you can optimize and speed up the DR process and you have enough room for manual interaction, to ensure a proper site move. A time window of 4 hours is comfortable, but if you run into trouble maybe it&#39;s not enough. That&#39;s why I recommend to implement as much automation as possible. ",
          "id": "33363110046831456",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Tschokko",
          "profile": "http://www.tschokko.de",
          "pub": "2013-01-31T10:18:29.371+01:00",
          "ref": "7615136461584204365",
          "type": "comment"
        }
      ],
      "date": "30 January 2013 19:16",
      "html": "SRM is nice, but ... try to imagine system which consists of database server and app server. This is standard. But in next step try to imagine that DB server ip address is hardcoded into apllication source code. This is reason why sys admins don&#39;t want even think about changing IP even with SRM. <br />In such situation - I think that only way is not to focuse on automatic switching but on DR manual procedures (like Ivan described). The possibility of DC failure is low. Such situation don&#39;t happen often (like single device failure), so you can prepare clear procedures of manual switching to second DC. <br />Also if you have to have L2 connectivity and can&#39;t convice PM or sysadmins to user L3 or manual procedures - please read this document http://www.juniper.net/us/en/local/pdf/implementation-guides/8010050-en.pdf Especially please focus on Figure 7. For me it&#39;s one and only way to implement L3 connectivity for streched L2 DC. <br />To be clear as network engineer I prefer clear L3 connectivity :)",
      "id": "7615136461584204365",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "PeterC",
      "profile": null,
      "pub": "2013-01-30T19:16:23.578+01:00",
      "ref": "7960341891125018904",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "31 January 2013 17:24",
          "html": "Bridging over dark fiber, I think. Definitely not OTV.",
          "id": "9048789272875311390",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2013-01-31T17:24:04.046+01:00",
          "ref": "1820045212382162297",
          "type": "comment"
        }
      ],
      "date": "31 January 2013 00:09",
      "html": "Not that it matters to the point you&#39;re making, but can you comment on how they implemented L2 between DCs? OTV, VPLS, EoMPLS, dark, etc.",
      "id": "1820045212382162297",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Jason Edelman (@jedelman8)",
      "profile": "http://jedelman.com",
      "pub": "2013-01-31T00:09:35.061+01:00",
      "ref": "7960341891125018904",
      "type": "comment"
    },
    {
      "date": "01 February 2013 01:49",
      "html": "Awesome post Ivan.  This is something that could possibly be optimized with network virtualization and edge gateway nodes that speak routing protocols.",
      "id": "5233944321845275448",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Brad Hedlund",
      "profile": "http://bradhedlund.com",
      "pub": "2013-02-01T01:49:10.948+01:00",
      "ref": "7960341891125018904",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "02 February 2013 08:41",
          "html": "While distances do matter (doing bridging across Atlantic clearly doesn&#39;t make sense), the most important fact remains that a L2 broadcast domain represents a single failure domain. <br /><br />If you think you need more than one failure domain (or availability zone), then you should be very careful with bridging (it usually makes sense to have more than one L2 domain within a single facility).<br /><br />",
          "id": "6888090407091686697",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2013-02-02T08:41:57.040+01:00",
          "ref": "322336426057960755",
          "type": "comment"
        }
      ],
      "date": "01 February 2013 22:09",
      "html": "How exactly is &quot;Long Distance&quot; defined?<br /><br />Say you have a primary data center and a secondary data center a mile apart with plenty of dark fiber between the two.  <br /><br />It would be great if we would do a layer 3 interconnect, but is a layer 2 DCI still a bad idea here?<br /><br />When exactly does a layer 2 DCI stop being acceptable?",
      "id": "322336426057960755",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Justin A",
      "profile": "https://www.blogger.com/profile/07567730572096907480",
      "pub": "2013-02-01T22:09:14.091+01:00",
      "ref": "7960341891125018904",
      "type": "comment"
    },
    {
      "date": "09 February 2013 16:13",
      "html": "Good post.  Best to get the facts of what a customer requires, facts of the apps and infrastructure available to them  and let those dictate the solution.  We find that in a lot of cases a stretched HA infrastructure brings more complexity than what the business requirements dictate, but the reverse is also true attempting to meet some requirements by the business with SRM is more complex or incompatible (cannot use SRM for vCD) and increases operational effort. In these cases, best to lay out all the caveats and risk and working with the customer choose the appropriate solution that meets the needs with the least amount of risk.  One size fits all does not exist.   <br />",
      "id": "42914845545101710",
      "image": "https://4.bp.blogspot.com/-ono3tqcScxc/XDT-aJ09gRI/AAAAAAAAVYQ/vCHtsvBOXboWVIDZ3-w1Lft8xJuMTE70wCK4BGAYYCw/s32/E8CB1A5F-B5F4-4A39-90BC-B523C328F617.jpeg",
      "name": "Rick Brunner",
      "profile": "https://www.blogger.com/profile/01460920754539813912",
      "pub": "2013-02-09T16:13:18.599+01:00",
      "ref": "7960341891125018904",
      "type": "comment"
    },
    {
      "date": "13 September 2014 13:23",
      "html": "Excellent post. I&#39;m facing these questions very often because I work for vendor having servers, storages, and networking. I&#39;m long time vmware expert guy but with networking and storage overlap. I&#39;m always trying to explain difference among local-HA, geo-HA, DR and concept of failure domains (availability zones) to my customers when delivering vSphere designs. And I can tell you it is hard, really hard. IT staff are siloed and influenced by marketing teams of particular vendors. It is pretty difficult to explain them all these cross domain topics and disprove some product marketing &quot;simplifications&quot;. On top of that they don&#39;t have right requirements and SLAs from business. I personally also prefer good and fast enough DR against stretched metro clusters (geo-HA). You can test it and really know deterministically what you will get in case of disaster. I believe it can cover 95% of business workload requirements. However if there are real geo-HA requirements it seems to me that it can be designed correctly nowadays. But you have to use the newest technologies and products (this is the first risk) and you have to carefully consider all possible failure scenarios. Even you use CISCO OTV or similar technology for L2 network overlay over L3, even you use geo distributed storage like pair of EMC VPLEX, NetApp MetroCluster, HP 3PAR stretched volume, IBM SVC/Storwise, DELL Compellent LiveVolume, etc. which appears as single storage distributed across datacenters and ensures optimal DCI usage, even you have third site used as split brain arbiter allowing  automatically failover geo distributed storage nodes ... vSphere stretched HA cluster and Distributed Storage will be single failure zones. It is not bug. It is a feature by design!!! If you need it and want it go for it. If you prefer split available zones, don&#39;t do it. Period. Just my $0.02 and your milage may vary.",
      "id": "2737813305422821478",
      "image": "https://2.bp.blogspot.com/-B9vwV5BBX6A/U0znzF-3RuI/AAAAAAAABBg/30SyQxU7QCY/s32/*",
      "name": "David Pasek",
      "profile": "https://www.blogger.com/profile/12894499511798467969",
      "pub": "2014-09-13T13:23:42.666+02:00",
      "ref": "7960341891125018904",
      "type": "comment"
    }
  ],
  "count": 10,
  "id": "7960341891125018904",
  "type": "post",
  "url": "2013/01/long-distance-vmotion-stretched-ha.html"
}