<div class="comments post" id="comments">
  <h4>7 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="3710790741178927515">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/07567730572096907480" rel="nofollow">Justin A</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3710790741178927515" href="#3710790741178927515">21 June 2013 14:36</a>
              </span>
            </div>
            <div class="comment-content">only issue I see (if you can even call it that) would be the address space pollution needed by all the ToR switches.<br /><br />Careful planning would be needed to ensure that the first/last 10-20 addresses of any subnet are reserved for the &#39;real&#39; ip addresses used by the ToR switches.<br /><br />I wonder if this could be modified somehow to let you simply do<br /><br />    ip virtual-router address 10.10.20.1/24<br /><br />and not have to individually address each ToR switch.<br /><br />Obviously the switch would lose the ability to source packets from that interface, but I&#39;m not sure what else would break.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="8610840472787841457">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8610840472787841457" href="#8610840472787841457">23 June 2013 07:47</a>
              </span>
            </div>
            <div class="comment-content">Good one. Totally missed this aspect. Thank you!<br /><br />Oh, and do I have to mention that this problem will go away with IPv6 ;)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7970615856175193180">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/06913023963950222205" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7970615856175193180" href="#7970615856175193180">23 July 2013 19:34</a>
              </span>
            </div>
            <div class="comment-content">I love the idea of distributed L3 switching and the complete convergence of L2 and L3 domains. Short of the single caveat that Justin points out, VARP seems to deliver functionality very similar to QFabric. In addition to the address space required of the real addresses, it just seems to increase the complexity of the implementation. If only ICMP could be sourced from the VARP address, then perhaps the real addresses would be unnecessary.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2065063386244371441">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2065063386244371441" href="#2065063386244371441">13 March 2014 13:20</a>
              </span>
            </div>
            <div class="comment-content">Just wondering how much VAR you implemented in real production. I have seen some strange things, VIP&#39;s not responding, routing going crazy. I moved the configuration back to VRRP....</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1927619754999295788">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1927619754999295788" href="#1927619754999295788">22 May 2014 18:14</a>
              </span>
            </div>
            <div class="comment-content">I agree with Anonymous, I have the issue where VIP gateways don&#39;t respond but pass traffic, and with OSPF plus MLAG see issues where routing + varp don&#39;t work correctly.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="893892318829383279">
          <!--
          <div class="avatar-image-container">
            <img src="https://3.bp.blogspot.com/-s-HwC2PjFmY/U8gH8BAwMEI/AAAAAAAAARU/p0ApyeZKmGo/s32/631446%253Fs%253D140">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/00620043019707946789" rel="nofollow">Vincent Bernat</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c893892318829383279" href="#893892318829383279">17 July 2014 14:45</a>
              </span>
            </div>
            <div class="comment-content">Let&#39;s assume that the virtual router has two switches SW1 and SW2 with VARP configured. SW3 is connected to both SW1 and SW2 and currently learned the virtual MAC on SW1&#39;s port. Now, if the link between SW1 and SW3 is down, SW3 won&#39;t know where the virtual MAC is. It will flood the packets to all links until the next GARP request. SW2 has no way to know that a link on SW1 failed and therefore cannot trigger a GARP request. Right?<br /><br />Unless VARP only works in the context of MC-LAG. In this case, when SW1 detects the downlink, it generates GARP requests that will be flooded to SW2 then back to SW3 which will now use SW2&#39;s port for the virtual MAC.</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5981241141777083866">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5981241141777083866" href="#5981241141777083866">14 July 2018 04:48</a>
              </span>
            </div>
            <div class="comment-content">That is correct, SW2 will never know that the link on SW1 has failed. But if you don&#39;t want flooding to happen, worst case for 30 sec (default GARP time), you can tighten the GARP timer setting, say to 10 sec. But that has the tradeoff of wasting CPU cycles of 3X of that default GARP timer for the nodes that are processing the GARP frames.   </div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
