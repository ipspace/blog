<div class="comments post" id="comments">
  <h4>7 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="5402278717791399581">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/05378426663611667686" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5402278717791399581" href="#5402278717791399581">21 March 2013 09:30</a>
              </span>
            </div>
            <div class="comment-content">Ivan,<br /><br />I think you&#39;re looking for the following:<br /><br />http://www.research.ibm.com/haifa/satran/ips/PaloAlto-MarkBakke-crc-recovery.pdf<br /><br />Cheers :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5049887534015524096">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/02571029118821999072" rel="nofollow">Jay Swan</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5049887534015524096" href="#5049887534015524096">21 March 2013 17:12</a>
              </span>
            </div>
            <div class="comment-content">We run iSCSI this way simply because it&#39;s what the SAN vendor design guide specifies. Storage vendors love to blame the network; it&#39;s one less thing they can complain about.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1602440197879163516">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">MikeH</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1602440197879163516" href="#1602440197879163516">21 March 2013 20:23</a>
              </span>
            </div>
            <div class="comment-content">I have never seen the benefit of running iSCSI this way.  It only made sense to me if you were running lower speed switches that were available when iSCSI was first popularized (i.e.3750).  When you have fast switches (Nexus, Brocade, etc.) why not collapse the storage distribution A&amp;B sides and mix it in with the front-side transport?  802.1Qbb is supposed to allow you to mix lossy and lossless transport together, the MTBF on network gear is just has high as storage in what I have seen (1 each in 5 years) and both events ended up as nothing-burgers due to redundancy.  I still don&#39;t see the need for ethernet AB separation from data let alone separation from each side.  I think the temptation to put in lesser gear when you have built that much &quot;redundancy&quot; is just too tempting for management.  I think that will result in more failures than just putting in big, bad monster switches and calling it a day.<br /><br />Now to be fair, you work with much larger DCs than I do so this might just be a scale thing.  <br /><br /></div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8970192206339100483">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="http://abnerg.tumblr.com" rel="nofollow">Abnerg</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8970192206339100483" href="#8970192206339100483">22 March 2013 02:02</a>
              </span>
            </div>
            <div class="comment-content">I interviewed a few people with this set up a while back (4yrs?) and the answers were:<br />- We know Ethernet better than we know FC, so if we build an iSCSI SAN we don&#39;t have to hire FC people.<br />- We know Ethernet can handle iSCSI and we know it can handle our front end traffic, but we&#39;re not sure it can do both at the same time.<br />- If we keep it separate, the storage vendor can&#39;t blame the network (as jswan mentioned)<br />- I don&#39;t recall many specific technical reasons for the decision, most were political or being conservative.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8928116839259218135">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8928116839259218135" href="#8928116839259218135">11 July 2013 15:26</a>
              </span>
            </div>
            <div class="comment-content">While the alleged &quot;FC-BB-5 violation&quot; does indeed give you two independent paths from the server into the ToR FCoE switch, it hardly provide &quot;A/B separation&quot;. For that, you need an actual air-gap between two completely independent fabric.<br /><br />In your own words: &quot;A single STP loop (or any other loop-generating bug, including some past vPC bugs) can bring down the whole layer-2 domain ... including both server-to-storage paths.&quot;, and I couldn&#39;t agree more. That&#39;s why my stance on FCoE is that it&#39;s great but if you really want (more like need) the HA required for mission critical applications you better break it out into native FC to go to separate FC A/B fabrics as soon as possible (first hop). Remember, logical A/B separation has been possible in native FC environments for many years (with VSANs and Virtual Fabrics), yet no mission critical FC storage environment is built on this logical isolation, and there&#39;s a good reason for that.<br /><br />Same thing goes for iSCSI. If you really, *really* want to build a truly HA, mission-critical iSCSI SAN then you need two dedicated iSCSI networks (and not VLANs on the same network). Remember, the proverbial substance *will* hit the rotating blades one day...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6022098808457797092">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14508392154453584343" rel="nofollow">garegin</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6022098808457797092" href="#6022098808457797092">21 December 2013 06:11</a>
              </span>
            </div>
            <div class="comment-content"> &quot;A SCSI failure arriving at just the right time could easily result in the famous colored screen&quot;.<br />Kernel panics only happen if the root filesystem is on the SAN. Meaning that the corruption has to be to the system files that are running the OS. And even then only the components running in kernel space and some important user space components like the service manager can crash the OS. Other system files are just user processes. <br /> The OS in most deployments is connected through a DAS.  Diskless thinclients connected to SANs are not really popular because HDDs are dirt cheap and engineering a SAN where hundreds of PCs pull their OS data through a fabric would cost sick amounts in dollars. </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="77472439661506741">
          <!--
          <div class="avatar-image-container">
            <img src="https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14508392154453584343" rel="nofollow">garegin</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c77472439661506741" href="#77472439661506741">21 December 2013 06:34</a>
              </span>
            </div>
            <div class="comment-content">&quot;Remember, the proverbial substance *will* hit the rotating blades one day...&quot;<br />I mean, who cares. If you are running Linux and to a lesser degree, Windows, you probably are going to get burned by cascading bad updates much sooner than a L2 meltdown. <br />Don&#39;t people remember when Dreamhost went down for days because of a bad Debian update.  <br />Unless you are running some hardened OS like CapROS or VOS you are looking at a software caused meltdown to be as realistic as catching cold in the flu season.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
