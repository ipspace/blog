comments:
- date: 27 March 2013 08:26
  html: Well written Ivan,<br /><br />If we leave too much to automating who is going
    to control that the configuration is working as expected?<br /><br />I know some
    people that provision all VLANs at once. It&#39;s easy to script. Negative side
    is number of STP processes if you run RPVST+ but if you run MST it&#39;s not an
    issue.
  id: '8763276034706893513'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-03-27T08:26:08.826+01:00'
  ref: '348997122779517340'
  type: comment
- date: 27 March 2013 09:15
  html: The marketing of SDN and automated DC added a very bad light on networks and
    network engineers. How many IT guys can say that they understand fully what SDN
    is all about? <br /><br />The image which everybody sees is that with SDN (for
    example) or other kind of automatic handling of network resources, is that we
    get rid of those old dinosaurs called network engineers. <br /><br />Next step,
    everybody wants automatic deployment of everything, but when some people encounter
    a problem (ex. network) they rush back to the dinosaurs for help, pointing again
    that network is an issue. The fact that everybody wants control over network resources
    without understanding the technical background, well, that&#39;s not an issue.<br
    /><br />I see SDN as an innovative technology, but I don&#39;t see it as the magic
    pill which will replace knowledge and experience. <br /><br />I don&#39;t want
    to offend anybody, but I meet people working with VMware products which had no
    idea how the product works actually.<br />It&#39;s not VMware&#39;s fault, don&#39;t
    get me wrong.<br /> <br />He was explaining a VM machine like you click here and
    then click there...ok, ok but what&#39;s going on in the background, how is the
    vSwitch communicate with physical network for example? Silence.<br /><br />This
    is the direction in which we want to go? Click here and there? I understand that
    we can do now more with less brain usage than 20 years ago, but this is only because
    there are &quot;dinosaurs&quot; which consider reading, learning and using brain
    for more than day to day activities. <br /><br />Don&#39;t worry, if things are
    going on this path and nobody understand, by everybody uses terms like SDN to
    hide real problems, in another 20 years we can click here and click there to eliminate
    the last IT &quot;dinosaurs&quot;.
  id: '1295352268310098972'
  image: https://resources.blogblog.com/img/blank.gif
  name: Calin C.
  profile: http://firstdigest.com
  pub: '2013-03-27T09:15:16.140+01:00'
  ref: '348997122779517340'
  type: comment
- comments:
  - date: 30 March 2013 16:31
    html: Very much agree with this.  The solution is Network Virtualization.
    id: '8736251355078596068'
    image: https://resources.blogblog.com/img/blank.gif
    name: Brad Hedlund
    profile: http://bradhedlund.com
    pub: '2013-03-30T16:31:49.375+01:00'
    ref: '4153456360539584786'
    type: comment
  - date: 10 January 2014 04:14
    html: Not necessarily network virtualization. What you mention is possible today
      with Q-in-Q. Network team handles the transport layer while the server guys
      can transport any number of VLANs over it.<br /><br />This doesn&#39;t (of course)
      prevent a server guy from borking a vswitch and complaining to the network guys.
      THAT&#39;S where SDN comes in to play. SDN should allow the network to be provisioned
      dynamically and automagically at that lower level from the Ethernet transport
      infrastructure. Ideally, SDN would allow a client to send a tagged frame (with
      some form of handshake I would presume) and the SDN faeries provision the access
      ports and ensure any trunk ports which connect to a switch with the same VLAN
      in use are configured to allow it.<br /><br />Of course both of those still
      rely on some form of STP which is a waste. If we&#39;re redefining the DC infrastructure,
      surely we can &quot;flatten&quot; it out a bit.
    id: '1675467959536846598'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Steve
    profile: https://www.blogger.com/profile/14350091502008101769
    pub: '2014-01-10T04:14:33.366+01:00'
    ref: '4153456360539584786'
    type: comment
  date: 27 March 2013 09:43
  html: 'Oh, boy. One more reason for me to write the damned blog post. :) Anyway,
    short summary: physical network should continue to be configured/managed by the
    networking team, but instead of VLANs that provide connectivity to VMs, they should
    provide &quot;transport&quot; connectivity to vSwitches (running in hosts and
    ToRs) for their virtualised networking overlays.<br /><br />Then server/virt team
    would configure/reconfigure vSwitches via SDN/whatever, while the transport network
    stays stable and secure. Everybody wins - networking team doesn&#39;t have to
    deal with high volumes of moves/adds/changes; no weird-ass protocols to track
    VMs; and server guys can do whatever they want without endangering the whole shabang.<br
    />'
  id: '4153456360539584786'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Dmitri Kalintsev
  profile: https://www.blogger.com/profile/16576726865924052243
  pub: '2013-03-27T09:43:47.894+01:00'
  ref: '348997122779517340'
  type: comment
- comments:
  - date: 30 March 2013 19:45
    html: 'Re: Puppet for Junos.  Point of clarification - it does not run as daemon
      process by default. So each Puppet run is independent on memory usage.  Plus
      version 1.0 was just released that will enable you to tune the memory usage
      if needed.<br />'
    id: '4004417560757303092'
    image: https://resources.blogblog.com/img/blank.gif
    name: Jeremy Schulman
    profile: https://twitter.com/nwkautomaniac
    pub: '2013-03-30T19:45:42.743+01:00'
    ref: '5315637993053906016'
    type: comment
  date: 27 March 2013 09:49
  html: 'For JUNOS, you can use Junoscript/NETCONF. There&#39;s even a Java toolkit
    for this: http://www.juniper.net/techpubs/en_US/junos12.3/information-products/pathway-pages/netconf-java-toolkit/netconf-java-toolkit.html
    and it&#39;s available in Perl *somewhere* as well. You can ofcourse write your
    own implementation in any programming language of your choice.<br /><br />Talking
    about Puppet: Recently, Juniper also launched Puppet for JUNOS: http://www.juniper.net/techpubs/en_US/junos-puppet0.8/topics/concept/automation-junos-puppet-overview.html
    . But it requires you to install a UNIX-like daemon on the machine, which comes
    &quot;as-is and without any warranty&quot; so that basically means nobody sensible
    will install it (hello memory leaks in there!)...'
  id: '5315637993053906016'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-03-27T09:49:32.054+01:00'
  ref: '348997122779517340'
  type: comment
- date: 27 March 2013 13:01
  html: 'User LANs: Use MAB or a full 802.1x solution (ISE works quite well)<br /><br
    />Data Center: If Cisco, Provisioning and unpruning new VLANs going to pruned
    VMware trunks is quite easy with port-profiles and your configuration management
    tool of choice. Never touch a port again, just update the VLAN DB, MST Region,
    and port-profile. Not a hassle at all.'
  id: '210360237571954235'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/09921719270863233800
  pub: '2013-03-27T13:01:58.465+01:00'
  ref: '348997122779517340'
  type: comment
- date: 27 March 2013 13:57
  html: As you say - we only have 10s of Vlans.  Maybe up to 200 in a large data center.  PuttyCM
    with credentials makes it easy and a non-issue.  When a new Vlan is provisioned
    - I just populate a container with the required switches to be updated per vm
    cluster.  Takes 2 to 5 minutes to add the vlans to ~30 switches.  It would probably
    take longer using an automated process.
  id: '624123781956372186'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-03-27T13:57:28.915+01:00'
  ref: '348997122779517340'
  type: comment
- date: 27 March 2013 18:05
  html: In the future Data Center and maybe even Campus, VLANs likely go away.  VXLAN
    is usually talked about just for increased multi-tenancy, but the bottom line
    is VXLANs are easier to provision b/c they are already integrated into Cloud Mgmt
    Platforms.  So if your virtual switch (two very popular ones) is supported by
    the mgmt platform, you&#39;re good to go.  Can we say this about physical switches?  Plus,
    no need to worry about all the intermediary devices and fat fingering anyway that
    could bring down the DC.  <br /><br />For physical devices, in large data centers,
    puppet seems to make sense for VLAN configs if there are lots of devices always
    being added/removed.  For the Enterprise, I&#39;m not sure yet, but the CLI and
    single-device mgmt isn&#39;t the way forward.<br />
  id: '5199928124958463003'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jason Edelman (@jedelman8)
  profile: http://jedelman.com
  pub: '2013-03-27T18:05:11.837+01:00'
  ref: '348997122779517340'
  type: comment
- date: 27 March 2013 21:01
  html: I believe one of the reasons that configuration operations, and particularly
    VLAN configuration are not automated is because there are no checks and balances
    that the configuration operation does the right thing for the hosts to talk.<br
    /><br />Its a hard problem for the network alone to solve as it does not typically
    know which nodes are meant to communicate, so we don&#39;t know if changing a
    VLAN is right or not, so a person does it. <br /><br />We need a way to tell us
    at the application level, which nodes are meant to talk, and then an automated
    way to deterministically verify that communication is valid before changes are
    made to device configurations.
  id: '3626833686320604080'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/09355536603272133241
  pub: '2013-03-27T21:01:14.737+01:00'
  ref: '348997122779517340'
  type: comment
- date: 27 March 2013 22:02
  html: If everybody thought like this, we still be using SDLC/SNA - hey ! That was
    stable ! It worked ! Everyone else was doing it ! Nobody got fired for buying
    .......<br /><br />The problem of course was cost. As soon as the first major
    bank moved to TCP/IP and IPX Routers (and it worked), all the other banks were
    at a competitive disadvantage. So in the 90&#39;s everybody moved from a stable,
    high cost network to a less stable lower cost network - because it worked most
    of the time and costed a hell of a lot less. (Let&#39;s not forget that it was
    mostly networking that broke the back of what was an expensive, proprietary and
    arrogant vendor).<br /><br />Today the network is a bigger problem than it was
    back then.<br /><br />It&#39;s highly UNSTABLE (just ask anyone who has to do
    a IOS or NX-OS bug scrub)<br />It has a high capital cost<br /><br />And to your
    point Ivan, it has a very high OPEX. Whilst Network Engineers are the greatest
    guys in IT (by a mile), you guys are much slower to respond than a computer running
    a program. You guys sleep, you take lunch, you drive a car and you sleep.<br /><br
    />You can call it SDN or whatever. But what we are talking about is automation.
    That is the revolution that is coming. And Network Guys can either understand
    that and embrace it.<br /><br />Or as Calin intimated, in a few years the Human
    Resources Department will be &quot;mouse clicking&quot; you out of the building
  id: '5629794060843728961'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-03-27T22:02:43.905+01:00'
  ref: '348997122779517340'
  type: comment
- comments:
  - date: 04 April 2013 05:22
    html: Hopefully it&#39;ll be as funny as this http://workflowsherpas.com/2013/04/01/asshole-hipsters
      :)
    id: '6886198950239322158'
    image: https://resources.blogblog.com/img/blank.gif
    name: Anonymous
    profile: null
    pub: '2013-04-04T05:22:52.861+02:00'
    ref: '1263184791121148288'
    type: comment
  date: 27 March 2013 23:27
  html: 'Ivan, I am working on a blog post response in parts to this...  '
  id: '1263184791121148288'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-03-27T23:27:50.434+01:00'
  ref: '348997122779517340'
  type: comment
- date: 28 March 2013 00:08
  html: For large scale multi-tenant cloud infrastructures, we simply provision an
    allocated number of VLANs for customer networks.  Your management networks, vmotion
    networks, etc. will take up a handful of VLANs.  Then we allocate a large number
    of VLANs for customer networks and allow those on the trunks to the hosts where
    those customers could reside (for example, we may allocate VLANs 2000 through
    2999 as tenant networks).  At that point, you then have to provision your VLANs
    on your vNICs but you can write an easy script to take care of that.<br /><br
    />You end up having to manage VLANs and when you consider that you might one day
    have a need for VLANs to span multiple data centers, you need to reserve a set
    of VLANs for that purpose.  I know how adamant you are against that and frankly,
    I agree that there are very few needs for it.<br /><br />One other thing you have
    to watch out for in these environments is your VLAN port counts (or STP logical
    interfaces in Cisco speak).  A cloud provider can quickly run up on that number
    long before they run up on the supported VLAN limit.  Every time you add a VLAN
    to a VNIC, it creates a new STP logical interface and that is a limited resource
    on the N5Ks, etc...<br /><br />Layer 2 sucks.
  id: '4007564942804356545'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/08330086972611588148
  pub: '2013-03-28T00:08:41.331+01:00'
  ref: '348997122779517340'
  type: comment
- date: 28 March 2013 03:02
  html: '@John Mc<br /><br />You prove the point again. You Network Engineers are
    extremely smart. But that is most of the problem. To do your job, you HAVE to
    be really smart ! The incumbent vendor insists that you work all this out for
    yourself. Your incumbent vendor insists that you write your own scripts. You say
    it&#39;s fairly simple. Maybe it is. But all your CIO sees is OPEX OPEX OPEX!<br
    /><br />* OPEX in that it&#39;s manual<br /><br />* OPEX in that you have to spend
    lot&#39;s of time working this out, and then selling it to other members of the
    team.<br /><br />* OPEX in that you are really smart  - meaning I have to pay
    you twice as much as a Server Admin that can point and click vCenter - because
    he DOESN&#39;T HAVE TO UNDERSTAND what is really going on underneath the covers
    (anymore that a developer needs to understand the x86 instruction set)<br /><br
    />Your CIO can&#39;t understand why he can have abstraction in everything else
    IT - but not the Holy network.<br /><br />When some customers start to replace
    &quot;you never get fired for buying... types&quot;, with the same kind of pioneering
    Engineers that threw out their FEP&#39;s, 3270 Terminals and Token Ring for a
    better/faster/cheaper alternative - guess what? Your CIO will start to as well.<br
    />'
  id: '8029810173562132265'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-03-28T03:02:15.938+01:00'
  ref: '348997122779517340'
  type: comment
- comments:
  - date: 30 March 2013 19:08
    html: 'The &quot;correct&quot; protocol to use between hypervisor and ToR switch
      is EVB (802.1Qbg), which does exactly what you&#39;re describing ... only it&#39;s
      getting nowhere. A few details in these blog posts: http://blog.ioshints.info/search?q=evb&amp;by-date=true'
    id: '711888989560154558'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2013-03-30T19:08:36.852+01:00'
    ref: '7802698022821353904'
    type: comment
  date: 30 March 2013 01:04
  html: 'The big challenge with VLAN provisioning for many folks IMO has been the
    risks of managing VLANs in the core of an STP-based network.  Configuring access
    ports and access trunks is comparatively very low risk.<br /><br />In the last
    few years, we&#39;ve seen new [non-STP] bridging technologies that don&#39;t require
    VLAN provisioning on core-facing links -- VXLAN, QFabric, FabricPath, SPB, etc.  These
    are all overlay based bridging technologies.  With these solutions, the major
    reason for slow VLAN deployments is done away with.  What remains is the lack
    of a standards-based solutions for the network to autonomically attach access
    ports to VLANs.  VDP as a solution seems to have gone nowhere because of bloat
    possibly.   However, we can expect to see an &quot;MVRP UNI&quot; arrive soon
    enough that will be coupled with overlay-based (ex: VXLAN) core bridging networks.<br
    /><br />In the MVRP UNI approach, a hypervisor will send VLAN declaration to a
    TOR when a VM requiring it shows up.  The TOR attaches the port/channel to the
    required VLAN, and the rest is handled by the overlay protocol.  The TOR never
    propogates or declares a VLAN to it&#39;s neighbors (including hypervisors).  This
    is an unconventional use of MVRP, but works fine for the purpose of autonomic
    VLAN configuration and satisfies the needs of the average enterprise.  Linux will
    have MVRP support (http://comments.gmane.org/gmane.linux.network/244153).  Now
    we just need to get Openstack support and the rest will follow.<br /><br />This
    isn&#39;t the glorious approach, but for most companies good-enough will do for
    now, and hopefully some measure of sanity restored.  There are a number of benefits
    for the average enterprise which I&#39;ll leave for another day.'
  id: '7802698022821353904'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Aldrin
  profile: https://www.blogger.com/profile/15493370358037866116
  pub: '2013-03-30T01:04:09.652+01:00'
  ref: '348997122779517340'
  type: comment
- comments:
  - date: 31 March 2013 05:03
    html: Yes -- but right now all those things are muddying the water of what most
      folks need and could have been achieved by now.  For most businesses revolutionizing
      their network isn&#39;t a goal.
    id: '7876023072970707357'
    image: https://resources.blogblog.com/img/blank.gif
    name: FullMesh
    profile: null
    pub: '2013-03-31T05:03:58.519+02:00'
    ref: '5424426310502235635'
    type: comment
  date: 30 March 2013 16:36
  html: Why stop at VLANs?  What about VRFs, ACLs, NAT, Firewall &amp; Load Balancers
    contexts, etc.  There&#39;s more to a virtual network than just Layer 2.
  id: '5424426310502235635'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: http://bradhedlund.com
  pub: '2013-03-30T16:36:29.018+01:00'
  ref: '348997122779517340'
  type: comment
- date: 16 April 2013 16:54
  html: Ivan is right, it is fear and too often on the networking side I had tools,
    even vendor supplied that only provided a percentage of success in automating
    changes. It is about a level of trust and at times to &quot;code&quot; that trust
    into the tool to guarantee 100% will take too long to do thus you might as well
    as do it manually and get it done quicker. Who do you trust the chance of the
    tool messing up or that fat finger?
  id: '5407541390128590015'
  image: https://resources.blogblog.com/img/blank.gif
  name: jsicuran
  profile: http://www.amilabs.com
  pub: '2013-04-16T16:54:40.635+02:00'
  ref: '348997122779517340'
  type: comment
- comments:
  - date: 29 April 2014 18:33
    html: '&quot;To make error is human. To propagate error to all server in automatic
      way is #devops.&quot;<br /><br />Source: https://twitter.com/DEVOPS_BORAT/status/41587168870797312'
    id: '8374224833905347814'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2014-04-29T18:33:38.992+02:00'
    ref: '2838802086560217264'
    type: comment
  date: 29 April 2014 16:32
  html: An automated configuration tool is also a &quot;weapon of mass configuration&quot;
    pointed at your network. Such a tool can amplify a single typing mistake into
    a major network outage. Just to make things worse, large network vendors have
    configuration tools that launch changes without so much as a single confirmation
    box or &quot;Are you sure?&quot; warning. Almost as though the developers have
    never heard of the concepts of change management. These &quot;tools&quot; practically
    guarantee a bad outcome. Fear is the appropriate emotion when faced with these
    options.
  id: '2838802086560217264'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2014-04-29T16:32:58.520+02:00'
  ref: '348997122779517340'
  type: comment
count: 23
id: '348997122779517340'
type: post
url: 2013/03/what-did-you-do-to-get-rid-of-manual.html
