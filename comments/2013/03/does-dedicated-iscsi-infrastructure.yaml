comments:
- date: 21 March 2013 09:30
  html: Ivan,<br /><br />I think you&#39;re looking for the following:<br /><br />http://www.research.ibm.com/haifa/satran/ips/PaloAlto-MarkBakke-crc-recovery.pdf<br
    /><br />Cheers :)
  id: '5402278717791399581'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/05378426663611667686
  pub: '2013-03-21T09:30:55.456+01:00'
  ref: '1448707925903426857'
  type: comment
- date: 21 March 2013 17:12
  html: We run iSCSI this way simply because it&#39;s what the SAN vendor design guide
    specifies. Storage vendors love to blame the network; it&#39;s one less thing
    they can complain about.
  id: '5049887534015524096'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Jay Swan
  profile: https://www.blogger.com/profile/02571029118821999072
  pub: '2013-03-21T17:12:37.970+01:00'
  ref: '1448707925903426857'
  type: comment
- date: 21 March 2013 20:23
  html: I have never seen the benefit of running iSCSI this way.  It only made sense
    to me if you were running lower speed switches that were available when iSCSI
    was first popularized (i.e.3750).  When you have fast switches (Nexus, Brocade,
    etc.) why not collapse the storage distribution A&amp;B sides and mix it in with
    the front-side transport?  802.1Qbb is supposed to allow you to mix lossy and
    lossless transport together, the MTBF on network gear is just has high as storage
    in what I have seen (1 each in 5 years) and both events ended up as nothing-burgers
    due to redundancy.  I still don&#39;t see the need for ethernet AB separation
    from data let alone separation from each side.  I think the temptation to put
    in lesser gear when you have built that much &quot;redundancy&quot; is just too
    tempting for management.  I think that will result in more failures than just
    putting in big, bad monster switches and calling it a day.<br /><br />Now to be
    fair, you work with much larger DCs than I do so this might just be a scale thing.  <br
    /><br />
  id: '1602440197879163516'
  image: https://resources.blogblog.com/img/blank.gif
  name: MikeH
  profile: null
  pub: '2013-03-21T20:23:51.501+01:00'
  ref: '1448707925903426857'
  type: comment
- date: 22 March 2013 02:02
  html: I interviewed a few people with this set up a while back (4yrs?) and the answers
    were:<br />- We know Ethernet better than we know FC, so if we build an iSCSI
    SAN we don&#39;t have to hire FC people.<br />- We know Ethernet can handle iSCSI
    and we know it can handle our front end traffic, but we&#39;re not sure it can
    do both at the same time.<br />- If we keep it separate, the storage vendor can&#39;t
    blame the network (as jswan mentioned)<br />- I don&#39;t recall many specific
    technical reasons for the decision, most were political or being conservative.
  id: '8970192206339100483'
  image: https://resources.blogblog.com/img/blank.gif
  name: Abnerg
  profile: http://abnerg.tumblr.com
  pub: '2013-03-22T02:02:43.196+01:00'
  ref: '1448707925903426857'
  type: comment
- date: 11 July 2013 15:26
  html: 'While the alleged &quot;FC-BB-5 violation&quot; does indeed give you two
    independent paths from the server into the ToR FCoE switch, it hardly provide
    &quot;A/B separation&quot;. For that, you need an actual air-gap between two completely
    independent fabric.<br /><br />In your own words: &quot;A single STP loop (or
    any other loop-generating bug, including some past vPC bugs) can bring down the
    whole layer-2 domain ... including both server-to-storage paths.&quot;, and I
    couldn&#39;t agree more. That&#39;s why my stance on FCoE is that it&#39;s great
    but if you really want (more like need) the HA required for mission critical applications
    you better break it out into native FC to go to separate FC A/B fabrics as soon
    as possible (first hop). Remember, logical A/B separation has been possible in
    native FC environments for many years (with VSANs and Virtual Fabrics), yet no
    mission critical FC storage environment is built on this logical isolation, and
    there&#39;s a good reason for that.<br /><br />Same thing goes for iSCSI. If you
    really, *really* want to build a truly HA, mission-critical iSCSI SAN then you
    need two dedicated iSCSI networks (and not VLANs on the same network). Remember,
    the proverbial substance *will* hit the rotating blades one day...'
  id: '8928116839259218135'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2013-07-11T15:26:52.794+02:00'
  ref: '1448707925903426857'
  type: comment
- date: 21 December 2013 06:11
  html: ' &quot;A SCSI failure arriving at just the right time could easily result
    in the famous colored screen&quot;.<br />Kernel panics only happen if the root
    filesystem is on the SAN. Meaning that the corruption has to be to the system
    files that are running the OS. And even then only the components running in kernel
    space and some important user space components like the service manager can crash
    the OS. Other system files are just user processes. <br /> The OS in most deployments
    is connected through a DAS.  Diskless thinclients connected to SANs are not really
    popular because HDDs are dirt cheap and engineering a SAN where hundreds of PCs
    pull their OS data through a fabric would cost sick amounts in dollars. '
  id: '6022098808457797092'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: garegin
  profile: https://www.blogger.com/profile/14508392154453584343
  pub: '2013-12-21T06:11:17.894+01:00'
  ref: '1448707925903426857'
  type: comment
- date: 21 December 2013 06:34
  html: '&quot;Remember, the proverbial substance *will* hit the rotating blades one
    day...&quot;<br />I mean, who cares. If you are running Linux and to a lesser
    degree, Windows, you probably are going to get burned by cascading bad updates
    much sooner than a L2 meltdown. <br />Don&#39;t people remember when Dreamhost
    went down for days because of a bad Debian update.  <br />Unless you are running
    some hardened OS like CapROS or VOS you are looking at a software caused meltdown
    to be as realistic as catching cold in the flu season.'
  id: '77472439661506741'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: garegin
  profile: https://www.blogger.com/profile/14508392154453584343
  pub: '2013-12-21T06:34:15.972+01:00'
  ref: '1448707925903426857'
  type: comment
count: 7
id: '1448707925903426857'
type: post
url: 2013/03/does-dedicated-iscsi-infrastructure.html
