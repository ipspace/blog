{
  "comments": [
    {
      "date": "13 April 2011 10:59",
      "html": "&gt; Maybe, just maybe, there\u2019s a technology out there that\u2019s been field-proven, known to scale, and works over switched Ethernet.<br /><br />802.1ah<br /><br />&gt; would solve the scalability ones<br /><br />Check<br /><br />&gt; breeze by the VLAN numbering barrier<br /><br />Check<br /><br />&gt; enable instant inter-DC connectivity<br /><br />Check, can do over L2TPv3 or VPLS or EoMPLS<br /><br />&gt; The full effect of the scalability improvements would only become visible after deploying P2MP LSPs in the network core<br /><br />Don&#39;t need that, as multicast is natively supported<br /><br />&gt; we won\u2019t see either of these options<br /><br />ALU does that - that&#39;s their flavour of &quot;DC fabric&quot;. S-VLAN per flexible configurable combination of selectors (L2/L3/L4, if I&#39;m not mistaken), automagically mapped to a hypervisor that needs it, when it needs is, across a single DC or a number of DCs.",
      "id": "7516952425288385800",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-13T10:59:38.068+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 11:00",
      "html": "Grr, forgot to &quot;follow&quot; the stream again.",
      "id": "3125925521445435741",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-13T11:00:49.703+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 13:52",
      "html": "After reading 802.1ah I got the impression that it maps broadcast+multicast in the C-component into multicast in the B-component. The standard does refer to potential use of MRRP (or static filters) but it&#39;s not mandatory.<br /><br />Am I missing something? Also, if ALU does all of that (and we know nothing about it), their marketing department deserves a &quot;kind nudge&quot;. Could you point me to relevant documents?<br /><br />Thank you!<br />Ivan",
      "id": "46087585677464817",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-13T13:52:18.168+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 17:05",
      "html": "AMEN!!!",
      "id": "8303255769405520904",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Mike Courtney",
      "profile": null,
      "pub": "2011-04-13T17:05:49.469+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 18:47",
      "html": "PBB and MPLS are both carrier originated technologies.<br />What is puzzling is the need by vendors to reinvent the wheel for DC.<br /><br />The vendors I talked to, were saying things like &quot;its too complex for user&quot;, &quot;the edge devices will not handle it&quot; (I suppose Ks of Vlans are OK :)).",
      "id": "8545089235260985008",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anton Yurchenko",
      "profile": null,
      "pub": "2011-04-13T18:47:01.782+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 18:50",
      "html": "And IS-IS-based TRILL/FabricPath/VCS Fabric/OTV is not complex? The only difference is the complexity is hidden (which will only bite you when you have to do real in-depth troubleshooting).",
      "id": "6132324199585341644",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-13T18:50:28.968+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 19:09",
      "html": "My point exactly :)<br />The more I read about those the less I understand the reasoning behind their introduction.<br />I thought that even PBB was needed to be brought to life(multicast?).",
      "id": "3834207327127955737",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anton Yurchenko",
      "profile": null,
      "pub": "2011-04-13T19:09:36.036+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 19:09",
      "html": "My point exactly :)<br />The more I read about those the less I understand the reasoning behind their introduction.<br />I thought that even PBB was needed to be brought to life(multicast?).",
      "id": "928519297001434519",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anton Yurchenko",
      "profile": null,
      "pub": "2011-04-13T19:09:47.960+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "13 April 2011 23:45",
      "html": "&gt; into multicast in the B-component<br /><br />...which is then handled as if it was normal Ethernet multicast, right?<br /><br />&gt; their marketing department deserves a &quot;kind nudge&quot;<br /><br />In my experience, they rely on the existing relationships and trade shows, when you can see the product in action and speak with somebody who actually knows what they are talking about.<br /><br />Here&#39;s one link I found which has some marketing fluff on the subject: http://searchnetworking.techtarget.com/news/2240034476/Alcatel-Lucent-debuts-monster-data-center-switch-fabric-architecture",
      "id": "4978103178390968003",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-13T23:45:33.652+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 00:19",
      "html": "you are my hero",
      "id": "3730676424338475101",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2011-04-14T00:19:25.245+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 08:23",
      "html": "&gt;&gt; into multicast in the B-component  <br />&gt; ...which is then handled as if it was normal Ethernet multicast, right?<br /><br />Exactly my point. Normal multicast doesn&#39;t scale as it&#39;s flooded throughout the broadcast domain ... unless, of course, you use something like MRRP.<br /><br />On the other hand, VPLS (more so with P2MP LSP) scales better, as the multicasts get flooded only to those PE-routers that actually need them (with P2MP LSP on a source-rooted tree).",
      "id": "5772654023002760520",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T08:23:06.070+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 10:03",
      "html": "You didn&#39;t really answer my question though. Yes it could possibly function as an alternative when someone invests the time/money to develop it. Or do you have a way of fully automating this for in case a tenant wants to isolate his vApp on a separate network?<br /><br />I am all ears,",
      "id": "3644845288476292357",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Duncan",
      "profile": null,
      "pub": "2011-04-14T10:03:19.185+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 10:07",
      "html": "&gt; Normal multicast doesn&#39;t scale as it&#39;s flooded throughout the broadcast domain ... unless, of course, you use something like MRRP. <br /><br />Ok, I should have called things by their proper names - I should have said &quot;SPBM&quot; instead of &quot;802.1ah&quot;. My bad! :(<br /><br />I think what I wanted to say is that SPBM takes care of pruning/replication by building multicast topology based on SPF trees (http://www.nanog.org/meetings/nanog50/presentations/Sunday/IEEE_8021aqShortest_Path.pdf)",
      "id": "8754146016979811362",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-14T10:07:09.864+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 10:26",
      "html": "Well, if it is not vMware but say KVM hypervisor I&#39;d guess one can use openvswitch which actually implements Ethernet over IP, though admittedly it&#39;s not VPLS.",
      "id": "7628391815000436428",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-14T10:26:51.311+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 10:26",
      "html": "Well, if it is not vMware but say KVM hypervisor I&#39;d guess one can use openvswitch which actually implements Ethernet over IP, though admittedly it&#39;s not VPLS.",
      "id": "2577779054970183911",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-14T10:26:57.772+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 10:38",
      "html": "The way I understood the documentation, openvswitch implements P2P links with EtherIP, which obviously doesn&#39;t scale. Also, you would need plenty of orchestration to ensure links are established on-demand as VMs move around (which is what vCDNI does automatically).<br /><br />HOWEVER - there&#39;s OpenFlow  :-P We could still be pleasantly surprised.",
      "id": "4533008662754008686",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T10:38:34.265+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "31 May 2012 16:09",
          "html": "There seems to be some confusion here. SPBM scopes customer broadcast to the I-SID end points. It is not single broadcast domain. I have trouble figuring how n**2 replication of VPLS tunnels to the hypervisor could possibly scale better. Defies laws of physics.",
          "id": "3385062334905681525",
          "image": "https://resources.blogblog.com/img/blank.gif",
          "name": "Dave Allan",
          "profile": null,
          "pub": "2012-05-31T16:09:05.838+02:00",
          "ref": "4040071039879486849",
          "type": "comment"
        }
      ],
      "date": "14 April 2011 10:40",
      "html": "Ah, now I&#39;m in a familiar territory  8-)<br /><br />SPBM uses MAC-in-MAC encapsulation with I-SIDs to provide different forwarding mechanism in the SPBM core. However, the design paradigm is still a single broadcast domain (modulo VLAN pruning). Doesn&#39;t scale.",
      "id": "4040071039879486849",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T10:40:59.453+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 11:33",
      "html": "&gt; However, the design paradigm is still a single broadcast domain (modulo VLAN pruning). Doesn&#39;t scale.<br /><br />Could you please elaborate where do you see the scaling issue? Multicast traffic inside &quot;huge&quot; I-SIDs?",
      "id": "2600315219199514019",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-14T11:33:06.401+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 11:40",
      "html": "As I wrote in the vCDNI post, if a single VM goes bonkers, the whole DC is flooded. Same with SPBM.<br /><br />You won&#39;t see that in a typical SP/VPLS/802.1ah network because sensible people use routers to connect to the Carrier Ethernet service.",
      "id": "6179770129786637295",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T11:40:20.753+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 12:06",
      "html": "&gt; the whole DC is flooded<br /><br />Wouldn&#39;t flood scope be contained by the I-SID to which this VM is mapped? I thought that I-SID is a broadcast container.",
      "id": "2421887087249964932",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-14T12:06:56.876+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 12:12",
      "html": "&gt;  I thought that I-SID is a broadcast container.<br /><br />Good question for a Packet Pushers Podcast we&#39;re doing next week. I don&#39;t think it is in SPBM.<br /><br />Also, in 802.1ah, multi/broadcasts are sent to default BA, which is multicast (unless you configure P2P service), so C-multicast becomes B-multicast.",
      "id": "3584821836149990480",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T12:12:02.691+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 12:29",
      "html": "Will look forward to the podcast :)",
      "id": "4911358553999168543",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-14T12:29:48.379+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 14:30",
      "html": "A little surprised there is no mention of Q-in-Q in the article or comments...",
      "id": "2170255152876065707",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Brad Hedlund",
      "profile": null,
      "pub": "2011-04-14T14:30:33.637+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 14:33",
      "html": "Q-in-Q doesn&#39;t help you. It&#39;s even worse than vCDNI - single MAC address space, single broadcast domain (unless you use VLAN pruning).",
      "id": "1870474363533287338",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T14:33:04.013+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 15:05",
      "html": "Well there is the mpls-linux project os not all is lost.<br /><br />it&#39;d be interesting to see running mpls-linux with say VPLS &amp; KVM. I guess I need to kill couple week ends to see whether it&#39;s possible  :-[",
      "id": "2030223588809890908",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-14T15:05:00.923+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 15:07",
      "html": "I don&#39;t understand how supporting SPB or whatever on physical device which is ALU is here going to fix lack of PE edge or other scalable Ethernet over IP transport in hypervisor ? Why even bring it into conversation unless ALU offers software implementation that can be instantiated as vswitch on hypervisors ?",
      "id": "8538099371651903603",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-14T15:07:26.795+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 15:11",
      "html": "And ? How having or not having ALU is going to fix lack of scalable of Ethernet-over-IP transport on hypervisor ? <br /><br />Whole premise of Ivan&#39;s post is to get rid of physical L2 and do L3 all the way down (because L3 just scales way better than L2, probably why Internet is L3 =-X  ). Stopping at the physical network and leaving virtual network as is not really a solution. Or may be I&#39;m missing something.",
      "id": "189513884854718469",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-14T15:11:26.612+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 16:09",
      "html": "Doesn&#39;t MPLS require manual configuration to bridge networks? How will that work?",
      "id": "4245471263657296341",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Guest",
      "profile": null,
      "pub": "2011-04-14T16:09:00.678+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 19:35",
      "html": "mpls-linux seems to be dead (last change ~2 years ago) ... and it only had basic LDP and PE-router labeling functionality.",
      "id": "6287737803480662034",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T19:35:23.385+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 19:36",
      "html": "Well, everything needs configuration (including vCDNI portgroups). The question is how much of the complexity you decide to hide behind a CLI/GUI.",
      "id": "6636383092464864476",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-14T19:36:05.647+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 22:20",
      "html": "Ivan, <br /><br />Building a RFC2547 PE isn&#39;t the answer for Cloud networks because the goal is not a transport network, but a broadcast domain. Really what&#39;s needed in the Cloud is an emulation of what customers get when they do into a datacenter like Savvis or Equinix - you get a set of racks in a cage, and you uplink your firewall/router up to the provider&#39;s redundant set of links that goes to their access layer switching and below your router/firewall, you create a number of routed networks backed by VLANs, etc - simple stuff, right? Well, in the Cloud - folks want the same thing. Perimeter firewalling and access routing between multiple broadcast domains and some level of network hierarchy. One thing you will see this year is the trend of mapping broadcast domain to multicast. This is no longer only a VMware thing - you will be pleasantly surprised this year by other vendors.",
      "id": "1536671550954741436",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "vSerge",
      "profile": null,
      "pub": "2011-04-14T22:20:08.699+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "14 April 2011 22:42",
      "html": "I would appreciate if you could explain what is the need for a scalable transport on hypervisor? Ivan pointed our that a VM &quot;gone bonkers&quot; can produce broadcast or multicast flood. Ok, understand. But this can be dealt with - with BMU rate limiters and contained broadcast domains in DC network above hypervisor. What else is there that needs additional intelligence on a VM host?",
      "id": "186921957685957474",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-14T22:42:05.531+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 01:52",
      "html": "I think Ivan post goes over that in 2nd paragraph. Assuming vSwitch is still L2 then its hand-off to physical network is still L2 802.1q trunk and you are still constrained by 2^12 VLAN ID space. Because if vSwitch L2, distributed vSwitch naturally would expect same VLAN ID across hypervisors for the same broadcast domain/VM L2 adjacency. How does mapping on ALU is going to extend VLAN ID space on trunk between vSwitch and physical switch. I am all ears.",
      "id": "1415911176957637728",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-15T01:52:44.776+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 02:17",
      "html": "Ok, went to the source (802.1ak draft 3.6) - the way it reads to me, &quot;client-side&quot; BUM *is* constrained to its I-SID, which includes member SPBM PEs (which have ports with this I-SID) and selected shortest path internodal links for this given I-SID.<br /><br />So I don&#39;t think the whole DC will get flooded - only PEs and links which serve an I-SID with a misbehaving VM in it.<br /><br />And yes, the behaviour inside I-SID appears to be flooding, but with efficient replication at fork points.",
      "id": "7262302602980326345",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T02:17:49.748+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 02:18",
      "html": "&gt; 802.1ak<br /><br />I mean 802.1aq, of course.",
      "id": "4528953827020058337",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T02:18:34.707+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 02:43",
      "html": "Second paragraph reads: &quot;If you need more than 1K VLANs, you\u2019ve either totally messed up your network design or you\u2019re a service provider offering multi-tenant services...&quot;. <br /><br />SPBM can support 2^24 &quot;VLANs&quot;, and has no problem with supporting 4K regular VLAN-based services per port (read: per ESX server). So with SPBM we are moving from &quot;4K VLANs per DC&quot; to &quot;4K VLANs per ESX&quot;.<br /><br />Hope this makes sense.",
      "id": "1666525770097752170",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T02:43:37.369+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 05:38",
      "html": "The more accurate statement would be 2^12 VLAN per virtual L2 domain that can have more than one ESX. Unless you can show me that with SPBM L2 domain can be shrank to single ESX/vSwitch and still provide L2 adjacency/vMotion/etc between different ESX hosts with non-matching VLAN IDs.<br /><br />So I can&#39;t quite see how you can have ESXes have independent VLAN ID spaces as long as there are L2 adjacency checks that rely on matching VLAN IDs between ESXes.",
      "id": "6621205019748186610",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-15T05:38:14.137+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 06:13",
      "html": "&gt; 2^12 VLAN per virtual L2 domain that can have more than one ESX<br /><br />There isn&#39;t &quot;an L2 domain with bunch of VLANs in it&quot;. May I recommend to check out the NANOG presentation I provided a link to above for how SPB solves these problems? Here&#39;s the link again: http://www.nanog.org/meetings/nanog50/presentations/Sunday/IEEE_8021aqShortest_Path.pdf - check slides 12, 16 and 17.",
      "id": "3986199664071406357",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T06:13:51.324+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 06:47",
      "html": "Thanks for the link, but I don&#39;t see how it address my point. it shows that C-VLAN is the same on both hyper visors, if that&#39;s not single L2 ID space then what that is ? And given that C-VLAN ID space must be same on all ESXes in the cluster then my statement stands.",
      "id": "8289710528214505125",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-15T06:47:21.402+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 07:11",
      "html": "&gt; C-VLAN ID space must be same on all ESXes in the cluster<br /><br />Correct. But you also can use this same C-VLAN ID space on a different ESX cluster within the same DC.<br /><br />How many VMs would you have running on a single ESX cluster?",
      "id": "7615250380413666119",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T07:11:03.853+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 07:46",
      "html": "Well, that&#39;s what my statement was about. I guess I worded it a bit vaguely. Virtual L2 domain in my not so clear definitions (that shares VLAN ID space) is the size of ESX cluster with current vSwitch. Obvious trend is for bigger ESX clusters. <br /><br />At about 64-128 (today is max st 32) x 4-socket systems IMHO we are going to hit VLAN ID space constraint. Even then I see nothing wrong with having plain vanilla physical L2 islands for each cluster at these sizes separated by L3.<br /><br />SPBM/etc really come to play when clusters get massive and at that stage VLAN ID space on hypervisors is going to be the constraint.<br /><br />IMHO for cloud SP, network edge must be pushed down to hypervisor, whether it&#39;s MPLS or SPBM is immaterial. The major theme of Ivan&#39;s post to me was that using dumbest possible network feature set in vSwitches is not going to get a cloud SP far enough. For SP like that vSwitches must have more than rudimentary features they have today.<br /><br />If ALU has a vSwitch appliance (doesn&#39;t matter whether it&#39;s for Xen/KVM/whatever) that can do SPBM I&#39;d be thrilled.",
      "id": "5969471862363172691",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-15T07:46:55.947+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 08:10",
      "html": "Thanks for the feedback! What you mention is a crucial point: the cloud providers have to decide whether they want to offer L3 connectivity or L2 broadcast domains to their customers.<br /><br />If I would be a forward-looking provider, I would try to limit myself to L3 connectivity. Easier to design, build &amp; deploy ... plus it can service all greenfield applications and most existing stuff. I may be totally wrong, but looking from the outside it seems this is what Amazon is doing.<br /><br />If I want to capture 100% of the market, I obviously have to provide virtual L2 broadcast domains. The question I would have to ask myself is: is the incremental revenue I&#39;ll get by providing L2 broadcast domain big enough to justify the extra complexity? Based on what I see happening in the industry, I don&#39;t believe too many people have seriously considered this question.<br /><br />Obviously the networking industry is more than happy to support the &quot;virtual L2 domain&quot; approach, as it allows them to sell new technology and more boxes, which begs another question: &quot;who exactly is generating the needs?&quot;",
      "id": "6090959652432531500",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T08:10:28.991+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 08:17",
      "html": "#1 - vDS is per vCenter, not per ESX cluster, so it (and its VLANs) can span 1.000 hosts and 20.000 virtual ports (http://www.vmware.com/pdf/vsphere4/r41/vsp_41_config_max.pdf)<br /><br />#2 - we&#39;re already hitting the VLAN ID space constraint in VDI cloud applications like our FlipIT http://flipit.nil.com/ (large # of VDI VMs per ESX server, small # of VMs in a VLAN)<br /><br />#3 - You totally got my point (and rephrased it better than I could) - complexity belongs in vSwitch, optimal end-to-end transport into the physical boxes.",
      "id": "1844308892085204530",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T08:17:44.617+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 08:46",
      "html": "Understand now, thanks! I&#39;m not too good with VMware, so please excuse me if the next question is stupid: does ESX have single VLAN ID space across all its vSwitch instances? Say an ESX host has two vSwitch instances, each linked to a separate pNIC. If you configure a vNIC on vSwitch 1 with VLAN ID 10, and then another vNIC on vSwitch 2 with the same VLAN ID, what is ESX / vSphere going to make of it? Will it expect that the VLAN 10 is bridged outside the ESX and treat both vNICs as members of a same port group?<br /><br />And question 2: what about VEPA? Doesn&#39;t it &quot;extend&quot; vNICs past vSwitch to an external actual real switch (which could be running SPBM)?",
      "id": "8997006072774556304",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T08:46:58.690+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 08:55",
      "html": "#1 - in my understanding, the VLAN ID space is tied to physical NICs. Outbound a portgroup does VLAN tagging (assuming it&#39;s configured that way), inbound the packets are only delivered to a portgroup if they arrive through the pNIC to which the portgroup is attached.<br /><br />#2 - I don&#39;t think VEPA would help you here and it definitely does NOT extend vNICs (that would be 802.1Qbh). More about it in a week or so.",
      "id": "859190960570129918",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T08:55:16.929+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 08:55",
      "html": "#1 - in my understanding, the VLAN ID space is tied to physical NICs. Outbound a portgroup does VLAN tagging (assuming it&#39;s configured that way), inbound the packets are only delivered to a portgroup if they arrive through the pNIC to which the portgroup is attached.<br /><br />#2 - I don&#39;t think VEPA would help you here and it definitely does NOT extend vNICs (that would be 802.1Qbh). More about it in a week or so.",
      "id": "1946604690243605292",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T08:55:29.958+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 09:57",
      "html": "1) ESX will not care whether VLAN 10 is disjointed unless you put ESX control plane traffic on it (Service Console, VMKernel, FT). But even for user VM VLAN ESX will show in that cute diagram (showing connectivity to datastores &amp; port-groups) in vCenter that these two ESX hosts in the same cluster are on the same VLAN so human operator is likely to assume that this not a disjointed VLAN. So implicitly ESX does make an assumption.<br /><br />I haven&#39;t played much with vDS but I expect vDS to have stronger assumptions if the same vnics are on the same VLAN ID. Not sure if it checks VLANs for disjointedness. <br /><br />2) VEPA/VN-tag/whatever (as usual there are competing tracks) that tries to move out virtual networking out of hypervisor to physical network box is certainly a valid approach but it requires hypervisor element manager (aka vCenter for ESX) to manage physical network device now to set up/tear down VM related network plumbing when VM moves in/moves off the host. I guess this could be another application for openflow  =-X but I guess either way hypervisor element manager will acquire some advanced network control-plane features to manage either virtual or physical switch box.",
      "id": "9097543887269769232",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-15T09:57:46.489+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 10:12",
      "html": "#1: If this is so, aren&#39;t we sorted out then? Just create multiple vDS, each with its 4K VLANs, and tie each vSwitch to a corresponding pNIC. Should be especially easy with UCS equipped with VICs. Then plug each pNIC into an SPBM switch, which can map either whole port or port/VLAN to an individual I-SID. Viola: multiple 4K VLAN domains per ESX, no?<br /><br />#2: After reading up on VEPA I see that yes, it won&#39;t help, but it won&#39;t need to, provided #1 is true.",
      "id": "4562755554352726868",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T10:12:05.658+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 10:36",
      "html": "Right, a bit more thinking now: in my scenario with two vSwitch instances on the same ESX, where each vSwitch is attached to its own pNIC uplink connection.<br /><br />On the vSwitch 1 we create a port group we call &quot;Network A&quot;, and associate it with VLAN 10.<br />On the vSwitch 2 we create a port group we call &quot;Network B&quot;, and also associate it with VLAN 10.<br /><br />Now, vCenter knows port groups by their network names, *not* by their VLAN IDs, correct? If so, it has no right to make an assumption that VLAN ID 10 on vSwitch 1&#39;s uplink is the same broadcast domain as VLAN ID 10 on vSwitch 2&#39;s uplink, correct? If so, we&#39;re all set! :)",
      "id": "1060223763327672757",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T10:36:12.866+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 10:40",
      "html": "... until a device, link or pNIC breaks in the middle of the night and the late shift operator assigns the port group to different pNICs trying to restore connectivity  :-E<br /><br />Never rely on overly complex designs, they will break at the most inappropriate moment.",
      "id": "237008453470441047",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T10:40:54.388+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 10:52",
      "html": "BTW each vSwitch is likely use 2 x pNICs for redundancy so we will be burning switch ports just because we need more VLAN IDs per host though it&#39;s probably good for network device vendors =-X<br /><br />Well, I guess that&#39;s a clever way we can prolong the misery a bit by adding more and more pNICs  =-X but in the end isn&#39;t easier to do the right thing  =-X",
      "id": "6310143811943689352",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-04-15T10:52:51.927+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 11:04",
      "html": "Wouldn&#39;t there be multiple (teamed) pNICs, just as they are today? And yes, agree with the complexity and breaking...",
      "id": "2095904805631908808",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-04-15T11:04:12.729+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 15:33",
      "html": "&gt; If I would be a forward-looking provider, I would try to limit myself to L3 connectivity. Easier to     &gt; design, build &amp; deploy ... plus it can service all greenfield applications and most existing stuff. I &gt;may be totally wrong, but looking from the outside it seems this is what Amazon is doing.<br /><br />I thought Amazon started to do just that (i.e. offering dedicated layer 2 domains). :<br /><br />http://aws.typepad.com/aws/2011/03/new-approach-amazon-ec2-networking.html<br /><br />&gt; Obviously the networking industry is more than happy to support the &quot;virtual L2 domain&quot; approach, &gt;it allows them to sell new technology and more boxes, which begs another question: &quot;who exactly is &gt;generating the needs?&quot;<br /><br />We don&#39;t want to sell any box at VMware ... yet we want the world to be better ;) <br /><br />Keep up with your posts... it helps us to stay on track. <br /><br />Massimo.",
      "id": "4340722026255433758",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Massimo Re Ferre'",
      "profile": null,
      "pub": "2011-04-15T15:33:24.915+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 15:46",
      "html": "Someone smart (like Amazon) would be able to do everything they publicly promise with smart access lists (like multi-tenant vShield App or Cisco&#39;s VSG).<br /><br />Trying to find someone to answer a few basic questions like:<br /><br />* Would IP multicast work?<br />* Would subnet-level broadcast work?<br />* Can I run Microsoft NLB or Microsoft cluster over it?<br />* Can I run non-IP protocols (like IPv6 or CLNP) between my EC2 instances?<br /><br />Will keep you updated if I ever get the answers ;)<br /><br />As for the industry classification - I thought you were the leader in the virtualization industry  8-)",
      "id": "377552735642722518",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T15:46:41.572+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 16:35",
      "html": "I think that yes, you could do that (technically). I believe however that it&#39;s easier to sell to two &quot;orgs&quot; that they are going to be hosted on two different broadcast domains (separated by &quot;traditional&quot; firewall architectures - although virtual and not physical) than selling to two &quot;orgs&quot; that they are hosted on a single broadcast domain but are kept separated by a &quot;firewall filter driver running on the hosts&quot;. Don&#39;t get me wrong, I am not saying it&#39;s less secure, I am saying that it&#39;s more difficult to sell (in 2011). People out there tend to be conservative (and have political agendas). They will make a step at a time. If you ask me vShield App (or similar technologies) are even more compelling than VCDNI or any other layer 2 virtualization technology we may come out with... I found interesting that you jumped from MPLS straight to APP/VSG. <br /><br />Massimo.",
      "id": "1608112201820433910",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2011-04-15T16:35:47.339+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 16:40",
      "html": "BTW: <br /><br />&gt; Can I run Microsoft NLB or Microsoft cluster over it?  <br /><br />if you want to run NLB or MSCS in the cloud.... I think you are not ready for the cloud. Me think.<br /><br />Massimo.",
      "id": "4383813621763052109",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2011-04-15T16:40:02.135+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 17:05",
      "html": "Me agree. Wholeheartedly. Probably not everyone does  :-E",
      "id": "1956133812376010792",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T17:05:54.919+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 17:34",
      "html": "&gt; Me agree. Wholeheartedly. Probably not everyone does<br /><br />Let them crash then. Let them learn the hard way.<br /><br />Massimo.",
      "id": "1942465619872794026",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2011-04-15T17:34:14.811+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 18:52",
      "html": "That&#39;s actually a great way to look at it - if you&#39;re a service provider interested in getting parity with Amazon capabilities (EC2 specifically, as we know that VPC does support bcast), then the approach needs to be equivalent in capabilities where VMs only communicate via L3, today accomplished via filtering (ebtables/iptables in case of Xen Clouds). If you&#39;re looking to build enteprise-like Cloud where clustering apps which rely on broadcast/L2multicast or simply want all your legacy apps to work like many of the Windows services which rely on broadcast - then you&#39;d want something either built via VLANs or the next-gen encapsulation alternatives. In the encapsulation approaches, it&#39;s key to provide solid debugging and tracing capabilities - from Wireshark plugins to decode the new protocol and tools that can help troubleshoot the topology. <br /><br />One of the benefits of keeping the notions that exist today in physical networks like zoning that&#39;s based on subnet + underlying bcast domain is that allows for easier migration of apps into the Cloud space. Vision is something like this - p-to-V your workloads, then p-to-V your network/security config (or something close to it), retain the same address space or renumber (if need be) and either via VPN or private connectivity to your provider - upload your apps and netsec config which stands up the services in the Cloud... It sounds a bit futuristic, but I have some REST scripts that I share which stand up multiple bcast domains, sets up access routing between them, sets up perimeter firewall w/ NAT towards the Internet, and some static routes pointing to the provider PE router for the MPLS VPN without NAT and then pushes the VMs in these containers and numbers them... <br /><br />Can I say that REST is becoming the new CLI for Cloud? I used to prefer Cisco IOS/CatOs until I played with my first Juniper in &#39;99 (Olive x86 box running JunOS - before Juniper had the hardware ready), fell in love with that Gated-style CLI, now it&#39;s REST... Maybe I&#39;m a single voice in the wilderness or maybe we&#39;ll become a new breed of network engineers?",
      "id": "3812088576243724825",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "vSerge",
      "profile": null,
      "pub": "2011-04-15T18:52:56.338+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 19:49",
      "html": "Amazon VPC does NOT support multicast or broadcast:<br /><br />http://aws.amazon.com/vpc/faqs/#R4",
      "id": "1467245659647119755",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T19:49:24.383+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 19:50",
      "html": "Willing to tell me more about the REST functionality you&#39;re talking about?",
      "id": "5418138968911146019",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-04-15T19:50:22.641+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 22:12",
      "html": "Thanks for the correction - here is a thread asking Amazon folks to implement broadcast/multicast functionality, a good read: <br /><br />https://forums.aws.amazon.com/thread.jspa?threadID=37972",
      "id": "8227996132515692654",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "vSerge",
      "profile": null,
      "pub": "2011-04-15T22:12:51.687+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "15 April 2011 22:15",
      "html": "Yep - let me know which areas to expand upon or you can contact me privately here: sergey at vmware dot com.",
      "id": "6664651527414971684",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "vSerge",
      "profile": null,
      "pub": "2011-04-15T22:15:26.506+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "date": "12 May 2011 19:23",
      "html": "Agree 100% with this article.",
      "id": "9093616582339104154",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Derick Winkworth",
      "profile": null,
      "pub": "2011-05-12T19:23:25.917+02:00",
      "ref": "6869770479657429957",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "01 December 2012 10:34",
          "html": "Yes, I can.",
          "id": "8485878168336498616",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2012-12-01T10:34:26.344+01:00",
          "ref": "8605567925129751751",
          "type": "comment"
        },
        {
          "date": "29 December 2012 02:34",
          "html": "Thank you. I am eagerly waiting for your comments. While you are at it, can you please look at: http://tools.ietf.org/html/draft-smith-lisp-layer2-01 too?",
          "id": "4132924705893477663",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Unknown",
          "profile": "https://www.blogger.com/profile/14893658048554590916",
          "pub": "2012-12-29T02:34:06.358+01:00",
          "ref": "8605567925129751751",
          "type": "comment"
        }
      ],
      "date": "01 December 2012 07:19",
      "html": "Ivan, Can you please offer your comments on these drafts?<br /><br />http://tools.ietf.org/html/draft-ietf-l3vpn-end-system-00<br />https://datatracker.ietf.org/doc/draft-rfernando-virt-topo-bgp-vpn/?include_text=1<br />http://tools.ietf.org/html/draft-ietf-l2vpn-evpn-02<br />",
      "id": "8605567925129751751",
      "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
      "name": "Unknown",
      "profile": "https://www.blogger.com/profile/14893658048554590916",
      "pub": "2012-12-01T07:19:46.207+01:00",
      "ref": "6869770479657429957",
      "type": "comment"
    }
  ],
  "count": 68,
  "id": "6869770479657429957",
  "type": "post",
  "url": "2011/04/vcloud-architects-ever-heard-of-mpls.html"
}