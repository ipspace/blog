<div class="comments post" id="comments">
  <h4>22 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="2449335232946643385">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Russell Heilling</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2449335232946643385" href="#2449335232946643385">04 April 2011 09:03</a>
              </span>
            </div>
            <div class="comment-content">Just tell me that this doesn&#39;t share any pedigree with the MRP link balancing code that has been causing no end of problems for the London Internet Exchange over the past 6 months.<br /><br />The other vendors have all been focused on constantly cutting latencies.  Good to see Brocade recognising that in some cases artificially increasing latency to match effective circuit lengths can improve overall performance.<br /><br />It would be interesting to know whether there is a maximum on the amount of link latency difference that this can cope with.  I have seen a production network where a link with 4ms latency was paired with one with 15ms. I am guessing this platform might have trouble balancing in this situation...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6505005523349577091">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6505005523349577091" href="#6505005523349577091">04 April 2011 09:17</a>
              </span>
            </div>
            <div class="comment-content">If I got the fundamentals sorted out correctly, MRP comes from Foundry, whereas this bag of tricks should have come from Brocade&#39;s SAN.<br /><br />I would only use it on short (intra-DC) links and it probably works only over physical links with microsecond-level skew. <br /><br />BTW, are you telling me the 4ms/15ms links were both P2P physical links (or lambdas) using different fiber runs?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1091139903320557485">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Russell Heilling</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1091139903320557485" href="#1091139903320557485">04 April 2011 14:41</a>
              </span>
            </div>
            <div class="comment-content">Why do you ask if they were physical layer? Does this tech use physical framing that would not work across a longhaul network carrying standard Ethernet frames transparently?<br /><br />This specific (&quot;special&quot;) example was several years ago and I am working from memory, but I think those latency figures are in the right ballpark.  The circuits were fully transparent Ethernet, but not raw L1, I believe they were both EoSDH.  When it was ordered we thought it was primary / backup and the distance difference wouldn&#39;t be an issue - wasn&#39;t until after it went in we realised the customer was using LACP across both links.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5822940310272605297">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5822940310272605297" href="#5822940310272605297">04 April 2011 17:53</a>
              </span>
            </div>
            <div class="comment-content">I __guess__ it might work over anything transparent enough (EoSDH for example), but I think I found some delay-related limitations somewhere. Not sure ... maybe Brook will add something.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8325417642911309633">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8325417642911309633" href="#8325417642911309633">04 April 2011 20:45</a>
              </span>
            </div>
            <div class="comment-content">Ivan,<br /><br />I have some information, in good confidence, that what you describe is what Brocade does on their FC Switches. On the VDX, I was told that they do the load balancing a bit differently to achieve perfect load balancing. Perhaps they have learnt a few tricks from their FC SAN expereince to improve things for the LAN folks. You may want to circle back with Brocade on this.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7389871078905671289">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">plapukhov</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7389871078905671289" href="#7389871078905671289">04 April 2011 21:57</a>
              </span>
            </div>
            <div class="comment-content">Would be great to see any comparison of in-order per-packet load-sharing to &quot;classic&quot; per flow load-sharing to begin with. In a decent-sized FC network, with tenths of servers and storage devices, there are good statistical odds that all port channel member links will be utilized close to optimum using per-flow load-sharing. Disparity, say on a scale of 10%, is not a serious issue as long as every link&#39;s utilization does not exceed 50% (this is when queueing delays become severely noticeable. It all boils down to the size of the network and flow matrix, but apparently the &quot;degenerate&quot; case with only a few servers and storage devices are not commonly seen in modern networks. <br /><br />This is why inverse-multiplexing solutions have been efficient all the time, to begin with. As soon as the number of endpoints is above some threshold, there is no significant advantage that one may gain using clever per-packet load-sharing. The per-packet solutions increase complexity, add marketing buzz, but seem to have little real use in decent-scale networks. <br /><br />Of course you may get say 25%,25%,25%,25% on a 4-link port-channel as opposed to 20%, 30%, 15%, 35% but does it really matter if you are only using fraction of the bundle capacity? One may say - well a 35% utilized link adds more latency but so does Brocade solution - and the delay is not predictable either. The solution that brocade uses might be seen as &quot;inverse reassembly&quot; where sending side needs to buffer packets to equalize arrival timing. As opposed to receive-side reassembly buffers we now have &quot;shaping&quot; buffers that ensure in-order packet delivery. Complexity did not vanish it just got pushed around.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1815048418352190105">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Michael Shipp</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1815048418352190105" href="#1815048418352190105">05 April 2011 08:40</a>
              </span>
            </div>
            <div class="comment-content">I think the real differance here is a flow based LACP can only go as fast as single link (if you have four 1 gb links - then the flow can not exceed 1 bg).<br /><br />On a per packet base then you can get the full 4gb - of couse as long as both sides can push and receive that amount, as in the case in the VDX (using 10gb) to a max of 8 ports per ISL (80 gb Trill path).<br /><br />Now this is used for switch to switch traffic, a server connecting to two or more VDX switches still uses LACP, so we have back to flow based from the serer.<br /><br />Of course Brocade could put the ISL feature in their CNA&#39;s however that would mean your server could only connect to 1 switch.  Not a good idea for HA.<br /><br />You could also say what about putting two dual port CNA&#39;s in a server, then you would have two trill paths of 20gb to two switches in the fabric - however four ports per server is like going back to 2 FC and 2 NICs.<br /><br />Just my thoughts,<br />Michael.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="253977398369064776">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c253977398369064776" href="#253977398369064776">05 April 2011 09:06</a>
              </span>
            </div>
            <div class="comment-content">As Michael pointed out in his reply, you&#39;ll notice the difference primarily when you have a single high-bandwidth flow that would benefit from being able to use multiple links (file transfer, backup ...).<br /><br />As soon as you have enough flows, the packet distribution method doesn&#39;t matter as long as it&#39;s random enough.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5152506559318523974">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Michael Schipp</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5152506559318523974" href="#5152506559318523974">05 April 2011 10:58</a>
              </span>
            </div>
            <div class="comment-content">Michael Shipp <br />MRP (versions 1 and 2) is indeed from Foundry.  <br />ASIC/s on the VDX is 6th gen ASIC fron the FC side. So maybe the new 16 gbe FC will get the update too for ISL&#39;s (I would guess so).  <br /> <br />Now please remeber this for ISL (Inter Switch Links) in a single datacenter (al least at this point), therefor I would suggest that this is P2P phyical layer links only.  <br /> <br />Also the current MAX number of supported VDX switches can form a frabic is 12. However if you find a need to have a larger size fabric then the solution needs to be validated by Brocade (Read there is not a hard limit, but a supported limit - 12 has been tested and approved) This is up from the first release of 10 units.  <br /> <br />Hope this adds value.  <br /> <br />Michael.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4191723772636528116">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">FoundryDude</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4191723772636528116" href="#4191723772636528116">05 April 2011 16:08</a>
              </span>
            </div>
            <div class="comment-content">Quote &quot;You might wonder how Brocade, a company with historical focus on Fiber Channel, managed to solve one of the tough LAN networking problems almost a decade ago.&quot;<br /><br />Brocade didn&#39;t, Foundry Networks did, Foundry was only recently acquired by Brocade. Foundry has been quietly providing Enterprise quality Ethernet Networking equipment for years. We have used their equipment since 2002 and can attest to their technical achievements.<br /><br />FD</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3965764426381697689">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3965764426381697689" href="#3965764426381697689">05 April 2011 18:00</a>
              </span>
            </div>
            <div class="comment-content">Michael Schipp wrote &quot;ASIC/s on the VDX is 6th gen ASIC from the FC side.&quot; (see below). Brook Reams pointed me to Brocade patents when I was asking him about the packet distribution algorithms.<br /><br />Maybe it&#39;s time you guys get your stories straight.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5642589039769382375">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">plapukhov</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5642589039769382375" href="#5642589039769382375">05 April 2011 20:27</a>
              </span>
            </div>
            <div class="comment-content">Ivan, Michael,<br /><br />For the &quot;fat single flow&quot; example. Normally, endpoints connect at physical line rate that is the same or below that of the &quot;uplink&quot; port. Therefore, a typical *single* flow cannot completely overwhelm ISL link. Packet-level balancing, therefore, would be most efficient if implemented on ever inter-connection (host-switch, switch-switch, etc) to effectively increase single endpoint&#39;s transmission rate.<br /><br />Link aggregation (inverse multiplexing) has been always used in the case of over-subscription scenarios where N downstream ports send traffic to M upstream and N&gt;M. (compare this to circuit-switched network where over-subscription is not possible). This is how imuxing works in packet networks anyways. It&#39;s just different levels of granularity (packets, flows, etc) that you can use in packet networks, with deeper granularity required to optimize for sparse source/receiver topologies.<br /><br />One interesting inherent problem with packet networks is that they are always designed contrary to one of their original ideas, which was &quot;maximizing link utilization&quot;. PSNs are bound to be &quot;flow oriented&quot; due to upper level requirements and have to be over-provisioned to support QoS needs. One might think upper levels should have been designed to perform packet reordering in the network endpoints, but that never happened due to the fact that most ULPs have been &quot;adapted to&quot; and not &quot;designed for&quot; PSNs.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7927257317740279491">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Juan Tarrio Brocade</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7927257317740279491" href="#7927257317740279491">06 April 2011 01:46</a>
              </span>
            </div>
            <div class="comment-content">Sorry FoundryDude but you are mistaken. Brocade has had frame-level trunking in our Fibre Channel products since the launch of our first 2 Gbps FC switch in 2000! We are porting this technology now to the Ethernet space, just like we are porting many other fabric-related technologies into our Ethernet Fabric technology, VCS.<br /><br />No other vendor in the entire industry (in either Ethernet or Fibre Channel networks) has or has ever had this type of technology.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6079732800237716597">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Juan Tarrio Brocade</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6079732800237716597" href="#6079732800237716597">06 April 2011 02:03</a>
              </span>
            </div>
            <div class="comment-content">Michael, as I pointed out before, Brocade has had frame-based ISL trunking since our 2 Gbps FC products in 2000, so it&#39;s a given in our next generation 16 Gbps products.<br /><br />This is mainly meant for intra-datacenter ISLs between adjacent switches. Obviously, spraying frames across multiple links means you need to be very careful about in-order delivery, so there are some &quot;limitations&quot;. The ASIC controls the timing of the frames within port groups, so all ports belonging to the same frame-based trunk have to reside in the same port group. Initially we have 4-port groups and we could trunk 4 x 2 Gbps into a single 8 Gbps link. Today we support 8 x 8 Gbps links into a single 64 Gbps trunk with frame-level load balanding in Fibre Channel, and as you know, 8 x 10 Gbps in our VDX 6720 switches. BTW, we&#39;ve had frame-based trunking in Ethernet since we launched our Brocade 8000 top-of-rack FCoE switch, but there it&#39;s limited to 40 Gbps (4 x 10 Gbps).<br /><br />Another &quot;limitation&quot; is that the difference in cable lengths can&#39;t be too big, and that is the main reason this is *mostly* for intra-datacenter connections. But we do support frame-based trunking over long distance (at least in FC) up to hundreds of kms as long as you can guarantee the minimum cable length difference between the links (if all go over the same lambda and physical path, for example). We&#39;ve also had this for years and you can see it clearly documented in all of our product manuals.<br /><br />The benefits are very clear. If you trunk 8 x 10 Gbps links, you are *guaranteed* to be able to use those 80 Gbps of bandwidth, and you won&#39;t run into scenarios where one link is congested and you have spare bandwidth on another one, like it can happen with LAG (see http://packetattack.org/2010/11/27/the-scaling-limitations-of-etherchannel-or-why-11-does-not-equal-2/) and even in FC with other approaches (like exchange-based load balancing).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4624768727498047627">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Juan Tarrio Brocade</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4624768727498047627" href="#4624768727498047627">06 April 2011 02:10</a>
              </span>
            </div>
            <div class="comment-content">Plapukhov, what about when you have three 6 Gbps flows sharing two 10 Gbps ISLs? One flow will get on ISL, and there will be 4 Gbps of spare bandwidth. The other two flows will share a 10 Gbps ISL and will be limited to 5 Gbps each.<br /><br />With frame-based trunking, you are guaranteed to have enough bandwidth for those flows as long as the aggregate bandwidth of the flows is lower than the aggregate bandwidth of the ISLs, and in this case 3 x 6 Gbps = 18 Gbps &lt; 20 Gbps, so you wouldn&#39;t congest any of your flows.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="83778741563536805">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c83778741563536805" href="#83778741563536805">06 April 2011 15:59</a>
              </span>
            </div>
            <div class="comment-content">I guess what&#39;s Petr trying to say is, if you say one flow is limited to single port speed anyway, max. benefit you can get over per-flow balancing is ~17% (and your 2isl/3flow example is more or less best-case with 16.6% benefit) and it drops fast with more or less flows.<br /> <br />So the question is if speed increase for that particular # of flows is actually worth the extra complexity.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6471686553554840026">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Juan Tarrio Brocade</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6471686553554840026" href="#6471686553554840026">06 April 2011 20:42</a>
              </span>
            </div>
            <div class="comment-content">@Guest - That&#39;s the beauty of it. While the technology may seem (and be) complex in its hardware implementation details, for the end user it couldn&#39;t be simpler. Just connect the ports and the trunks form automatically, and you have [almost]perfect load balancing and fault tolerance. My example was just a small example to make it understood. The bottom line is that because one single flow can&#39;t exceed the capacity of one single ISL doesn&#39;t mean there isn&#39;t a benefit. Who doesn&#39;t want the extra bandwidth that is going to waste otherwise?<br /><br />I think the Ethernet world has been OK with wasted bandwidth for far too long, considering how we&#39;ve been living with STP for this long...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="355763506285235707">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c355763506285235707" href="#355763506285235707">06 April 2011 20:50</a>
              </span>
            </div>
            <div class="comment-content">Have to chime in on the STP part. We&#39;ve been willing to live with STP because we knew that there&#39;s place for bridging and there&#39;s place for routing (where you get to use all the bandwidth) ... and the bridging domains were usually small.<br /><br />Now that the hardware vendors have focused their persuasive powers onto server admins who don&#39;t understand that long-distance bridging is bad, we have to deal with the fact that STP was broken for the last few decades.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2138770390609406171">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Juan Tarrio Brocade</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2138770390609406171" href="#2138770390609406171">06 April 2011 20:55</a>
              </span>
            </div>
            <div class="comment-content">Fair point. STP has clearly served its purpose and it&#39;s been a very valuable technology. But as virtualization demands larger L2 domains it&#39;s time for fabric-based technologies to take over...  :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6409951046127511826">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Breams</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6409951046127511826" href="#6409951046127511826">07 April 2011 01:46</a>
              </span>
            </div>
            <div class="comment-content">Ivan and All the Folks Who Commented,  <br /> <br />Thanks to everyone for providing interesting comments, observations and follow-up questions to this post. I decided to put together more content on the subject of Brocade ISL Trunks and just added it to the Brocade community site on VCS Technology. You will find it here:  <br /> <br />http://community.brocade.com/community/brocadeblogs/vcs/blog/2011/04/06/brocade-isl-trunking-has-almost-perfect-load-balancing  <br /> <br />I think it provides more color on how we extended the original &quot;Brocade Trunking&quot; for Fibre channel, (sometimes referred to as &quot;frame trunking&quot; for obvious reasons) to create &quot;Brocade ISL Trunking&quot; which is included in a VCS Ethernet Fabric. I also provied some additional information at the end of my blog in response to some of the questions, comments and speculations several of you posted here.  <br /> <br />Ivan, as always, you provide sound informative content for the community.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6749959567984476986">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Blake</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6749959567984476986" href="#6749959567984476986">29 October 2011 10:40</a>
              </span>
            </div>
            <div class="comment-content">This load-balancing technique was developed by Foundry engineers before Brocade acquired them.  One of the major customers that drove the development was the AMS-IX, who was also a major reason for the existence of the 32 slot MLX/XMR chassis.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1907">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow"> Christian H</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1907" href="#1907">01 September 2023 06:03</a>
              </span>
            </div>
            <div class="comment-content"><p>I&#39;m curious how this plays out today? We&#39;re an R&amp;E w/ needs for better LB algorithms and Cisco themselves are telling us the higher throughput links simply use polynomials. Our testing of those polynomials has shown upwards of 15% loss at line rate across 4 x 10 Gigs, and higher at 3 x 100 Gigs.</p>
</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="1910">
          <!--
          <div class="avatar-image-container">
            <img src="">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1910" href="#1910">02 September 2023 08:47</a>
              </span>
            </div>
            <div class="comment-content"><p>The Brocade hardware is long gone. Most vendors use hash-based load balancing these days, and most of them have a nerd knob to turn on dynamic reshuffling. Obviously that works only on directly-connected egress links. Beyond that, Cisco ACI might be doing something, but most everyone else cannot as they don&#39;t have visibility into congestion beyond the egress interface.</p>

<p>The right way to solve this challenge is to implement uncongested path finding at the source host. Something as simple as FlowBender or MP-TCP could do the trick.</p>
</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
