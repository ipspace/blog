comments:
- date: 13 April 2011 10:59
  html: "&gt; Maybe, just maybe, there\u2019s a technology out there that\u2019s been\
    \ field-proven, known to scale, and works over switched Ethernet.<br /><br />802.1ah<br\
    \ /><br />&gt; would solve the scalability ones<br /><br />Check<br /><br />&gt;\
    \ breeze by the VLAN numbering barrier<br /><br />Check<br /><br />&gt; enable\
    \ instant inter-DC connectivity<br /><br />Check, can do over L2TPv3 or VPLS or\
    \ EoMPLS<br /><br />&gt; The full effect of the scalability improvements would\
    \ only become visible after deploying P2MP LSPs in the network core<br /><br />Don&#39;t\
    \ need that, as multicast is natively supported<br /><br />&gt; we won\u2019t\
    \ see either of these options<br /><br />ALU does that - that&#39;s their flavour\
    \ of &quot;DC fabric&quot;. S-VLAN per flexible configurable combination of selectors\
    \ (L2/L3/L4, if I&#39;m not mistaken), automagically mapped to a hypervisor that\
    \ needs it, when it needs is, across a single DC or a number of DCs."
  id: '7516952425288385800'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-13T10:59:38.068+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 11:00
  html: Grr, forgot to &quot;follow&quot; the stream again.
  id: '3125925521445435741'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-13T11:00:49.703+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 13:52
  html: After reading 802.1ah I got the impression that it maps broadcast+multicast
    in the C-component into multicast in the B-component. The standard does refer
    to potential use of MRRP (or static filters) but it&#39;s not mandatory.<br /><br
    />Am I missing something? Also, if ALU does all of that (and we know nothing about
    it), their marketing department deserves a &quot;kind nudge&quot;. Could you point
    me to relevant documents?<br /><br />Thank you!<br />Ivan
  id: '46087585677464817'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-13T13:52:18.168+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 17:05
  html: AMEN!!!
  id: '8303255769405520904'
  image: https://resources.blogblog.com/img/blank.gif
  name: Mike Courtney
  profile: null
  pub: '2011-04-13T17:05:49.469+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 18:47
  html: PBB and MPLS are both carrier originated technologies.<br />What is puzzling
    is the need by vendors to reinvent the wheel for DC.<br /><br />The vendors I
    talked to, were saying things like &quot;its too complex for user&quot;, &quot;the
    edge devices will not handle it&quot; (I suppose Ks of Vlans are OK :)).
  id: '8545089235260985008'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anton Yurchenko
  profile: null
  pub: '2011-04-13T18:47:01.782+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 18:50
  html: And IS-IS-based TRILL/FabricPath/VCS Fabric/OTV is not complex? The only difference
    is the complexity is hidden (which will only bite you when you have to do real
    in-depth troubleshooting).
  id: '6132324199585341644'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-13T18:50:28.968+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 19:09
  html: My point exactly :)<br />The more I read about those the less I understand
    the reasoning behind their introduction.<br />I thought that even PBB was needed
    to be brought to life(multicast?).
  id: '3834207327127955737'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anton Yurchenko
  profile: null
  pub: '2011-04-13T19:09:36.036+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 19:09
  html: My point exactly :)<br />The more I read about those the less I understand
    the reasoning behind their introduction.<br />I thought that even PBB was needed
    to be brought to life(multicast?).
  id: '928519297001434519'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anton Yurchenko
  profile: null
  pub: '2011-04-13T19:09:47.960+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 13 April 2011 23:45
  html: '&gt; into multicast in the B-component<br /><br />...which is then handled
    as if it was normal Ethernet multicast, right?<br /><br />&gt; their marketing
    department deserves a &quot;kind nudge&quot;<br /><br />In my experience, they
    rely on the existing relationships and trade shows, when you can see the product
    in action and speak with somebody who actually knows what they are talking about.<br
    /><br />Here&#39;s one link I found which has some marketing fluff on the subject:
    http://searchnetworking.techtarget.com/news/2240034476/Alcatel-Lucent-debuts-monster-data-center-switch-fabric-architecture'
  id: '4978103178390968003'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-13T23:45:33.652+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 00:19
  html: you are my hero
  id: '3730676424338475101'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2011-04-14T00:19:25.245+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 08:23
  html: '&gt;&gt; into multicast in the B-component  <br />&gt; ...which is then handled
    as if it was normal Ethernet multicast, right?<br /><br />Exactly my point. Normal
    multicast doesn&#39;t scale as it&#39;s flooded throughout the broadcast domain
    ... unless, of course, you use something like MRRP.<br /><br />On the other hand,
    VPLS (more so with P2MP LSP) scales better, as the multicasts get flooded only
    to those PE-routers that actually need them (with P2MP LSP on a source-rooted
    tree).'
  id: '5772654023002760520'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T08:23:06.070+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 10:03
  html: You didn&#39;t really answer my question though. Yes it could possibly function
    as an alternative when someone invests the time/money to develop it. Or do you
    have a way of fully automating this for in case a tenant wants to isolate his
    vApp on a separate network?<br /><br />I am all ears,
  id: '3644845288476292357'
  image: https://resources.blogblog.com/img/blank.gif
  name: Duncan
  profile: null
  pub: '2011-04-14T10:03:19.185+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 10:07
  html: '&gt; Normal multicast doesn&#39;t scale as it&#39;s flooded throughout the
    broadcast domain ... unless, of course, you use something like MRRP. <br /><br
    />Ok, I should have called things by their proper names - I should have said &quot;SPBM&quot;
    instead of &quot;802.1ah&quot;. My bad! :(<br /><br />I think what I wanted to
    say is that SPBM takes care of pruning/replication by building multicast topology
    based on SPF trees (http://www.nanog.org/meetings/nanog50/presentations/Sunday/IEEE_8021aqShortest_Path.pdf)'
  id: '8754146016979811362'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-14T10:07:09.864+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 10:26
  html: Well, if it is not vMware but say KVM hypervisor I&#39;d guess one can use
    openvswitch which actually implements Ethernet over IP, though admittedly it&#39;s
    not VPLS.
  id: '7628391815000436428'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-14T10:26:51.311+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 10:26
  html: Well, if it is not vMware but say KVM hypervisor I&#39;d guess one can use
    openvswitch which actually implements Ethernet over IP, though admittedly it&#39;s
    not VPLS.
  id: '2577779054970183911'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-14T10:26:57.772+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 10:38
  html: The way I understood the documentation, openvswitch implements P2P links with
    EtherIP, which obviously doesn&#39;t scale. Also, you would need plenty of orchestration
    to ensure links are established on-demand as VMs move around (which is what vCDNI
    does automatically).<br /><br />HOWEVER - there&#39;s OpenFlow  :-P We could still
    be pleasantly surprised.
  id: '4533008662754008686'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T10:38:34.265+02:00'
  ref: '6869770479657429957'
  type: comment
- comments:
  - date: 31 May 2012 16:09
    html: There seems to be some confusion here. SPBM scopes customer broadcast to
      the I-SID end points. It is not single broadcast domain. I have trouble figuring
      how n**2 replication of VPLS tunnels to the hypervisor could possibly scale
      better. Defies laws of physics.
    id: '3385062334905681525'
    image: https://resources.blogblog.com/img/blank.gif
    name: Dave Allan
    profile: null
    pub: '2012-05-31T16:09:05.838+02:00'
    ref: '4040071039879486849'
    type: comment
  date: 14 April 2011 10:40
  html: Ah, now I&#39;m in a familiar territory  8-)<br /><br />SPBM uses MAC-in-MAC
    encapsulation with I-SIDs to provide different forwarding mechanism in the SPBM
    core. However, the design paradigm is still a single broadcast domain (modulo
    VLAN pruning). Doesn&#39;t scale.
  id: '4040071039879486849'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T10:40:59.453+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 11:33
  html: '&gt; However, the design paradigm is still a single broadcast domain (modulo
    VLAN pruning). Doesn&#39;t scale.<br /><br />Could you please elaborate where
    do you see the scaling issue? Multicast traffic inside &quot;huge&quot; I-SIDs?'
  id: '2600315219199514019'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-14T11:33:06.401+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 11:40
  html: As I wrote in the vCDNI post, if a single VM goes bonkers, the whole DC is
    flooded. Same with SPBM.<br /><br />You won&#39;t see that in a typical SP/VPLS/802.1ah
    network because sensible people use routers to connect to the Carrier Ethernet
    service.
  id: '6179770129786637295'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T11:40:20.753+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 12:06
  html: '&gt; the whole DC is flooded<br /><br />Wouldn&#39;t flood scope be contained
    by the I-SID to which this VM is mapped? I thought that I-SID is a broadcast container.'
  id: '2421887087249964932'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-14T12:06:56.876+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 12:12
  html: '&gt;  I thought that I-SID is a broadcast container.<br /><br />Good question
    for a Packet Pushers Podcast we&#39;re doing next week. I don&#39;t think it is
    in SPBM.<br /><br />Also, in 802.1ah, multi/broadcasts are sent to default BA,
    which is multicast (unless you configure P2P service), so C-multicast becomes
    B-multicast.'
  id: '3584821836149990480'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T12:12:02.691+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 12:29
  html: Will look forward to the podcast :)
  id: '4911358553999168543'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-14T12:29:48.379+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 14:30
  html: A little surprised there is no mention of Q-in-Q in the article or comments...
  id: '2170255152876065707'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2011-04-14T14:30:33.637+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 14:33
  html: Q-in-Q doesn&#39;t help you. It&#39;s even worse than vCDNI - single MAC address
    space, single broadcast domain (unless you use VLAN pruning).
  id: '1870474363533287338'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T14:33:04.013+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 15:05
  html: Well there is the mpls-linux project os not all is lost.<br /><br />it&#39;d
    be interesting to see running mpls-linux with say VPLS &amp; KVM. I guess I need
    to kill couple week ends to see whether it&#39;s possible  :-[
  id: '2030223588809890908'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-14T15:05:00.923+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 15:07
  html: I don&#39;t understand how supporting SPB or whatever on physical device which
    is ALU is here going to fix lack of PE edge or other scalable Ethernet over IP
    transport in hypervisor ? Why even bring it into conversation unless ALU offers
    software implementation that can be instantiated as vswitch on hypervisors ?
  id: '8538099371651903603'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-14T15:07:26.795+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 15:11
  html: And ? How having or not having ALU is going to fix lack of scalable of Ethernet-over-IP
    transport on hypervisor ? <br /><br />Whole premise of Ivan&#39;s post is to get
    rid of physical L2 and do L3 all the way down (because L3 just scales way better
    than L2, probably why Internet is L3 =-X  ). Stopping at the physical network
    and leaving virtual network as is not really a solution. Or may be I&#39;m missing
    something.
  id: '189513884854718469'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-14T15:11:26.612+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 16:09
  html: Doesn&#39;t MPLS require manual configuration to bridge networks? How will
    that work?
  id: '4245471263657296341'
  image: https://resources.blogblog.com/img/blank.gif
  name: Guest
  profile: null
  pub: '2011-04-14T16:09:00.678+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 19:35
  html: mpls-linux seems to be dead (last change ~2 years ago) ... and it only had
    basic LDP and PE-router labeling functionality.
  id: '6287737803480662034'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T19:35:23.385+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 19:36
  html: Well, everything needs configuration (including vCDNI portgroups). The question
    is how much of the complexity you decide to hide behind a CLI/GUI.
  id: '6636383092464864476'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-14T19:36:05.647+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 22:20
  html: Ivan, <br /><br />Building a RFC2547 PE isn&#39;t the answer for Cloud networks
    because the goal is not a transport network, but a broadcast domain. Really what&#39;s
    needed in the Cloud is an emulation of what customers get when they do into a
    datacenter like Savvis or Equinix - you get a set of racks in a cage, and you
    uplink your firewall/router up to the provider&#39;s redundant set of links that
    goes to their access layer switching and below your router/firewall, you create
    a number of routed networks backed by VLANs, etc - simple stuff, right? Well,
    in the Cloud - folks want the same thing. Perimeter firewalling and access routing
    between multiple broadcast domains and some level of network hierarchy. One thing
    you will see this year is the trend of mapping broadcast domain to multicast.
    This is no longer only a VMware thing - you will be pleasantly surprised this
    year by other vendors.
  id: '1536671550954741436'
  image: https://resources.blogblog.com/img/blank.gif
  name: vSerge
  profile: null
  pub: '2011-04-14T22:20:08.699+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 14 April 2011 22:42
  html: I would appreciate if you could explain what is the need for a scalable transport
    on hypervisor? Ivan pointed our that a VM &quot;gone bonkers&quot; can produce
    broadcast or multicast flood. Ok, understand. But this can be dealt with - with
    BMU rate limiters and contained broadcast domains in DC network above hypervisor.
    What else is there that needs additional intelligence on a VM host?
  id: '186921957685957474'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-14T22:42:05.531+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 01:52
  html: I think Ivan post goes over that in 2nd paragraph. Assuming vSwitch is still
    L2 then its hand-off to physical network is still L2 802.1q trunk and you are
    still constrained by 2^12 VLAN ID space. Because if vSwitch L2, distributed vSwitch
    naturally would expect same VLAN ID across hypervisors for the same broadcast
    domain/VM L2 adjacency. How does mapping on ALU is going to extend VLAN ID space
    on trunk between vSwitch and physical switch. I am all ears.
  id: '1415911176957637728'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-15T01:52:44.776+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 02:17
  html: Ok, went to the source (802.1ak draft 3.6) - the way it reads to me, &quot;client-side&quot;
    BUM *is* constrained to its I-SID, which includes member SPBM PEs (which have
    ports with this I-SID) and selected shortest path internodal links for this given
    I-SID.<br /><br />So I don&#39;t think the whole DC will get flooded - only PEs
    and links which serve an I-SID with a misbehaving VM in it.<br /><br />And yes,
    the behaviour inside I-SID appears to be flooding, but with efficient replication
    at fork points.
  id: '7262302602980326345'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T02:17:49.748+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 02:18
  html: '&gt; 802.1ak<br /><br />I mean 802.1aq, of course.'
  id: '4528953827020058337'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T02:18:34.707+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 02:43
  html: "Second paragraph reads: &quot;If you need more than 1K VLANs, you\u2019ve\
    \ either totally messed up your network design or you\u2019re a service provider\
    \ offering multi-tenant services...&quot;. <br /><br />SPBM can support 2^24 &quot;VLANs&quot;,\
    \ and has no problem with supporting 4K regular VLAN-based services per port (read:\
    \ per ESX server). So with SPBM we are moving from &quot;4K VLANs per DC&quot;\
    \ to &quot;4K VLANs per ESX&quot;.<br /><br />Hope this makes sense."
  id: '1666525770097752170'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T02:43:37.369+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 05:38
  html: The more accurate statement would be 2^12 VLAN per virtual L2 domain that
    can have more than one ESX. Unless you can show me that with SPBM L2 domain can
    be shrank to single ESX/vSwitch and still provide L2 adjacency/vMotion/etc between
    different ESX hosts with non-matching VLAN IDs.<br /><br />So I can&#39;t quite
    see how you can have ESXes have independent VLAN ID spaces as long as there are
    L2 adjacency checks that rely on matching VLAN IDs between ESXes.
  id: '6621205019748186610'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-15T05:38:14.137+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 06:13
  html: '&gt; 2^12 VLAN per virtual L2 domain that can have more than one ESX<br /><br
    />There isn&#39;t &quot;an L2 domain with bunch of VLANs in it&quot;. May I recommend
    to check out the NANOG presentation I provided a link to above for how SPB solves
    these problems? Here&#39;s the link again: http://www.nanog.org/meetings/nanog50/presentations/Sunday/IEEE_8021aqShortest_Path.pdf
    - check slides 12, 16 and 17.'
  id: '3986199664071406357'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T06:13:51.324+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 06:47
  html: Thanks for the link, but I don&#39;t see how it address my point. it shows
    that C-VLAN is the same on both hyper visors, if that&#39;s not single L2 ID space
    then what that is ? And given that C-VLAN ID space must be same on all ESXes in
    the cluster then my statement stands.
  id: '8289710528214505125'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-15T06:47:21.402+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 07:11
  html: '&gt; C-VLAN ID space must be same on all ESXes in the cluster<br /><br />Correct.
    But you also can use this same C-VLAN ID space on a different ESX cluster within
    the same DC.<br /><br />How many VMs would you have running on a single ESX cluster?'
  id: '7615250380413666119'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T07:11:03.853+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 07:46
  html: Well, that&#39;s what my statement was about. I guess I worded it a bit vaguely.
    Virtual L2 domain in my not so clear definitions (that shares VLAN ID space) is
    the size of ESX cluster with current vSwitch. Obvious trend is for bigger ESX
    clusters. <br /><br />At about 64-128 (today is max st 32) x 4-socket systems
    IMHO we are going to hit VLAN ID space constraint. Even then I see nothing wrong
    with having plain vanilla physical L2 islands for each cluster at these sizes
    separated by L3.<br /><br />SPBM/etc really come to play when clusters get massive
    and at that stage VLAN ID space on hypervisors is going to be the constraint.<br
    /><br />IMHO for cloud SP, network edge must be pushed down to hypervisor, whether
    it&#39;s MPLS or SPBM is immaterial. The major theme of Ivan&#39;s post to me
    was that using dumbest possible network feature set in vSwitches is not going
    to get a cloud SP far enough. For SP like that vSwitches must have more than rudimentary
    features they have today.<br /><br />If ALU has a vSwitch appliance (doesn&#39;t
    matter whether it&#39;s for Xen/KVM/whatever) that can do SPBM I&#39;d be thrilled.
  id: '5969471862363172691'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-15T07:46:55.947+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 08:10
  html: 'Thanks for the feedback! What you mention is a crucial point: the cloud providers
    have to decide whether they want to offer L3 connectivity or L2 broadcast domains
    to their customers.<br /><br />If I would be a forward-looking provider, I would
    try to limit myself to L3 connectivity. Easier to design, build &amp; deploy ...
    plus it can service all greenfield applications and most existing stuff. I may
    be totally wrong, but looking from the outside it seems this is what Amazon is
    doing.<br /><br />If I want to capture 100% of the market, I obviously have to
    provide virtual L2 broadcast domains. The question I would have to ask myself
    is: is the incremental revenue I&#39;ll get by providing L2 broadcast domain big
    enough to justify the extra complexity? Based on what I see happening in the industry,
    I don&#39;t believe too many people have seriously considered this question.<br
    /><br />Obviously the networking industry is more than happy to support the &quot;virtual
    L2 domain&quot; approach, as it allows them to sell new technology and more boxes,
    which begs another question: &quot;who exactly is generating the needs?&quot;'
  id: '6090959652432531500'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T08:10:28.991+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 08:17
  html: '#1 - vDS is per vCenter, not per ESX cluster, so it (and its VLANs) can span
    1.000 hosts and 20.000 virtual ports (http://www.vmware.com/pdf/vsphere4/r41/vsp_41_config_max.pdf)<br
    /><br />#2 - we&#39;re already hitting the VLAN ID space constraint in VDI cloud
    applications like our FlipIT http://flipit.nil.com/ (large # of VDI VMs per ESX
    server, small # of VMs in a VLAN)<br /><br />#3 - You totally got my point (and
    rephrased it better than I could) - complexity belongs in vSwitch, optimal end-to-end
    transport into the physical boxes.'
  id: '1844308892085204530'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T08:17:44.617+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 08:46
  html: 'Understand now, thanks! I&#39;m not too good with VMware, so please excuse
    me if the next question is stupid: does ESX have single VLAN ID space across all
    its vSwitch instances? Say an ESX host has two vSwitch instances, each linked
    to a separate pNIC. If you configure a vNIC on vSwitch 1 with VLAN ID 10, and
    then another vNIC on vSwitch 2 with the same VLAN ID, what is ESX / vSphere going
    to make of it? Will it expect that the VLAN 10 is bridged outside the ESX and
    treat both vNICs as members of a same port group?<br /><br />And question 2: what
    about VEPA? Doesn&#39;t it &quot;extend&quot; vNICs past vSwitch to an external
    actual real switch (which could be running SPBM)?'
  id: '8997006072774556304'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T08:46:58.690+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 08:55
  html: '#1 - in my understanding, the VLAN ID space is tied to physical NICs. Outbound
    a portgroup does VLAN tagging (assuming it&#39;s configured that way), inbound
    the packets are only delivered to a portgroup if they arrive through the pNIC
    to which the portgroup is attached.<br /><br />#2 - I don&#39;t think VEPA would
    help you here and it definitely does NOT extend vNICs (that would be 802.1Qbh).
    More about it in a week or so.'
  id: '859190960570129918'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T08:55:16.929+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 08:55
  html: '#1 - in my understanding, the VLAN ID space is tied to physical NICs. Outbound
    a portgroup does VLAN tagging (assuming it&#39;s configured that way), inbound
    the packets are only delivered to a portgroup if they arrive through the pNIC
    to which the portgroup is attached.<br /><br />#2 - I don&#39;t think VEPA would
    help you here and it definitely does NOT extend vNICs (that would be 802.1Qbh).
    More about it in a week or so.'
  id: '1946604690243605292'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T08:55:29.958+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 09:57
  html: 1) ESX will not care whether VLAN 10 is disjointed unless you put ESX control
    plane traffic on it (Service Console, VMKernel, FT). But even for user VM VLAN
    ESX will show in that cute diagram (showing connectivity to datastores &amp; port-groups)
    in vCenter that these two ESX hosts in the same cluster are on the same VLAN so
    human operator is likely to assume that this not a disjointed VLAN. So implicitly
    ESX does make an assumption.<br /><br />I haven&#39;t played much with vDS but
    I expect vDS to have stronger assumptions if the same vnics are on the same VLAN
    ID. Not sure if it checks VLANs for disjointedness. <br /><br />2) VEPA/VN-tag/whatever
    (as usual there are competing tracks) that tries to move out virtual networking
    out of hypervisor to physical network box is certainly a valid approach but it
    requires hypervisor element manager (aka vCenter for ESX) to manage physical network
    device now to set up/tear down VM related network plumbing when VM moves in/moves
    off the host. I guess this could be another application for openflow  =-X but
    I guess either way hypervisor element manager will acquire some advanced network
    control-plane features to manage either virtual or physical switch box.
  id: '9097543887269769232'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-15T09:57:46.489+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 10:12
  html: '#1: If this is so, aren&#39;t we sorted out then? Just create multiple vDS,
    each with its 4K VLANs, and tie each vSwitch to a corresponding pNIC. Should be
    especially easy with UCS equipped with VICs. Then plug each pNIC into an SPBM
    switch, which can map either whole port or port/VLAN to an individual I-SID. Viola:
    multiple 4K VLAN domains per ESX, no?<br /><br />#2: After reading up on VEPA
    I see that yes, it won&#39;t help, but it won&#39;t need to, provided #1 is true.'
  id: '4562755554352726868'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T10:12:05.658+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 10:36
  html: 'Right, a bit more thinking now: in my scenario with two vSwitch instances
    on the same ESX, where each vSwitch is attached to its own pNIC uplink connection.<br
    /><br />On the vSwitch 1 we create a port group we call &quot;Network A&quot;,
    and associate it with VLAN 10.<br />On the vSwitch 2 we create a port group we
    call &quot;Network B&quot;, and also associate it with VLAN 10.<br /><br />Now,
    vCenter knows port groups by their network names, *not* by their VLAN IDs, correct?
    If so, it has no right to make an assumption that VLAN ID 10 on vSwitch 1&#39;s
    uplink is the same broadcast domain as VLAN ID 10 on vSwitch 2&#39;s uplink, correct?
    If so, we&#39;re all set! :)'
  id: '1060223763327672757'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T10:36:12.866+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 10:40
  html: '... until a device, link or pNIC breaks in the middle of the night and the
    late shift operator assigns the port group to different pNICs trying to restore
    connectivity  :-E<br /><br />Never rely on overly complex designs, they will break
    at the most inappropriate moment.'
  id: '237008453470441047'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T10:40:54.388+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 10:52
  html: BTW each vSwitch is likely use 2 x pNICs for redundancy so we will be burning
    switch ports just because we need more VLAN IDs per host though it&#39;s probably
    good for network device vendors =-X<br /><br />Well, I guess that&#39;s a clever
    way we can prolong the misery a bit by adding more and more pNICs  =-X but in
    the end isn&#39;t easier to do the right thing  =-X
  id: '6310143811943689352'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ulan Mamytov
  profile: null
  pub: '2011-04-15T10:52:51.927+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 11:04
  html: Wouldn&#39;t there be multiple (teamed) pNICs, just as they are today? And
    yes, agree with the complexity and breaking...
  id: '2095904805631908808'
  image: https://resources.blogblog.com/img/blank.gif
  name: Dmitri Kalintsev
  profile: null
  pub: '2011-04-15T11:04:12.729+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 15:33
  html: '&gt; If I would be a forward-looking provider, I would try to limit myself
    to L3 connectivity. Easier to     &gt; design, build &amp; deploy ... plus it
    can service all greenfield applications and most existing stuff. I &gt;may be
    totally wrong, but looking from the outside it seems this is what Amazon is doing.<br
    /><br />I thought Amazon started to do just that (i.e. offering dedicated layer
    2 domains). :<br /><br />http://aws.typepad.com/aws/2011/03/new-approach-amazon-ec2-networking.html<br
    /><br />&gt; Obviously the networking industry is more than happy to support the
    &quot;virtual L2 domain&quot; approach, &gt;it allows them to sell new technology
    and more boxes, which begs another question: &quot;who exactly is &gt;generating
    the needs?&quot;<br /><br />We don&#39;t want to sell any box at VMware ... yet
    we want the world to be better ;) <br /><br />Keep up with your posts... it helps
    us to stay on track. <br /><br />Massimo.'
  id: '4340722026255433758'
  image: https://resources.blogblog.com/img/blank.gif
  name: Massimo Re Ferre'
  profile: null
  pub: '2011-04-15T15:33:24.915+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 15:46
  html: Someone smart (like Amazon) would be able to do everything they publicly promise
    with smart access lists (like multi-tenant vShield App or Cisco&#39;s VSG).<br
    /><br />Trying to find someone to answer a few basic questions like:<br /><br
    />* Would IP multicast work?<br />* Would subnet-level broadcast work?<br />*
    Can I run Microsoft NLB or Microsoft cluster over it?<br />* Can I run non-IP
    protocols (like IPv6 or CLNP) between my EC2 instances?<br /><br />Will keep you
    updated if I ever get the answers ;)<br /><br />As for the industry classification
    - I thought you were the leader in the virtualization industry  8-)
  id: '377552735642722518'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T15:46:41.572+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 16:35
  html: I think that yes, you could do that (technically). I believe however that
    it&#39;s easier to sell to two &quot;orgs&quot; that they are going to be hosted
    on two different broadcast domains (separated by &quot;traditional&quot; firewall
    architectures - although virtual and not physical) than selling to two &quot;orgs&quot;
    that they are hosted on a single broadcast domain but are kept separated by a
    &quot;firewall filter driver running on the hosts&quot;. Don&#39;t get me wrong,
    I am not saying it&#39;s less secure, I am saying that it&#39;s more difficult
    to sell (in 2011). People out there tend to be conservative (and have political
    agendas). They will make a step at a time. If you ask me vShield App (or similar
    technologies) are even more compelling than VCDNI or any other layer 2 virtualization
    technology we may come out with... I found interesting that you jumped from MPLS
    straight to APP/VSG. <br /><br />Massimo.
  id: '1608112201820433910'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2011-04-15T16:35:47.339+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 16:40
  html: 'BTW: <br /><br />&gt; Can I run Microsoft NLB or Microsoft cluster over it?  <br
    /><br />if you want to run NLB or MSCS in the cloud.... I think you are not ready
    for the cloud. Me think.<br /><br />Massimo.'
  id: '4383813621763052109'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2011-04-15T16:40:02.135+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 17:05
  html: Me agree. Wholeheartedly. Probably not everyone does  :-E
  id: '1956133812376010792'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T17:05:54.919+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 17:34
  html: '&gt; Me agree. Wholeheartedly. Probably not everyone does<br /><br />Let
    them crash then. Let them learn the hard way.<br /><br />Massimo.'
  id: '1942465619872794026'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2011-04-15T17:34:14.811+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 18:52
  html: That&#39;s actually a great way to look at it - if you&#39;re a service provider
    interested in getting parity with Amazon capabilities (EC2 specifically, as we
    know that VPC does support bcast), then the approach needs to be equivalent in
    capabilities where VMs only communicate via L3, today accomplished via filtering
    (ebtables/iptables in case of Xen Clouds). If you&#39;re looking to build enteprise-like
    Cloud where clustering apps which rely on broadcast/L2multicast or simply want
    all your legacy apps to work like many of the Windows services which rely on broadcast
    - then you&#39;d want something either built via VLANs or the next-gen encapsulation
    alternatives. In the encapsulation approaches, it&#39;s key to provide solid debugging
    and tracing capabilities - from Wireshark plugins to decode the new protocol and
    tools that can help troubleshoot the topology. <br /><br />One of the benefits
    of keeping the notions that exist today in physical networks like zoning that&#39;s
    based on subnet + underlying bcast domain is that allows for easier migration
    of apps into the Cloud space. Vision is something like this - p-to-V your workloads,
    then p-to-V your network/security config (or something close to it), retain the
    same address space or renumber (if need be) and either via VPN or private connectivity
    to your provider - upload your apps and netsec config which stands up the services
    in the Cloud... It sounds a bit futuristic, but I have some REST scripts that
    I share which stand up multiple bcast domains, sets up access routing between
    them, sets up perimeter firewall w/ NAT towards the Internet, and some static
    routes pointing to the provider PE router for the MPLS VPN without NAT and then
    pushes the VMs in these containers and numbers them... <br /><br />Can I say that
    REST is becoming the new CLI for Cloud? I used to prefer Cisco IOS/CatOs until
    I played with my first Juniper in &#39;99 (Olive x86 box running JunOS - before
    Juniper had the hardware ready), fell in love with that Gated-style CLI, now it&#39;s
    REST... Maybe I&#39;m a single voice in the wilderness or maybe we&#39;ll become
    a new breed of network engineers?
  id: '3812088576243724825'
  image: https://resources.blogblog.com/img/blank.gif
  name: vSerge
  profile: null
  pub: '2011-04-15T18:52:56.338+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 19:49
  html: Amazon VPC does NOT support multicast or broadcast:<br /><br />http://aws.amazon.com/vpc/faqs/#R4
  id: '1467245659647119755'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T19:49:24.383+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 19:50
  html: Willing to tell me more about the REST functionality you&#39;re talking about?
  id: '5418138968911146019'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-04-15T19:50:22.641+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 22:12
  html: 'Thanks for the correction - here is a thread asking Amazon folks to implement
    broadcast/multicast functionality, a good read: <br /><br />https://forums.aws.amazon.com/thread.jspa?threadID=37972'
  id: '8227996132515692654'
  image: https://resources.blogblog.com/img/blank.gif
  name: vSerge
  profile: null
  pub: '2011-04-15T22:12:51.687+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 15 April 2011 22:15
  html: 'Yep - let me know which areas to expand upon or you can contact me privately
    here: sergey at vmware dot com.'
  id: '6664651527414971684'
  image: https://resources.blogblog.com/img/blank.gif
  name: vSerge
  profile: null
  pub: '2011-04-15T22:15:26.506+02:00'
  ref: '6869770479657429957'
  type: comment
- date: 12 May 2011 19:23
  html: Agree 100% with this article.
  id: '9093616582339104154'
  image: https://resources.blogblog.com/img/blank.gif
  name: Derick Winkworth
  profile: null
  pub: '2011-05-12T19:23:25.917+02:00'
  ref: '6869770479657429957'
  type: comment
- comments:
  - date: 01 December 2012 10:34
    html: Yes, I can.
    id: '8485878168336498616'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Ivan Pepelnjak
    profile: https://www.blogger.com/profile/13457151406311272386
    pub: '2012-12-01T10:34:26.344+01:00'
    ref: '8605567925129751751'
    type: comment
  - date: 29 December 2012 02:34
    html: 'Thank you. I am eagerly waiting for your comments. While you are at it,
      can you please look at: http://tools.ietf.org/html/draft-smith-lisp-layer2-01
      too?'
    id: '4132924705893477663'
    image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
    name: Unknown
    profile: https://www.blogger.com/profile/14893658048554590916
    pub: '2012-12-29T02:34:06.358+01:00'
    ref: '8605567925129751751'
    type: comment
  date: 01 December 2012 07:19
  html: Ivan, Can you please offer your comments on these drafts?<br /><br />http://tools.ietf.org/html/draft-ietf-l3vpn-end-system-00<br
    />https://datatracker.ietf.org/doc/draft-rfernando-virt-topo-bgp-vpn/?include_text=1<br
    />http://tools.ietf.org/html/draft-ietf-l2vpn-evpn-02<br />
  id: '8605567925129751751'
  image: https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35
  name: Unknown
  profile: https://www.blogger.com/profile/14893658048554590916
  pub: '2012-12-01T07:19:46.207+01:00'
  ref: '6869770479657429957'
  type: comment
count: 68
id: '6869770479657429957'
type: post
url: 2011/04/vcloud-architects-ever-heard-of-mpls.html
