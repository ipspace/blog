{
  "comments": [
    {
      "date": "09 February 2011 14:12",
      "html": "I&#39;m always interested in a better network design.  What would you recommend that still achieves server portability?",
      "id": "5381511150258904229",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "lcg",
      "profile": null,
      "pub": "2011-02-09T14:12:20.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 14:46",
      "html": "As I wrote - there is nothing, because nobody was working on this problem for the last 5+ years.<br /><br />LISP in Nexus 1000V might be the answer, but I don&#39;t like the extra layer of encapsulation it introduces.",
      "id": "995690887920511518",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-09T14:46:53.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 14:49",
      "html": "I must admit that the scale at which I am thinking is in the range of dozens of servers at most, but what is wrong with visualizing the first hop router? For every workload that needs mobility for outside of an ethernet domain, virtualize the default gateway.<br /><br />Your gateway would need VLAN interfaces for the VMs that it routes for, and an interface for some kind of &#39;OSPF adjacency&#39; VLAN in each datacenter. As your workload migrates from one datacenter (or cluster) to another one, once the virtualized router is migrated OSPF adjacencies are formed on the appropriate OSPF VLAN, and new routes to the workload are propagated to the network. Keep in mind that when your router is in one cluster or DC and your workload is in the other then no traffic flows.<br /><br />Problems:<br /> How do you keep your virtual router from advertising the routes for datacenter OSPF networks that it is not connected to? How does this scale beyond a workload that needs 1 Gbps of network throughput? How do you get access to the storage? Does VMware SRM take care of the second two?",
      "id": "5416508256698285708",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anon2",
      "profile": null,
      "pub": "2011-02-09T14:49:24.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 15:10",
      "html": "Then I&#39;m not sure I understand the issue.  If there are no alternatives, then we&#39;re doing the best we can with the technologies available to implement desired capabilities that have immediate benefits.  Should we do nothing simply because it&#39;s not optimal, or because at some future time it will no longer scale under a set of particular assumptions?",
      "id": "5818208100224366525",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "lcg",
      "profile": null,
      "pub": "2011-02-09T15:10:10.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 15:59",
      "html": "Solutions do exist: you can use load balancers or (even better) a more optimized application architecture. <br /><br />But all you&#39;re willing to do is &quot;move this VM to the other end of the world&quot;, then we have a problem ;) LISP can solve it, but (as I said) introduces yet another layer of encapsulation.",
      "id": "2534666075205317561",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-09T15:59:25.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 16:05",
      "html": "Load balancers require more integration with the application install and configuration.  That is much harder and more time consuming, and increases the ongoing operational maintenance activities (more servers = more work).  And not all applications can support it.  Again, we&#39;re dealing with the capabilities available now, not what we wish we had.",
      "id": "4811014296166538007",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "lcg",
      "profile": null,
      "pub": "2011-02-09T16:05:54.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 16:28",
      "html": "It also increases costs because you have to license the additional server OS and application, as well as the load balancer.",
      "id": "8609650517217992361",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "lcg",
      "profile": null,
      "pub": "2011-02-09T16:28:32.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 17:36",
      "html": "Ivan, the problem of clusters, security and other interesting stuff almost always can be solved with a good design in the application layer, and a little help from operating system, network &amp; storage.<br /><br />Nowadays, most applications  have bad designs and seek for a *a lot* of help from the operating system, network &amp; storage.<br /><br />The fact that most engineers in the last 10+ years do not have a broad view of the above areas, has led to the problems that you describe.<br /><br />It&#39;s the application (&amp; protocol) design that needs to be fixed!<br /><br />ps. Tell to the server admin to configure the application to perform all the network stuff from a loopback address and you can take care of the rest easily.",
      "id": "2323914923086232254",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "John",
      "profile": null,
      "pub": "2011-02-09T17:36:46.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "09 February 2011 18:49",
      "html": "I always thought that LAM failed because IOS wasn&#39;t able to hold enough routes in memory. The idea that /32 routes could exist in large volumes and be constantly updated meant the Cisco hardware was incapable of scaling to sufficient size.  (see TCAM, Memory, and undersized CPUs). <br /><br />Is that your view as well ?",
      "id": "5067675096924143599",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Etherealmind",
      "profile": null,
      "pub": "2011-02-09T18:49:49.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "10 February 2011 03:05",
      "html": "Nicely timed post from Brad Hedlund...<br /><br />http://bradhedlund.com/2011/02/09/emergence-of-the-massively-scalable-data-center/",
      "id": "3298511912624462772",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "lcg",
      "profile": null,
      "pub": "2011-02-10T03:05:30.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "10 February 2011 04:14",
      "html": "&gt; ps. Tell to the server admin to configure the application to perform all the network <br />&gt; stuff from a loopback address and you can take care of the rest easily.<br /><br />I&#39;ll second that. So often network engineering is an effort to solve problems of crappy application development and/or server deployment practices that could be easily fixed if app/server people could think just a little bit outside of their domain.",
      "id": "8131633358684392540",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ulan Mamytov",
      "profile": null,
      "pub": "2011-02-10T04:14:37.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "10 February 2011 08:04",
      "html": "Absolutely. Love it. Did you notice how both proposals he finds sensible modify existing L2 or L3 behavior? Proves my point: we&#39;ve been too complacent for too long.",
      "id": "4781358403997024980",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-10T08:04:27.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "10 February 2011 14:06",
      "html": "How does Cisco&#39;s OTV play into this scenario?",
      "id": "2416478260705148861",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "chris j",
      "profile": null,
      "pub": "2011-02-10T14:06:09.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "10 February 2011 15:23",
      "html": "Of course it wasn&#39;t *that* long ago that server OSes shipped with a routing daemon running by default (Solaris &amp; gated anyone?) and could advertise a /32 route (my employer still has that for one legacy &quot;cluster&quot;).<br /><br />Still doesn&#39;t scale nicely of course!  :)",
      "id": "862794636156717660",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Jon Still",
      "profile": null,
      "pub": "2011-02-10T15:23:05.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "10 February 2011 17:27",
      "html": "IMHO, the whole idea behind what vMotion thinks it is trying to solve is suboptimal. If a service is so critical that one needs very high availability, put a load balancer in front of those servers. Requiring the network to be overly complicated or potentially unstable across an entire datacenter seems pretty crazy.<br /><br />Believe me when I say that I do not love load balancers. They are temperamental and expensive black boxes. They do provide a much more scalable solution than vMotion. My biggest problem with vMotion is that it allows application owners to be lazy. They will develop their software assuming that the network will always allow them to shift their services around. It won&#39;t challenge them to think about how to scale their service by orders of magnitude.<br /><br />There are all sorts of &quot;great&quot; protocols out there, like otv, that allow network engineers to come up with &quot;creative solutions&quot; to suboptimal service requirements. IMHO",
      "id": "3148957173381113964",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Peter John Hill",
      "profile": null,
      "pub": "2011-02-10T17:27:17.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "11 February 2011 08:56",
      "html": "LAM sucks.  That&#39;s why it didn&#39;t succeed.  How can something based upon ARP/RARP be considered a viable solution?<br /><br />The meta-issue here isn&#39;t layer-2 vs. layer-3, it&#39;s a) the overloading of the IPv4 (and now IPv6) addresses with locator/EID information, b) the policy overloading of IPv4 (and now IPv6) addresses with policy information via ACLs, firewall rules, et. al., and c) the continued worst practice of application developers further overloading IPv4 (and now IPv6) addresses by directly hardcoding IP addresses into their applications/platforms/services, instead of abstracting this away via a naming service (e.g., DNS, at least for now).",
      "id": "3180933601556719747",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Roland Dobbins",
      "profile": null,
      "pub": "2011-02-11T08:56:26.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "11 February 2011 10:46",
      "html": "OTV is a slightly better bridging. Still doesn&#39;t scale (although at least they got rid of unknown unicast flooding).",
      "id": "3551672886171020582",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-11T10:46:59.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "11 February 2011 10:48",
      "html": "vMotion is not necessarily solving high-availability issues. It also provides load distribution / adjustment capabilities.<br /><br />As for the appdev laziness, I couldn&#39;t agree more with you ;)",
      "id": "8209566245045803156",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-11T10:48:40.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "11 February 2011 10:51",
      "html": "I absolutely agree with everything you wrote. However, you&#39;re missing an important point: live VM migration for load distribution purposes. It would be tough to implement persistent connections in that scenario ... unless we would have a robust session layer that would survive transport layer failures (and a very quick failure detection mechanism).",
      "id": "2618387539010974267",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-11T10:51:32.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "15 February 2011 21:22",
      "html": "We&#39;re not waiting anymore.  We bought L2 service from a pair of national L2 service providers (their VPLS service) with large MTUs (4400 bytes) at all data centers and we are moving forward with VPLS.  Its there, its vendor interoperable.  It can be a pain in the ass, but it works.  We already have MPLS in our WAN core so it seems like the natural thing to do.",
      "id": "1609074720892247198",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "ccie15672",
      "profile": null,
      "pub": "2011-02-15T21:22:28.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "15 February 2011 21:56",
      "html": "Will you do L2 or L3 DCI? If you go for L2, what technology will you use? Just bridging with STP over (provider-delivered) VPLS or something fancier?",
      "id": "8672833181682628091",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-15T21:56:34.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "16 February 2011 03:17",
      "html": "Sorry for being ambiguous..  <br /><br />We&#39;re buying large MTU CoS-enabled multi-point VPLS from two different providers (for redundancy).  Over top of this service, we run our own MPLS infrastructure.  We terminate the access circuits on P nodes that we manage.  Looking at the header of a packet in one of the two service-provider&#39;s networks you would see  something like ETH|MPLS|MPLS|ETH|MPLS|MPLS|ETH.    With the rightmost ethernet header being one of our customers (internal or external), the left two MPLS tags being our own VPLS and transport tags, the middle ethernet header belonging to one of our own P nodes... then everything to the right of that belonging to one of the two service providers (and therefore invisible to us).<br /><br />So we are running our own L3VPN and L2VPN/VPLS services for one of many internal L2 or L3 networks.  Think many lines of business each with their own web tier, app tier, storage tier, etc...  Some components are shared, many are not.  Multiple network teams with a fair degree of autonomy (because business units themselves are marketable things that can broken off and sold whole).  Even some of the web tiers within individual lines of business are so large they are broken into multiple logically isolated networks.  <br /><br />We have piles of L2 and L3 DCI requirements.  We have multiple vendors in the network and server spaces, so we looked at the problem as if we were a service-provider ourselves and decided to turn our network into a service rather than what it is/was... which was many parallel circuits poorly utilized owned by different groups and in total costing us outrageous amounts of money.  <br /><br />Which is more info than what you were looking for.  From my team&#39;s perspective OTV would be something that one or some of these lines-of-business might buy into and we would provide a multipoint VPLS service in support of that.  I think we are a pretty good case study on why its just wrong to compare OTV and VPLS like they are competing technologies.",
      "id": "9078882725914769905",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "ccie15672",
      "profile": null,
      "pub": "2011-02-16T03:17:03.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    },
    {
      "date": "16 February 2011 07:29",
      "html": "Thanks for an extensive answer. You&#39;re doing exactly what I would recommend someone to do (which is nice to see; seems like I&#39;m too far off the mark ;)<br /><br />I was trying to figure out how someone would use SP-delivered VPLS service for L2 DCI and the only viable use I could see was to turn it into an IP(+MPLS) subnet, which is what you did.",
      "id": "2491205779809797132",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-02-16T07:29:55.000+01:00",
      "ref": "6663754495548952529",
      "type": "comment"
    }
  ],
  "count": 23,
  "id": "6663754495548952529",
  "type": "post",
  "url": "2011/02/layer-3-gurus-asleep-at-wheel.html"
}