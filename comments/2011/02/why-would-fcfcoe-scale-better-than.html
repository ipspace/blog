<div class="comments post" id="comments">
  <h4>11 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="4149173238334577543">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">PG</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4149173238334577543" href="#4149173238334577543">15 February 2011 12:47</a>
              </span>
            </div>
            <div class="comment-content">I see no reason it can&#39;t scale to thousands of nodes. Especially if you built a completely separate network just for your iSCSI storage traffic.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7629030952496586754">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7629030952496586754" href="#7629030952496586754">15 February 2011 13:30</a>
              </span>
            </div>
            <div class="comment-content">I don&#39;t think you need a separate network. ETS (WDRR) + separate CoS class for iSCSI should be enough. Plus you might want to limit the size of your bridged domains ;)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5305394711101600732">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Stuart Miniman</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5305394711101600732" href="#5305394711101600732">15 February 2011 15:14</a>
              </span>
            </div>
            <div class="comment-content">Ivan,<br />Theoretically iSCSI can scale, but in practice, the management tools have not been put in place. iSCSI is configured on a host level, while FC can be centrally managed. The ecosystem for iSCSI is centered around Microsoft hosts (iSCSI initiator) with storage from Dell (EqualLogic), HP (LeftHand), NetApp and EMC (CLARiiON now VNX) - all of these are midrange products that are not targeted for larger configurations. In talking with many of these vendors, the average configuration for iSCSI tends to be around 20 hosts and it was very rare to find a customer that had deployed 100 servers in a single network. Once again, there is no architectural limitation, but from an operations standpoint it is prohibitive and from what I have seen, no company is putting together the tools to allow simple deployment and management of large scale iSCSI (would probably want to further develop around iSNS). The average FC switch is larger than the typical iSCSI environment, with edge switches going to 80 or more ports and directors to hundreds of ports.<br />Cheers,<br />@stu</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3786988538142959827">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">PG</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3786988538142959827" href="#3786988538142959827">16 February 2011 00:19</a>
              </span>
            </div>
            <div class="comment-content">I agree, was just comparing apples with apples, since FC is a completely separate network  :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4971379570909197225">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">jopo</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4971379570909197225" href="#4971379570909197225">16 February 2011 02:05</a>
              </span>
            </div>
            <div class="comment-content">I like to think of the services that FC provides as the Active Directory of storage.   If you have only have 1 storage array and 25 hosts, Zoning, Masking, are not that big of a deal.  Fundamenally though, storage is growing a 1.6x compounded per annum.  Even if you dedup you are only buying yourself limited time until you have a serious disk problem.<br /><br />iSCSI scales the same way that creating user account on everyone desktop scales  .  . . n^((n-1)/2)*.  You need a centralized AAA, TE and security mechanism for block disc access.  FC provides that.<br /><br />I have customers who in 18 months have hit the wall on iSCSI deployments cause the disc outgrew the network too fast.  Like the early days of VoIP.<br /><br /><br /> * I haven&#39;t actually proven that math, but, could do so for a small donation of a bottle of Rye.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7757641441536156025">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Milen Trifonov</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7757641441536156025" href="#7757641441536156025">16 February 2011 13:09</a>
              </span>
            </div>
            <div class="comment-content">AFAIK  Amazon uses iSCSI in their AWS, this is a very large deployment with tens of thousand servers, so obviously iSCSI can scale.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="802858495886569974">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Stuart Miniman</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c802858495886569974" href="#802858495886569974">16 February 2011 15:18</a>
              </span>
            </div>
            <div class="comment-content">Milen - care to share a source of your data? Amazon and Google do not use traditional storage arrays. My understanding is that they use commodity hardware with the disks inside the servers (DAS). AWS EBS may support iSCSI, but that does not translate to Amazon being a single network of thousands of iSCSI nodes.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2163277270971596994">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Dave</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2163277270971596994" href="#2163277270971596994">19 February 2011 05:35</a>
              </span>
            </div>
            <div class="comment-content">The reason I setup iSCSI as a separate network is it performs much better with flow control and/or jumbo frames (and preferably, or if some piece along the way can&#39;t do both at once).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5952626013610158894">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Pete Welcher</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5952626013610158894" href="#5952626013610158894">07 March 2011 02:22</a>
              </span>
            </div>
            <div class="comment-content">I like the comments already posted about management. <br /><br />I think it&#39;s also about control. SAN and FC people like being separate, so that nothing messes up the SAN. That seems to me to be the source of most of the resistance to FCoE. What, share a cable?! How could one possibly troubleshoot in, gasp, someone else&#39;s box?! (And the network guys meanwhile worry about FCIP killing their shared WAN link?)<br /><br />I gather iSCSI is gatewayed. OTOH, in a sense the FCoE FCF forwarding to native FC is too. OTOH, the latter is arguably simpler (strip or add L2 header). <br /><br />Some people argue that iSCSI is higher overhead. I&#39;d think that depends on the NIC. <br /><br />Since iSCSI shares a wire with other network traffic, what do you do for QoS? It&#39;s the worst case, need it there ASAP but there&#39;s a LOT of it. And if it goes lossy on you, or duplicates packets, FC and related SCSI protocols don&#39;t generally handle that at all well.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2608883746468018953">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2608883746468018953" href="#2608883746468018953">07 March 2011 16:58</a>
              </span>
            </div>
            <div class="comment-content">As far as I understand iSCSI, you&#39;d use direct iSCSI session in a usual server-to-storage scenario (no gateway). iSCSI is an application on top of TCP, so duplicate/lost/corrupted packets never reach the SCSI level (you might have performance issues, but that&#39;s a different story). FC(oE)? obviously can&#39;t cope with packet loss, so you have to make FCoE truly lossless.<br /><br />If you have DCB-enabled switches, it would be best to use separate 802.1p priority value for iSCSI traffic, make it lossless (with PFC) and allocate it a fixed bandwidth percentage (with ETS/WDRR).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8471429649847601732">
          <!--
          <div class="avatar-image-container">
            <img src="https://resources.blogblog.com/img/blank.gif">
          </div>
          -->
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="None" rel="nofollow">Etherealmind</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8471429649847601732" href="#8471429649847601732">08 March 2011 14:46</a>
              </span>
            </div>
            <div class="comment-content">A separate network has nothing to do with scale, in fact it&#39;s make scale worse since dedicated resources are necessary. <br /><br />However, a dedicated network makes it deterministic, that is, performance can be determined absolutely in terms of storage traffic parameters. Note that you still cannot guarantee delivery unless you oversubscribe every element of the network..... which is what FC does at great expense to the customer.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
