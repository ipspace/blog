<div class="comments post" id="comments">
  <h4>28 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="5954352233798592577">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Rob turner</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5954352233798592577" href="#5954352233798592577">20 September 2011 11:30</a>
              </span>
            </div>
            <div class="comment-content">Your QFabric diagram is actually overly-simplified: each QF/Node has to have dual connections to the Control Plane, and this is achieved by having separate GbE copper connections to separate EX4200-based Virtual Chassis. Similarly the QF/Directors - the &quot;Director Group&quot; - has multiple connections (3..?) to the same VCs, and much the same for the QF/Interconnects. This is obvioulsy all about redundancy (good), but it does place significant restrictions on the physical topolgy of a QFabric Switch deployment. Everything needs to be within something like a 100m reach, and with the cable routing requirements of a real-world Data Centre I&#39;m not sure how practical the scale-out capabilities will be... Facoring in the pricing, the high start-up costs for QF/I, QF/D, and all of the software licensing, it&#39;s difficult to see where the economies of scale will be achieved. Certainly it&#39;s not something to be considered lighlty...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4262934722987402932">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Rob Turner</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4262934722987402932" href="#4262934722987402932">20 September 2011 11:32</a>
              </span>
            </div>
            <div class="comment-content">Your QFabric diagram is actually overly-simplified: each QF/Node has to have dual connections to the Control Plane, and this is achieved by having separate GbE copper connections to separate EX4200-based Virtual Chassis. Similarly the QF/Directors - the &quot;Director Group&quot; - has multiple connections (3..?) to the same VCs, and much the same for the QF/Interconnects. This is obvioulsy all about redundancy (good), but it does place significant restrictions on the physical topolgy of a QFabric Switch deployment. Everything needs to be within something like a 100m reach, and with the cable routing requirements of a real-world Data Centre I&#39;m not sure how practical the scale-out capabilities will be... Factoring in the pricing, the high start-up costs for QF/I, QF/D, and all of the software licensing, it&#39;s difficult to see where the economies of scale (of many multiple QF/N) will be achieved. Certainly it&#39;s not something to be considered lighlty...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2538668835377118103">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2538668835377118103" href="#2538668835377118103">20 September 2011 11:45</a>
              </span>
            </div>
            <div class="comment-content">Thank you. Updated.<br /><br />The only mention of the 100m Node-to-Interconnect cable length I found was in the PFC section (you have to stay within 100m if you want lossless transport).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="888618362866217541">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Matthew Stone</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c888618362866217541" href="#888618362866217541">20 September 2011 17:40</a>
              </span>
            </div>
            <div class="comment-content">500,000,000 phone calls.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5983503150358375642">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5983503150358375642" href="#5983503150358375642">20 September 2011 18:08</a>
              </span>
            </div>
            <div class="comment-content">... or approximately the whole China talking at once. Impressive  8-)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6368353029432768457">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ben Jencks</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6368353029432768457" href="#6368353029432768457">20 September 2011 19:27</a>
              </span>
            </div>
            <div class="comment-content">I&#39;d be very nervous about putting my entire network under one control plane, even with redundancy. It just takes one memory leak in the controller plus a failover bug to take down every switch in the network. Not to mention the idea of a switch stack as critical to control-plane functioning.<br /><br />There&#39;s a reason distributed architectures have been favored for thirty years.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4041937798867838453">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4041937798867838453" href="#4041937798867838453">20 September 2011 19:31</a>
              </span>
            </div>
            <div class="comment-content">Tell that to the OpenFlow crowd  :-P<br /><br />Actually, the QFabric&#39;s control-plane architecture is pretty distributed. More in the next post.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8894763118674969849">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Matthew Stone</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8894763118674969849" href="#8894763118674969849">20 September 2011 23:17</a>
              </span>
            </div>
            <div class="comment-content">So you&#39;re saying QFabric will probably be what&#39;s deployed in the data center filtering China&#39;s traffic? Makes perfect sense.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6294041354465479587">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ryan Malayter</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6294041354465479587" href="#6294041354465479587">21 September 2011 00:02</a>
              </span>
            </div>
            <div class="comment-content">Am I missing something, or is QFabric basically a giant chassis switch with each card broken into separate 1-2U device, with custom cabling lashing it all together? Once you get to the ingress port, it&#39;s all totally proprietary, and not at all &quot;distributed&quot;,&quot;scale-out&quot; or anything else buzzword-complaint. Cisco couldn&#39;t have dreamed up a better proprietary non-solution to what customers need.<br /><br />Give me a bunch of 48+ port TOR-style switches connected with a folded-Clos (or fattened butterfly or 3D torus or whatever) topology that can be managed as one, have one unified control plane, and do ECMP across all links at layer 2 and layer 3. Bonus points for some smart adaptive routing that doesn&#39;t require global information sharing. <br /><br />Plug-and-play, scale-out networking. That&#39;s revolutionary. QFabric isn&#39;t.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1667219675767350080">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1667219675767350080" href="#1667219675767350080">21 September 2011 02:27</a>
              </span>
            </div>
            <div class="comment-content">Although QFabric has to mature for sure,  the concept seems ingeniously simple. Instead of building a flat layer 2 network out a bunch of vendor pushed half-baked &quot;standards&quot;, Juniper seems to be extending upon an idea most are comfortable, a distributed layer2/layer3 switch. <br /><br /> Is it just me or does it seem ripe for eventual OpenFlow support?  To me it seems that the QF/Director must act in many ways like an OpenFlow Controller.<br /><br />  I agree the cost could be a prohibiting point, but I&#39;m impressed with premise.  Maybe some day they will release a scaled down version to gain footing in smaller deployments</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4873901636713643034">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Chris Jones</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4873901636713643034" href="#4873901636713643034">21 September 2011 21:57</a>
              </span>
            </div>
            <div class="comment-content">It is, actually...<br /><br />See, what you&#39;re describing is more than one lookup. That&#39;s the beauty of QFabric... it&#39;s a single L2 lookup. So its actually a brilliant technology.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7542736207624603036">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Chris Jones</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7542736207624603036" href="#7542736207624603036">21 September 2011 21:58</a>
              </span>
            </div>
            <div class="comment-content">Time to move on from the stone age architectures.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2299438548506118270">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Chris Jones</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2299438548506118270" href="#2299438548506118270">21 September 2011 21:59</a>
              </span>
            </div>
            <div class="comment-content">1 tier logically, yes.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6026679041924496884">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Chris Jones</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6026679041924496884" href="#6026679041924496884">21 September 2011 22:01</a>
              </span>
            </div>
            <div class="comment-content">They already are ;) Watch what happens in 1H2012 :)<br /><br />As far as cost goes, since the QFX3500 is just a 40x10GigE switch, you can actually deploy them as part of a migration. No need to green field! Interconnects can always be added later.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1529925791712281426">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ryan Malayter</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1529925791712281426" href="#1529925791712281426">22 September 2011 15:03</a>
              </span>
            </div>
            <div class="comment-content">Except there&#39;s nothing that prevents a scale-out architecture from doing source-routing using port numbers (or whatever headers are used on QFabric&#39;s proprietery links) instead of MACs. Many HPC interconnects work exactly this way.<br /><br />QFrabric is &quot;brilliant&quot; in the same way the Spruce Goose was brilliant. Impressive engineering, but a solution looking for a problem, and at way too high of a cost.<br />The cost of a redundant QFabric system is something like $2100 per host-facing 10 GbE port with 3:1 oversubscription for 1920 host-facing ports. A two-stage folded-clos network of the same size, also with 3:1 osub at the edge, would only be about $600 per host-facing port based on current list pricing of 56 * (48-port 10 GbE switch) pricing from other vendors (assuming such switches could someday actually do TRILL or other L2 ECMP).<br /><br />Arista/Dell/HP: Juniper messed up, and you&#39;ve got a big opportunity here.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4565208046944869300">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Abner Germanow</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4565208046944869300" href="#4565208046944869300">24 September 2011 03:13</a>
              </span>
            </div>
            <div class="comment-content">Hi Ryan,<br /><br />Is your argument that you have to deploy two fabrics for redundancy? We have had a couple of early customers consider that path and elect to go with a single fabric instead. While the fabric behaves like a single switch, it is designed to be as resilient as the network many would like to deploy, but rarely do in practice.<br /><br />As far as cost goes. We believe there is significant value that goes along with providing applications with the performance they want, infrastructure flexibility rarely found before, and what should be significant operational savings that typically overshadow the capital cost of a network. Time will tell which and to what degree those assumptions are true. <br /><br />On the size front, there are those who have accused us of building something specifically for large search engines and such and others who say a 6000 port fabric is too small for their needs. We&#39;ve said we know how to scale the architecture up and down, so don&#39;t count us out yet. This game is just getting interesting.<br /><br />Cheers,<br />Abner from Juniper</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6308031741938926195">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">The Louman</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6308031741938926195" href="#6308031741938926195">24 September 2011 19:06</a>
              </span>
            </div>
            <div class="comment-content">Q-Fabric is nothing new.  Geez...back in the day, Bay Networks Centillion did the same exact thing, with a complex redundant backplane that was way ahead of it&#39;s time.  Nice repackaging of a legacy concept...and yes, Q-Fabric is actually less intelligent than the Centillion was from back in the 90&#39;s...or is it really, Q-Past_Fabric?     ;)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5710366570007417168">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Matthew Norwood</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5710366570007417168" href="#5710366570007417168">27 September 2011 05:07</a>
              </span>
            </div>
            <div class="comment-content">I asked about a smaller version at VMworld. They had all 3 components on display just like they did at Interop in Vegas back in May. I was told that a smaller interconnect piece will be coming next year. The QFX3008 is a beast. Not that the Nexus 7k&#39;s are tiny mind you.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5764087702334723455">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo Carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5764087702334723455" href="#5764087702334723455">03 October 2011 18:06</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan.<br /><br />Still I can&#39;t understand the &quot;one-lookup&quot; statement. I clearly see that the Edge Node will perform one lookup, decide which way to reach the destination QFabric port, and embed that forwarding information into the proprietary frame format for the Interconnect to use it to send the packet accordingly.<br /><br />What I see is that, if this assumption is correct, the Interconnect must itself perform another, non-standard lookup in order to read that proprietary forwarding information and switch the frame to the destination node... which I assume will somehow know which port to place it into.<br /><br />I see one &quot;Ethernet destination MAC address&quot; lookup, and two &quot;proprietary forwarding information header&quot; lookups.<br /><br />Where am I wrong? I surely must be missing something.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8294510139472478378">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8294510139472478378" href="#8294510139472478378">03 October 2011 18:11</a>
              </span>
            </div>
            <div class="comment-content">You&#39;re absolutely right. One L2/L3 lookup, encapsulation into proprietary format and multiple tag (or whatever they call it) lookups throughout QFabric (Interconnect is a 3-stage Clos tree).</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="7203158830321993629">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo Carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c7203158830321993629" href="#7203158830321993629">03 October 2011 18:35</a>
              </span>
            </div>
            <div class="comment-content">OK.<br /><br />What I don&#39;t see then is convergence time information. I understand that for the edge node to attach the proper &quot;tag&quot;, it must have visibility of the topology relevant to its connected nodes. Therefore, every topology change (like a VM move) must trigger a FIB update to edge nodes from the QF Directors.<br /><br />With potentially hundreds of edge nodes... What procedure is used to guarantee timely convergence times for the distribution of this info? What is the target convergence times advertised by JNPR?<br /><br />As usual, congrats on the post Ivan.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5455849475811461313">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo Carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5455849475811461313" href="#5455849475811461313">03 October 2011 18:40</a>
              </span>
            </div>
            <div class="comment-content">An MPLS cloud is also logically a 1-tier hop, then. It all depends on perspective. :)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="855865344880275229">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo Carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c855865344880275229" href="#855865344880275229">03 October 2011 18:44</a>
              </span>
            </div>
            <div class="comment-content">Well... I would not trust my network to just one huge control plane, no matter how much redundancy is thrown into the mix. Modern DC networks should, IMHO, support complete control plane separation between fabric A and B whenever you involve FCoE transport. I have a hard time figuring out how to do that with just one QFabric.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="4461614343359077993">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo Carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c4461614343359077993" href="#4461614343359077993">03 October 2011 18:48</a>
              </span>
            </div>
            <div class="comment-content">Gotta agree with Ben here.<br /><br />Not that distributed, Ivan, as only first-tier protocols (LACP and such) are distributed. Forwarding information base building is delegated to a single control plane, with no visibility into how this information is propagated, what split-horizon mechanisms are implemented, how its coherence is protected... It&#39;s basically a huge black-box with regards to Control Plane.<br /><br />Plus, it relies upon the stability of a separate out-of-band control network, with its own overlapping control plane...<br /><br />Talk about stone age, Chris... ;)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6945874730614800174">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Abner Germanow</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6945874730614800174" href="#6945874730614800174">05 October 2011 10:09</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan,  <br /> <br />One comment on &quot;totally proprietary.&quot; To be clear - and I don&#39;t want to put words into your mouth - you are referring to the internals of the system that actually create the fabric. Just like a chassis switch system, the internal components are proprietary, while all external facing ports are Ethernet and/or fiber channel. More here: http://j.mp/ov18e2  <br /> <br />Our view is that creating a protocol based fabric can get you slightly better efficiency and any to any connectivity than legacy kit, but switch fabrics will deliver much better efficiency, predictability, performance, security, and manageability. We expect those benefits to outweigh the complexity protocol based fabrics will foist on customers. While some have suggested the only way to deploy a QFabric is in a greenfield DC, that would be insane on our part. Early customers are deploying in a corner of the datacenter and expanding from there. I also expect the economics to get more interesting over time.  <br /> <br />Cheers,  <br />Abner (Juniper Networks)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2283213420767951562">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2283213420767951562" href="#2283213420767951562">05 October 2011 13:00</a>
              </span>
            </div>
            <div class="comment-content">You&#39;re absolutely correct ... but we both know that networking engineers hate large black boxes; they make troubleshooting way harder than it should be.<br /><br />However, seems like everyone is moving in the same direction, including Cisco&#39;s FEX products and OpenFlow (where although the controller-to-switch communication is standard, the real troubleshooting will have to be done in the guts of the controller, which will be proprietary).<br /><br />Already looking forward to the NFD2 discussions  8-)<br />Ivan</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1460514427212663573">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1460514427212663573" href="#1460514427212663573">05 October 2011 14:25</a>
              </span>
            </div>
            <div class="comment-content">I believe it is only fair to state that I am a Cisco engineer, just in case. :-D</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3734847725805918518">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Pablo Carlier</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3734847725805918518" href="#3734847725805918518">05 October 2011 14:28</a>
              </span>
            </div>
            <div class="comment-content">I believe it is only fair to state that I am a Cisco engineer, just in case.  :-D</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
