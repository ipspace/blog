comments:
- date: 20 September 2011 11:30
  html: 'Your QFabric diagram is actually overly-simplified: each QF/Node has to have
    dual connections to the Control Plane, and this is achieved by having separate
    GbE copper connections to separate EX4200-based Virtual Chassis. Similarly the
    QF/Directors - the &quot;Director Group&quot; - has multiple connections (3..?)
    to the same VCs, and much the same for the QF/Interconnects. This is obvioulsy
    all about redundancy (good), but it does place significant restrictions on the
    physical topolgy of a QFabric Switch deployment. Everything needs to be within
    something like a 100m reach, and with the cable routing requirements of a real-world
    Data Centre I&#39;m not sure how practical the scale-out capabilities will be...
    Facoring in the pricing, the high start-up costs for QF/I, QF/D, and all of the
    software licensing, it&#39;s difficult to see where the economies of scale will
    be achieved. Certainly it&#39;s not something to be considered lighlty...'
  id: '5954352233798592577'
  image: https://resources.blogblog.com/img/blank.gif
  name: Rob turner
  profile: null
  pub: '2011-09-20T11:30:46.257+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 11:32
  html: 'Your QFabric diagram is actually overly-simplified: each QF/Node has to have
    dual connections to the Control Plane, and this is achieved by having separate
    GbE copper connections to separate EX4200-based Virtual Chassis. Similarly the
    QF/Directors - the &quot;Director Group&quot; - has multiple connections (3..?)
    to the same VCs, and much the same for the QF/Interconnects. This is obvioulsy
    all about redundancy (good), but it does place significant restrictions on the
    physical topolgy of a QFabric Switch deployment. Everything needs to be within
    something like a 100m reach, and with the cable routing requirements of a real-world
    Data Centre I&#39;m not sure how practical the scale-out capabilities will be...
    Factoring in the pricing, the high start-up costs for QF/I, QF/D, and all of the
    software licensing, it&#39;s difficult to see where the economies of scale (of
    many multiple QF/N) will be achieved. Certainly it&#39;s not something to be considered
    lighlty...'
  id: '4262934722987402932'
  image: https://resources.blogblog.com/img/blank.gif
  name: Rob Turner
  profile: null
  pub: '2011-09-20T11:32:27.129+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 11:45
  html: Thank you. Updated.<br /><br />The only mention of the 100m Node-to-Interconnect
    cable length I found was in the PFC section (you have to stay within 100m if you
    want lossless transport).
  id: '2538668835377118103'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-09-20T11:45:07.027+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 17:40
  html: 500,000,000 phone calls.
  id: '888618362866217541'
  image: https://resources.blogblog.com/img/blank.gif
  name: Matthew Stone
  profile: null
  pub: '2011-09-20T17:40:11.549+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 18:08
  html: '... or approximately the whole China talking at once. Impressive  8-)'
  id: '5983503150358375642'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-09-20T18:08:46.760+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 19:27
  html: I&#39;d be very nervous about putting my entire network under one control
    plane, even with redundancy. It just takes one memory leak in the controller plus
    a failover bug to take down every switch in the network. Not to mention the idea
    of a switch stack as critical to control-plane functioning.<br /><br />There&#39;s
    a reason distributed architectures have been favored for thirty years.
  id: '6368353029432768457'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ben Jencks
  profile: null
  pub: '2011-09-20T19:27:00.873+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 19:31
  html: Tell that to the OpenFlow crowd  :-P<br /><br />Actually, the QFabric&#39;s
    control-plane architecture is pretty distributed. More in the next post.
  id: '4041937798867838453'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-09-20T19:31:40.167+02:00'
  ref: '524082304037976027'
  type: comment
- date: 20 September 2011 23:17
  html: So you&#39;re saying QFabric will probably be what&#39;s deployed in the data
    center filtering China&#39;s traffic? Makes perfect sense.
  id: '8894763118674969849'
  image: https://resources.blogblog.com/img/blank.gif
  name: Matthew Stone
  profile: null
  pub: '2011-09-20T23:17:54.971+02:00'
  ref: '524082304037976027'
  type: comment
- date: 21 September 2011 00:02
  html: Am I missing something, or is QFabric basically a giant chassis switch with
    each card broken into separate 1-2U device, with custom cabling lashing it all
    together? Once you get to the ingress port, it&#39;s all totally proprietary,
    and not at all &quot;distributed&quot;,&quot;scale-out&quot; or anything else
    buzzword-complaint. Cisco couldn&#39;t have dreamed up a better proprietary non-solution
    to what customers need.<br /><br />Give me a bunch of 48+ port TOR-style switches
    connected with a folded-Clos (or fattened butterfly or 3D torus or whatever) topology
    that can be managed as one, have one unified control plane, and do ECMP across
    all links at layer 2 and layer 3. Bonus points for some smart adaptive routing
    that doesn&#39;t require global information sharing. <br /><br />Plug-and-play,
    scale-out networking. That&#39;s revolutionary. QFabric isn&#39;t.
  id: '6294041354465479587'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ryan Malayter
  profile: null
  pub: '2011-09-21T00:02:09.113+02:00'
  ref: '524082304037976027'
  type: comment
- date: 21 September 2011 02:27
  html: Although QFabric has to mature for sure,  the concept seems ingeniously simple.
    Instead of building a flat layer 2 network out a bunch of vendor pushed half-baked
    &quot;standards&quot;, Juniper seems to be extending upon an idea most are comfortable,
    a distributed layer2/layer3 switch. <br /><br /> Is it just me or does it seem
    ripe for eventual OpenFlow support?  To me it seems that the QF/Director must
    act in many ways like an OpenFlow Controller.<br /><br />  I agree the cost could
    be a prohibiting point, but I&#39;m impressed with premise.  Maybe some day they
    will release a scaled down version to gain footing in smaller deployments
  id: '1667219675767350080'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2011-09-21T02:27:43.922+02:00'
  ref: '524082304037976027'
  type: comment
- date: 21 September 2011 21:57
  html: It is, actually...<br /><br />See, what you&#39;re describing is more than
    one lookup. That&#39;s the beauty of QFabric... it&#39;s a single L2 lookup. So
    its actually a brilliant technology.
  id: '4873901636713643034'
  image: https://resources.blogblog.com/img/blank.gif
  name: Chris Jones
  profile: null
  pub: '2011-09-21T21:57:46.533+02:00'
  ref: '524082304037976027'
  type: comment
- date: 21 September 2011 21:58
  html: Time to move on from the stone age architectures.
  id: '7542736207624603036'
  image: https://resources.blogblog.com/img/blank.gif
  name: Chris Jones
  profile: null
  pub: '2011-09-21T21:58:28.998+02:00'
  ref: '524082304037976027'
  type: comment
- date: 21 September 2011 21:59
  html: 1 tier logically, yes.
  id: '2299438548506118270'
  image: https://resources.blogblog.com/img/blank.gif
  name: Chris Jones
  profile: null
  pub: '2011-09-21T21:59:11.183+02:00'
  ref: '524082304037976027'
  type: comment
- date: 21 September 2011 22:01
  html: They already are ;) Watch what happens in 1H2012 :)<br /><br />As far as cost
    goes, since the QFX3500 is just a 40x10GigE switch, you can actually deploy them
    as part of a migration. No need to green field! Interconnects can always be added
    later.
  id: '6026679041924496884'
  image: https://resources.blogblog.com/img/blank.gif
  name: Chris Jones
  profile: null
  pub: '2011-09-21T22:01:01.935+02:00'
  ref: '524082304037976027'
  type: comment
- date: 22 September 2011 15:03
  html: 'Except there&#39;s nothing that prevents a scale-out architecture from doing
    source-routing using port numbers (or whatever headers are used on QFabric&#39;s
    proprietery links) instead of MACs. Many HPC interconnects work exactly this way.<br
    /><br />QFrabric is &quot;brilliant&quot; in the same way the Spruce Goose was
    brilliant. Impressive engineering, but a solution looking for a problem, and at
    way too high of a cost.<br />The cost of a redundant QFabric system is something
    like $2100 per host-facing 10 GbE port with 3:1 oversubscription for 1920 host-facing
    ports. A two-stage folded-clos network of the same size, also with 3:1 osub at
    the edge, would only be about $600 per host-facing port based on current list
    pricing of 56 * (48-port 10 GbE switch) pricing from other vendors (assuming such
    switches could someday actually do TRILL or other L2 ECMP).<br /><br />Arista/Dell/HP:
    Juniper messed up, and you&#39;ve got a big opportunity here.'
  id: '1529925791712281426'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ryan Malayter
  profile: null
  pub: '2011-09-22T15:03:41.223+02:00'
  ref: '524082304037976027'
  type: comment
- date: 24 September 2011 03:13
  html: Hi Ryan,<br /><br />Is your argument that you have to deploy two fabrics for
    redundancy? We have had a couple of early customers consider that path and elect
    to go with a single fabric instead. While the fabric behaves like a single switch,
    it is designed to be as resilient as the network many would like to deploy, but
    rarely do in practice.<br /><br />As far as cost goes. We believe there is significant
    value that goes along with providing applications with the performance they want,
    infrastructure flexibility rarely found before, and what should be significant
    operational savings that typically overshadow the capital cost of a network. Time
    will tell which and to what degree those assumptions are true. <br /><br />On
    the size front, there are those who have accused us of building something specifically
    for large search engines and such and others who say a 6000 port fabric is too
    small for their needs. We&#39;ve said we know how to scale the architecture up
    and down, so don&#39;t count us out yet. This game is just getting interesting.<br
    /><br />Cheers,<br />Abner from Juniper
  id: '4565208046944869300'
  image: https://resources.blogblog.com/img/blank.gif
  name: Abner Germanow
  profile: null
  pub: '2011-09-24T03:13:20.194+02:00'
  ref: '524082304037976027'
  type: comment
- date: 24 September 2011 19:06
  html: Q-Fabric is nothing new.  Geez...back in the day, Bay Networks Centillion
    did the same exact thing, with a complex redundant backplane that was way ahead
    of it&#39;s time.  Nice repackaging of a legacy concept...and yes, Q-Fabric is
    actually less intelligent than the Centillion was from back in the 90&#39;s...or
    is it really, Q-Past_Fabric?     ;)
  id: '6308031741938926195'
  image: https://resources.blogblog.com/img/blank.gif
  name: The Louman
  profile: null
  pub: '2011-09-24T19:06:21.618+02:00'
  ref: '524082304037976027'
  type: comment
- date: 27 September 2011 05:07
  html: I asked about a smaller version at VMworld. They had all 3 components on display
    just like they did at Interop in Vegas back in May. I was told that a smaller
    interconnect piece will be coming next year. The QFX3008 is a beast. Not that
    the Nexus 7k&#39;s are tiny mind you.
  id: '5710366570007417168'
  image: https://resources.blogblog.com/img/blank.gif
  name: Matthew Norwood
  profile: null
  pub: '2011-09-27T05:07:40.406+02:00'
  ref: '524082304037976027'
  type: comment
- date: 03 October 2011 18:06
  html: Hi Ivan.<br /><br />Still I can&#39;t understand the &quot;one-lookup&quot;
    statement. I clearly see that the Edge Node will perform one lookup, decide which
    way to reach the destination QFabric port, and embed that forwarding information
    into the proprietary frame format for the Interconnect to use it to send the packet
    accordingly.<br /><br />What I see is that, if this assumption is correct, the
    Interconnect must itself perform another, non-standard lookup in order to read
    that proprietary forwarding information and switch the frame to the destination
    node... which I assume will somehow know which port to place it into.<br /><br
    />I see one &quot;Ethernet destination MAC address&quot; lookup, and two &quot;proprietary
    forwarding information header&quot; lookups.<br /><br />Where am I wrong? I surely
    must be missing something.
  id: '5764087702334723455'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo Carlier
  profile: null
  pub: '2011-10-03T18:06:23.986+02:00'
  ref: '524082304037976027'
  type: comment
- date: 03 October 2011 18:11
  html: You&#39;re absolutely right. One L2/L3 lookup, encapsulation into proprietary
    format and multiple tag (or whatever they call it) lookups throughout QFabric
    (Interconnect is a 3-stage Clos tree).
  id: '8294510139472478378'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-10-03T18:11:09.308+02:00'
  ref: '524082304037976027'
  type: comment
- date: 03 October 2011 18:35
  html: OK.<br /><br />What I don&#39;t see then is convergence time information.
    I understand that for the edge node to attach the proper &quot;tag&quot;, it must
    have visibility of the topology relevant to its connected nodes. Therefore, every
    topology change (like a VM move) must trigger a FIB update to edge nodes from
    the QF Directors.<br /><br />With potentially hundreds of edge nodes... What procedure
    is used to guarantee timely convergence times for the distribution of this info?
    What is the target convergence times advertised by JNPR?<br /><br />As usual,
    congrats on the post Ivan.
  id: '7203158830321993629'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo Carlier
  profile: null
  pub: '2011-10-03T18:35:09.379+02:00'
  ref: '524082304037976027'
  type: comment
- date: 03 October 2011 18:40
  html: An MPLS cloud is also logically a 1-tier hop, then. It all depends on perspective.
    :)
  id: '5455849475811461313'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo Carlier
  profile: null
  pub: '2011-10-03T18:40:15.359+02:00'
  ref: '524082304037976027'
  type: comment
- date: 03 October 2011 18:44
  html: Well... I would not trust my network to just one huge control plane, no matter
    how much redundancy is thrown into the mix. Modern DC networks should, IMHO, support
    complete control plane separation between fabric A and B whenever you involve
    FCoE transport. I have a hard time figuring out how to do that with just one QFabric.
  id: '855865344880275229'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo Carlier
  profile: null
  pub: '2011-10-03T18:44:21.793+02:00'
  ref: '524082304037976027'
  type: comment
- date: 03 October 2011 18:48
  html: Gotta agree with Ben here.<br /><br />Not that distributed, Ivan, as only
    first-tier protocols (LACP and such) are distributed. Forwarding information base
    building is delegated to a single control plane, with no visibility into how this
    information is propagated, what split-horizon mechanisms are implemented, how
    its coherence is protected... It&#39;s basically a huge black-box with regards
    to Control Plane.<br /><br />Plus, it relies upon the stability of a separate
    out-of-band control network, with its own overlapping control plane...<br /><br
    />Talk about stone age, Chris... ;)
  id: '4461614343359077993'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo Carlier
  profile: null
  pub: '2011-10-03T18:48:29.042+02:00'
  ref: '524082304037976027'
  type: comment
- date: 05 October 2011 10:09
  html: 'Hi Ivan,  <br /> <br />One comment on &quot;totally proprietary.&quot; To
    be clear - and I don&#39;t want to put words into your mouth - you are referring
    to the internals of the system that actually create the fabric. Just like a chassis
    switch system, the internal components are proprietary, while all external facing
    ports are Ethernet and/or fiber channel. More here: http://j.mp/ov18e2  <br />
    <br />Our view is that creating a protocol based fabric can get you slightly better
    efficiency and any to any connectivity than legacy kit, but switch fabrics will
    deliver much better efficiency, predictability, performance, security, and manageability.
    We expect those benefits to outweigh the complexity protocol based fabrics will
    foist on customers. While some have suggested the only way to deploy a QFabric
    is in a greenfield DC, that would be insane on our part. Early customers are deploying
    in a corner of the datacenter and expanding from there. I also expect the economics
    to get more interesting over time.  <br /> <br />Cheers,  <br />Abner (Juniper
    Networks)'
  id: '6945874730614800174'
  image: https://resources.blogblog.com/img/blank.gif
  name: Abner Germanow
  profile: null
  pub: '2011-10-05T10:09:27.231+02:00'
  ref: '524082304037976027'
  type: comment
- date: 05 October 2011 13:00
  html: You&#39;re absolutely correct ... but we both know that networking engineers
    hate large black boxes; they make troubleshooting way harder than it should be.<br
    /><br />However, seems like everyone is moving in the same direction, including
    Cisco&#39;s FEX products and OpenFlow (where although the controller-to-switch
    communication is standard, the real troubleshooting will have to be done in the
    guts of the controller, which will be proprietary).<br /><br />Already looking
    forward to the NFD2 discussions  8-)<br />Ivan
  id: '2283213420767951562'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-10-05T13:00:56.712+02:00'
  ref: '524082304037976027'
  type: comment
- date: 05 October 2011 14:25
  html: I believe it is only fair to state that I am a Cisco engineer, just in case.
    :-D
  id: '1460514427212663573'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo carlier
  profile: null
  pub: '2011-10-05T14:25:41.508+02:00'
  ref: '524082304037976027'
  type: comment
- date: 05 October 2011 14:28
  html: I believe it is only fair to state that I am a Cisco engineer, just in case.  :-D
  id: '3734847725805918518'
  image: https://resources.blogblog.com/img/blank.gif
  name: Pablo Carlier
  profile: null
  pub: '2011-10-05T14:28:53.390+02:00'
  ref: '524082304037976027'
  type: comment
count: 28
id: '524082304037976027'
type: post
url: 2011/09/qfabric-part-1-hardware-architecture.html
