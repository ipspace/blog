{
  "comments": [
    {
      "date": "30 September 2011 07:09",
      "html": "Not disagreeing, just a couple of comments. :)<br /><br />&gt; Obviously the moved VMs will cause long-distance traffic trombones, further increasing the utilization of inter-DC link and reducing effective migration rate.<br /><br />Apologies for stating the obvious, but if the traffic has to trombone back to the first datacenter (which supposedly in in the harm&#39;s way), you&#39;re not really avoiding the disaster, are you?<br /><br />&gt; Major obstacle: maximum round-trip time supported by vMotion with vSphere 4.x is 5 ms (some other documents cite 200 km), extended to 10 ms in vSphere 5.x.<br /><br />5ms RTT is about 500km away. Should be plenty, no?<br /><br />&gt; Assuming perfect link utilization, no protocol overhead, and no repeat copies of dirty memory pages, you can move a GB of RAM in 8 seconds<br /><br />I wonder if WAN accelerators can make any difference?<br /><br />&gt; Last step: ask your service provider for a quote<br /><br />This, all in all, sounds like an *excellent* opportunity for a service provider to offer you a bandwidth on demand (putting aside the number of SP&#39;s products/marketing people murdered by network engineers on the grounds of all challenges that Bandwidth on Demand presents, especially when talking about any significant bandwidths). ;)<br /><br />I guess the summary of my thoughts on the subject is this: yes, there are plenty challenges; so how about coming up with a checklist of a sort for &quot;how do I find out if the Disaster Avoidance makes sense in my circumstances&quot;?  :-$",
      "id": "7466704755819993072",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Dmitri Kalintsev",
      "profile": null,
      "pub": "2011-09-30T07:09:06.667+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "30 September 2011 14:26",
      "html": "Excellent points Ivan.  LD vMotion en mass sounds kinda silly.  Why vMotion at all? There is something to be said for push button application restart.  Just stop and start the vApp in the other DC with all the same network configuration. No scripting or other complicated tools to re-configured IP &amp; DNS addresses, no painfully long vMotions to wait for.  Rather, the app starts up in the new DC and just works.  Assuming of course that you have a functional implementation of LISP :-)<br /><br />Cheers,<br />Brad",
      "id": "7746172173601802537",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Brad Hedlund",
      "profile": null,
      "pub": "2011-09-30T14:26:30.345+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "30 September 2011 14:36",
      "html": "WAN accelerators will help if the data stored in memory is redundant and compressible (i.e. not encrypted)",
      "id": "12949755334196539",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "John P.",
      "profile": null,
      "pub": "2011-09-30T14:36:06.791+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "30 September 2011 14:58",
      "html": "I agree long-distance virtual machine migration is unworkable, but sad fact is that the majority of datacenter (read: financials, ERP, operations) applications are not architected to support load balancing and clustering across WAN distances, especially at the database layer. Fixing that problem is much harder, and in fact decades of research have not produced a database system which can do distributed transactions and replication across high-latency links.<br /><br />Like the speed of light for network engineers, the CAP theorem is a bitch for application engineers. So what is your alternate solution Ivan? Active/passive systems with asynchronous state replication (which means data loss in the event of failover)?",
      "id": "2231794383637483538",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ryan Malayter",
      "profile": null,
      "pub": "2011-09-30T14:58:04.560+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "30 September 2011 21:29",
      "html": "&quot;I wonder if WAN accelerators can make any difference?&quot; From my experience with Wan Optimizers, you can see an improvement in traffic transport in 95% of the cases, so I don&#39;t see why they will not make a difference in case of vMotion.",
      "id": "7733664897759161936",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Calin Chiorean",
      "profile": null,
      "pub": "2011-09-30T21:29:55.209+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 05:44",
      "html": "Is anyone really vmotioning across data centers or are people just talking about it?  I keep hearing that it&#39;s not supported to vmotion across different Nexus 1000V switches.",
      "id": "8874530414142826169",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Will",
      "profile": null,
      "pub": "2011-10-01T05:44:31.465+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 06:54",
      "html": "Hi Ivan,<br /><br />two points:<br />- could physical transfer of data be faster? I.e., taking all the data to be migrated into a briefcase and driving this briefcase away to safety?<br />- my calculation of NPV for perpetuity leaves me at 1.2m$, no decimals. How come your result is not rounded, are you calculating in monthly increments?<br /><br />Cheers,<br />Gregor",
      "id": "5723372212633423944",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Gcuzak",
      "profile": null,
      "pub": "2011-10-01T06:54:26.895+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 08:06",
      "html": "#2 - I converted yearly discount rate into compounded monthly discount rate (1.10^(1/12)-1)",
      "id": "5849255328942840463",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-10-01T08:06:16.743+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 08:08",
      "html": "I can&#39;t see a reason why you couldn&#39;t vMotion between two Nexus 1000V switches ... might be a VMware/vDS limitation.",
      "id": "6034463987679622889",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-10-01T08:08:26.027+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 08:12",
      "html": "It&#39;s so nice to see we&#39;re in agreement  8-)<br /><br />You might not need LISP if you control the L3 network between the two data centers. Host routes also work, but of course LISP (or MPLS) scales better as the intermediate nodes don&#39;t have to keep track of migrated IP addresses.<br /><br />However, I don&#39;t think the current NX-OS releases support a mechanism that would create host routes on-demand (like LAM did decades ago), whereas LISP with VM mobility is available.",
      "id": "8003353164242988237",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-10-01T08:12:34.369+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 08:18",
      "html": "Trombones: to complete the disaster avoidance exercise, you have to shut down the subnets in the first data center or fix the IP routing in some other way. As long as you have split subnets, traffic will flow in somewhat unpredictable direction.<br /><br />RTT: you have to take in account the queuing/processing/serialization delay in all intermediate devices as well as circuitous ways in which your lambdas might go over physical fiber infrastructure. Just heard a great example yesterday: a carrier was not willing to commit to 5ms delay within London.<br /><br />WAN acceleration: it does help. F5&#39;s EtherIP is a great solution that provides vMotion traffic compression and bridging-over-IP at the same time. Search their web site for vMotion/EtherIP.<br /><br />Bandwidth-on-demand: might be useful for maintenance/migration purposes. Not sure I want to rely on that feature being available when a major disaster is heading my way; everyone would probably want to get more bandwidth at that time.",
      "id": "4895546436731488640",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-10-01T08:18:17.364+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "date": "01 October 2011 08:31",
      "html": "Obviously you&#39;re exposed to some data loss if you can&#39;t afford synchronous replication. The question you have to ask is: how much loss is acceptable. <br /><br />If you want to retain true transactional integrity with roll-forward to the exact point of failure (which sounds great, but is not always as mandatory as people think it is), you cannot rely on asynchronous block storage replication, but there are other database-level mechanisms like transaction logs.<br /><br />If you&#39;re willing to accept loss of the transactions that were completed just prior to the failure, life becomes way simpler - for example, you can use read-only replicas.<br /><br />Disclaimer: I know absolutely nothing about relational databases ... apart from the syntax of the SELECT statement  :-P",
      "id": "5854823028041964588",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-10-01T08:31:51.866+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    },
    {
      "comments": [
        {
          "date": "19 July 2016 16:06",
          "html": "&quot;Silly typo, but the &quot;or&quot; there broke the momentum of the argument.&quot; &lt;&lt; We can&#39;t have that, can we? Thank you, fixed.",
          "id": "2107764693243891939",
          "image": "https://lh3.googleusercontent.com/zFdxGE77vvD2w5xHy6jkVuElKv-U9_9qLkRYK8OnbDeJPtjSZ82UPq5w6hJ-SA=s35",
          "name": "Ivan Pepelnjak",
          "profile": "https://www.blogger.com/profile/13457151406311272386",
          "pub": "2016-07-19T16:06:36.763+02:00",
          "ref": "121381575197805773",
          "type": "comment"
        }
      ],
      "date": "18 July 2016 21:16",
      "html": "&quot;completely saturate a 1Gbps link to vacate a physical server with 256 GB or RAM in just over half an hour&quot;<br />Silly typo, but the &quot;or&quot; there broke the momentum of the argument.",
      "id": "121381575197805773",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Anonymous",
      "profile": null,
      "pub": "2016-07-18T21:16:29.106+02:00",
      "ref": "6467986399207032597",
      "type": "comment"
    }
  ],
  "count": 14,
  "id": "6467986399207032597",
  "type": "post",
  "url": "2011/09/long-distance-vmotion-for-disaster.html"
}