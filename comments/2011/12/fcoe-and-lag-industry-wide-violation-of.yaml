comments:
- date: 14 December 2011 12:24
  html: How would FC multipathing work over a bonded interface in an FCoE scenario?
    As I understand it, each FC interface logs into the fabric and is zoned to one
    or more storage ports. Then the multipathing driver on the host manages which
    path is used from the host to the storage port. In most cases, this results in
    near instant failover if a path fails.<br /><br />If I understand what you&#39;re
    suggesting properly, multipathing would be handed over to the LACP bond. Wouldn&#39;t
    that require significant changes to the FC stacks? And in my experience, LACP
    bonds don&#39;t failover nearly as fast as FC drivers. That could very well have
    implications for the &#39;lossless&#39; requirement of FCoE...<br /><br />-Loren
  id: '1505812363445759208'
  image: https://resources.blogblog.com/img/blank.gif
  name: Loren Gordon
  profile: null
  pub: '2011-12-14T12:24:54.069+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 14 December 2011 12:40
  html: Loren is absolutely correct. For multipathing and failover to work at the
    FC layer (and to work exactly the same as it works today with native FC, which
    is one of the promises--and premises--of FCoE), there have to be two independent
    FC initiators logging into the fabric (actually separate fabrics, so the last
    diagram hardly represents fabric A/B separation) independently, zoned independently
    to two independent FC target ports that present the same LUN through two independent
    controllers. Without multipathing software the host should actually see two identical
    devices. The multipathing software at the SCSI device driver layer will mask that
    into a single SCSI device for the OS and will handle load balancing (active/active
    or active/passive depending on the capabilities of the storage controller) and
    failover/failback transparently, and much faster than can be achieved with LACP.
  id: '2769441477939587167'
  image: https://resources.blogblog.com/img/blank.gif
  name: Juan Tarrio Brocade
  profile: null
  pub: '2011-12-14T12:40:57.080+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 14 December 2011 13:01
  html: Of course you&#39;re both absolutely correct - the current _implementations_
    make perfect sense from the server/storage/HA requirements perspective ... but
    that&#39;s not how they&#39;re supposed to be working according to FC-BB-5.<br
    /><br />Also, the current (non-standard) behavior forces the first switch to be
    FCF (or not to use LAG). Just imagine what would happen if the first switch is
    a regular DCB switch and you use LAG to connect to it.
  id: '2488364516514239368'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-14T13:01:11.457+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 02:20
  html: Hey Ivan,<br /><br />I think there&#39;s a piece missing from your conversation
    (or *I* am missing something, which is entirely probable :) ). The FC-BB-5 standard
    does not specify anything about the underlying MAC address, whether it&#39;s bonded
    or not. This is by design. <br /><br />When FCoE gets its address it does not
    simply use the MAC address of the host. Instead, FCoE uses its own addresses that
    can be mapped wherever you like. In this case, the only difference is that the
    interface MAC address is not the LAG MAC address. <br /><br />Each VN_Port gets
    its own FPMA address, that is uniquely identified by a triplet:<br /><br />MAC
    address of FCoE Device A (bonded or not); MAC address of device B (bonded or not);
    FCoE VLAN ID<br /><br />This has nothing to do with the physical addresses of
    the LAG nor of the interface. <br /><br />Or, am I missing something in your explanation?<br
    /><br />J
  id: '8590984348039774603'
  image: https://resources.blogblog.com/img/blank.gif
  name: J Metz
  profile: null
  pub: '2011-12-15T02:20:00.894+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 02:21
  html: In short, FC-BB-5 is above LAG, and therefore it doesn&#39;t care.
  id: '7857616615484494626'
  image: https://resources.blogblog.com/img/blank.gif
  name: J Metz
  profile: null
  pub: '2011-12-15T02:21:35.421+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 03:38
  html: Ultimately, I think our point is that LACP is not really sufficient for FC
    failover requirements...
  id: '6611303599119945262'
  image: https://resources.blogblog.com/img/blank.gif
  name: Loren Gordon
  profile: null
  pub: '2011-12-15T03:38:09.208+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 04:47
  html: Interesting, hadn&#39;t thought of this. With Cisco UCS as well as many hypervisor
    deployments the issue is somewhat sidestepped. LAG isn&#39;t supported, and traffic
    distribution between two active Ethernet uplinks is handled usually by the hypervisor
    or UCS doing (VM pinning) or active/standby failover without the OS being involved
    (such as Windows baremetal). In either case, the FC traffic is still separated
    A/B.
  id: '6189403673031192120'
  image: https://resources.blogblog.com/img/blank.gif
  name: tbourke
  profile: null
  pub: '2011-12-15T04:47:53.856+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 08:54
  html: In summer I had some (very hard) discussions with Cisco fellow-staff in order
    to get<br />information about etherchannel load balancing; amazingly, TMEs were
    not able to <br />explain (reveal) schema(s) to combine IP and FCoE across the
    same channel.<br />Do you have more details for channels between switches / Nexus:<br
    />&quot;A port based load balancing&quot; =&gt; L1&amp;L2&amp;L3&amp;L4 hashing
    =&gt; asymmetric side utilization...??
  id: '1021341602679444885'
  image: https://resources.blogblog.com/img/blank.gif
  name: 6VPE
  profile: null
  pub: '2011-12-15T08:54:35.291+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 20:18
  html: I hope I don&#39;t come off sounding pedantic; I&#39;m really trying to understand
    where the issue lies.<br /><br />FCoE traffic - at a high level - is just another
    VLAN. Each switch in a VPC (where you have a link going to the two switches) must
    allow the VLAN in order to be able to provide the appropriate connectivity for
    both SAN A and B.<br /><br />So, suppose you have VLAN 101 for SAN A (on Switch
    A) and VLAN 102 for SAN B (on Switch B). Each FCF sees the MAC address behind
    the VLAN for the instantiation of the FCoE_LEP. FPMA provides the address based
    on the triplet I indicated before - in this case the MAC address is the bonded
    MAC address.<br /><br />In this scenario, SAN A traffic does not get forwarded
    to Switch B because the VLAN 101 is not in Switch B&#39;s database; the inverse
    is true for SAN B traffic.<br /><br />So, while my non-FCoE traffic (say, e.g.,
    VLAN 1 and 100 for iSCSI traffic) gets hashed across both switches, the FCoE VLAN
    is forwarded and configured to a particular switch only, thus maintaining the
    separation. <br /><br />Because of this, I don&#39;t see how the standard for
    FC-BB-5 is broken between the document and implementation (addressing is still
    based upon the presented MAC address), and LAG bonding is still maintained for
    the port.<br /><br />Again, it&#39;s entirely possible that I&#39;m missing your
    point here, so I apologize if it&#39;s right in front of me and I just can&#39;t
    see it.
  id: '7925030169578889402'
  image: https://resources.blogblog.com/img/blank.gif
  name: J Metz
  profile: null
  pub: '2011-12-15T20:18:07.932+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 20:30
  html: What they said. FCoE multipathing (HA and perf) exists above anything you&#39;re
    doing with Ethernet including LAg and that&#39;s a good thing. I think you&#39;re
    reading too much into the FC-BB-5 standard to assume they intend to recommend
    (or not) LAg.
  id: '4094356239331239903'
  image: https://resources.blogblog.com/img/blank.gif
  name: Stephen Foskett
  profile: null
  pub: '2011-12-15T20:30:28.331+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 20:36
  html: Now you nailed it. FCoE _should_ exist above the petty Ethernet things, but
    it doesn&#39;t. The current implementation exists _by the side of_ petty Ethernet
    things.
  id: '9029803648781537226'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-15T20:36:04.945+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 20:39
  html: 'Your paragraph #3 is a great description of (one of) the problem(s). VLAN
    list should match on all port-channel ports, but doesn&#39;t because Switch A
    uses VLAN 101 and Switch B uses VLAN 102.<br /><br />LAG should be one logical
    link from Ethernet perspective, with one MAC address, and all the link parameters
    (including speed, VLAN list ...) should match between LAG members. In FCoE case,
    that&#39;s not true because of NIC/HBA separation in CNA.'
  id: '5397526462292374502'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-15T20:39:04.921+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 20:39
  html: Ahmmm ... can&#39;t count anymore. Seems to be Para#4  :-E
  id: '8038527106494099726'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-15T20:39:49.561+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 21:32
  html: Ivan,<br />I&#39;m scratching my head here because I still don&#39;t understand
    the problem.  LAG from server to switch has nothing to do with how the FC topology
    is viewed by the FC stack on the server.  When the CNA is connected via LAG to
    the FCoE switch, the LAG is only visible to the Ethernet/IP topology, not the
    FC topology.  What am I missing?<br /><br />Cheers,<br />Brad
  id: '1245131702964333325'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2011-12-15T21:32:33.616+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 21:44
  html: You just said it - LAG is not visible to FCoE, just to IP ... But IP and FCoE
    should both be above the same Ethernet link, be it a simple 10GE link or a LAG.
    As Stephen wrote, FCoE shpuld be _above_ petty Ethernet things.<br /><br />Do
    I make more sense now? If not, I&#39;m giving up ;) If I can&#39;t explain myself
    in a way that you&#39;d understand, I have no chance whatsoever to explain it
    to anyone else.
  id: '3863870320843452411'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-15T21:44:12.201+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 21:53
  html: Thank you for this article and drawings!  It really helps to visualize why
    and how things are the way they are.  <br /><br />This was a bit of a confusing
    topic a few months ago.  Even more so when you through VMware into the mix.  <br
    /><br />As @tbourke mentioned, and Ivan you have blogged about - http://goo.gl/Ky7iP
    - LAG isn&#39;t supported the vSwitch and distribution between two active uplinks
    is handled usually by the hypervisor.  Configuration on both the VMware side and
    the switch side wasn&#39;t as simple as anyone expected.
  id: '9030533831761570022'
  image: https://resources.blogblog.com/img/blank.gif
  name: dj spry
  profile: null
  pub: '2011-12-15T21:53:24.094+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 22:01
  html: OK, I see your point, but I completely disagree that you would want the FC
    stack on the server to use the LAG.  You don&#39;t want that at all for the reasons
    others have pointed out already.  The upstream vFC ports on the FCoE switch are
    not configured as LAG, therefore its not a LAG in the context of FC-BB-5.<br />It&#39;s
    when you make switch-to-switch FCoE LAG connections when the FC-BB-5 language
    about LAG is applicable.
  id: '6643706341297618962'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2011-12-15T22:01:10.940+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 22:10
  html: We&#39;re in perfect agreement that you wouldn&#39;t want to see FCoE over
    LAG ... But then you should not be allowed to use LAG with FCoE on the same link.<br
    /><br />Also - don&#39;t you think it&#39;s weird that we run one L3 protocol
    (FCoE) over physical interfaces and another one (IP) over port-channel interfaces?
    Does it sound right to be able to configure inconsistent parameters on port channel
    members?<br /><br />As for &quot;FC-BB-5 language is applicable to inter-switch
    links&quot;, I thought FC-BB-5 was _the_ standard defining all of FCoE ;))
  id: '4869553595479006164'
  image: https://resources.blogblog.com/img/blank.gif
  name: Anonymous
  profile: null
  pub: '2011-12-15T22:10:36.663+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 22:12
  html: BTW, I was that &quot;Guest&quot;. Stupid iPad can&#39;t share Safari cookies
    across in-app instances.
  id: '7434592487053067055'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-15T22:12:18.329+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 22:34
  html: I guess I don&#39;t see that as &quot;weird&quot;.  LAG helps one (IP), and
    breaks the other (FC), so why dumb down the whole network to the lowest common
    denominator when you don&#39;t need to do that? *That* to me, is &quot;weird&quot;
    :-)
  id: '7174673510400841377'
  image: https://resources.blogblog.com/img/blank.gif
  name: Brad Hedlund
  profile: null
  pub: '2011-12-15T22:34:28.257+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 15 December 2011 22:38
  html: Remember that standards are a way of how to solve a particular problem. If
    you have a problem that a standard doesn&#39;t solve, you don&#39;t need to use
    it.<br /><br />Conversely, if the standard doesn&#39;t solve the problem you have,
    then you are free to determine your own solution.<br /><br />This is just a generic
    message and in no way contradicts my earlier statements. :P
  id: '8998051540238305028'
  image: https://resources.blogblog.com/img/blank.gif
  name: J Metz
  profile: null
  pub: '2011-12-15T22:38:52.468+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 16 December 2011 14:34
  html: 'I agree with most of what Loren, Brad, Juan and J have already said.  FCoE
    was designed to work within the framework of FC.  As a result multipathing is
    handled by MPIO (e.g., PowerPath) and not via physical link aggregation.   <br
    /><br />In addition, I&#39;d also like to point out that due to the formatting
    of the &quot;FC-BB-5 nitpicking&quot; paragraph in your original post, a reader
    may incorrectly conclude that FC-BB-5 mentions &quot;Link Aggregation&quot; and
    it does not, FC-BB-5 (the standard that defines FCoE) only mentions &quot;Ethernet
    MAC&quot; and &quot;Lossless Ethernet MAC&quot;. FC-BB-5 says no more on the topic
    because it would have been inappropriate for FC-BB-5 (a T11 working group) to
    say anything more about a topic (Ethernet MACs) that are defined by IEEE.  With
    this in mind, one should conclude that the &quot;MAC Client&quot; referenced in
    section 5 of 802.1AX and an &quot;FCoE ENode&quot; are different ways of describing
    the same thing.  <br /><br />That having been said, I don&#39;t see any relevant
    text in 802.1AX that indicates all MAC Clients using the same MAC need to be aggregated
    in the same manner.  Additionally, you specifically referenced the following text:
    <br /><br />&quot;A MAC Client communicates with a set of ports through an Aggregator,
    which presents a standard IEEE 802.3 service interface to the MAC Client.&quot;<br
    /><br />I would like to point out that later on in this same section the following
    text is included:<br /><br />&quot;This standard does not impose any particular
    distribution algorithm on the Distributor. Whatever<br />algorithm is used should
    be appropriate for the MAC Client being supported.&quot;<br /><br />Therefore,
    since distributing FCoE frames across multiple physical links would not be appropriate
    for the MAC Client (FCoE ENode), it is not done by the Distributor.<br /><br />BTW,
    if you want to see an example of Teaming/Bonding and FCoE coexisting quite happily,
    take a look at the &quot;FCoE Tech Book&quot; and the &quot;Nexus 7000, Nexus
    5000, and MDS 9500 series topology&quot; case study.'
  id: '1275910098067434011'
  image: https://resources.blogblog.com/img/blank.gif
  name: Erik Smith
  profile: null
  pub: '2011-12-16T14:34:16.997+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 16 December 2011 15:28
  html: Hi Erik,<br /><br />Thanks for your comment. This is the first comment that
    really addresses my concerns. I agree that one could read the 802.1AX standard
    in the way you interpret it. <br /><br />You might still face an interesting problem
    if the first-hop switch is a DCB-capable switch w/o FC stack (and potentially
    even without FIP snooping), distributing FCoE frames across LAG at will ... but
    hopefully we&#39;ll eventually come to a point where everyone agrees that doesn&#39;t
    make too much sense.<br /><br />Ivan
  id: '3546390796173331461'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-12-16T15:28:03.773+01:00'
  ref: '1745345160016018440'
  type: comment
- date: 16 December 2011 17:12
  html: Believe it or not, Ivan, I completely agree with you about the dangers of
    placing a DCB switch inbetween a host and FCF. I&#39;m going to make sure that
    I keep LAG as one of those reasons.<br /><br />Ultimately, your bonded interface
    only brings up half the issue. We need to tie the process to the SCSI process
    at the OS level. SCSI requires *one* single path. <br /><br />Multiple paths for
    SCSI operation have been a thorn in the side for ages. I&#39;m told that it has
    to do with nanosecond (or tighter) timing, so that there has to be some referee
    to ensure that bits are to/from the SCSI stack in guaranteed order or risk corruption.
    <br /><br />Array vendors have developed multipathing software to sit in-between
    the HBAs and the OS. If the OS/SCSI can&#39;t deal with 2 paths natively, how
    would FC-BB-5 solve this without some form of middleware?<br /><br />Erik, of
    course, sitting on the T11 committee has a much greater understanding of the text
    than I do (I&#39;m just a on-again, off-again observer in the meetings). But FWIW
    Claudio DeSanti mentioned last night that:<br /><br />&quot;A standard is violated
    when an *explicit* requirement (i.e., a &quot;shall&quot; statement) is not observed.
    To state that a standard is violated he has to point out a specific &quot;shall&quot;
    statement that is not observed. Everything else is not a violation of a standard.&quot;<br
    /><br />I know it&#39;s semantics and nitpicks, but then again, that&#39;s precisely
    what standards *are* - semantics and nitpicks.  8-)
  id: '1069808377162690001'
  image: https://resources.blogblog.com/img/blank.gif
  name: J Metz
  profile: null
  pub: '2011-12-16T17:12:23.410+01:00'
  ref: '1745345160016018440'
  type: comment
count: 24
id: '1745345160016018440'
type: post
url: 2011/12/fcoe-and-lag-industry-wide-violation-of.html
