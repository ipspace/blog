{
  "comments": [
    {
      "date": "09 December 2011 07:36",
      "html": "Great post Ivan. What methods are available to provide scalable L3 isolation between the virtual L2 networks? MPLS/VPN and VPLS would seem reasonable though obviously not an available solution for vswitch at this point, not to mention cost of hardware goes up, choice of vendors goes down, and I don&#39;t believe there are any high port density 10G ToR switches available with the necessary protocol support. What other scalable options are there?",
      "id": "5054121377852564280",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "sh0x",
      "profile": null,
      "pub": "2011-12-09T07:36:41.879+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "09 December 2011 22:00",
      "html": "I&#39;m actually not convinced Amazon does something like that, at least for the majority of their VMs. Remember, they don&#39;t support any VM or IP mobility. Their solution to maintenance on a host box is just to reboot or kill all the VMs running on the box. As a result, they can build an incredibly simple, aggregatable, entirely L3-routing based network. I wouldn&#39;t be surprised if the routing begins on the host itself, using proxy-ARP to get the packets.<br /><br />It&#39;s true that Virtual Private Cloud lets you bring your own addresses and has more flexibility in assigning them, but even there I don&#39;t believe they give you an L2 domain with the usual broadcast/multicast semantics -- it&#39;s just a bunch of machines with IPs from the same subnet. They probably use IP-in-IP or IP-in-MPLS to handle it; I strongly doubt ethernet headers get beyond the virtualization host.",
      "id": "8503784297308023182",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ben Jencks",
      "profile": null,
      "pub": "2011-12-09T22:00:41.658+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "09 December 2011 22:02",
      "html": "Ivan, nice write up.  I wouldn&#39;t mind hearing your thoughts and speculation on using CAPWAP between virtual switches.  Cough cough Nicira.  Maybe it will help with de-coupling the control plane reliance from the physical network, but not so helpful with VLAN scale?",
      "id": "2745807074903287510",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "@jedelman8",
      "profile": null,
      "pub": "2011-12-09T22:02:53.823+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "10 December 2011 13:42",
      "html": "Amazon runs IP-over-IP with L3 switch in the hypervisor (or something equivalent). Follow the link in the blog post for more details.",
      "id": "1824277383451101554",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-12-10T13:42:15.658+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "10 December 2011 13:43",
      "html": "I don&#39;t think Nicira uses more than CAPWAP encapsulation in the Open vSwitch. They seem to be relying exclusively on P2P inter-hypervisor tunnels and use whatever encapsulation comes handy, be it GRE, CAPWAP or VXLAN.",
      "id": "6037274524173496099",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-12-10T13:43:38.335+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "12 December 2011 20:47",
      "html": "As long as you stay in the L2 domain (be it VLANs, VXLAN-based virtual networks or anything else), the L3 isolation is automatic. Once you cross L3 boundaries and want to keep L3 path separation, MPLS/VPN is the only widely-available scalable technology (MultiVRF doesn&#39;t scale).",
      "id": "1882167402519559901",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-12-12T20:47:11.167+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "18 December 2011 18:31",
      "html": "I am not sure I understand why using Core Multicast for L2 flooding is a bad thing, that needs to be avoided?   Any modern Core IP platform is designed to handle large-scale Multicast forwarding, and majority of Enterprise datacenter Core networks are already Multicast enabled.   <br /><br />Why invent something new and kludgy (like full mesh of hypervisor tunnels), when the most efficient IP-based solution has already been invented and proven in real networks?   <br /><br />Not to mention, Unicast-based flooding will be inherintly less efficient than Multicast-based.  Think about it - a hypervisor that needs to flood a frame to 10 other hypervisors needs to send that frame to the IP Core (to Multicast group for that L2 domain) just once.. as opposed to forwarding that frame 10 times via Unicast across 10 tunnels.    Multicast was invented for this.",
      "id": "5190316074188024335",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "hitekalex",
      "profile": null,
      "pub": "2011-12-18T18:31:45.759+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "19 December 2011 20:02",
      "html": "Arista: 4000 multicast entries per linecard<br />Nexus 7000: 32000 MC entries, 15000 in vPC environment<br />Nexus 5548: 2000 (verified), 4000 (maximum)<br />QFabric: No info. OOPS?<br /><br />Not that much if you want to have an IP MC group per VNI. A blog post coming in early January (it&#39;s already on the to-write list).",
      "id": "6306299933545288674",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2011-12-19T20:02:19.144+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "04 January 2012 20:50",
      "html": "The latter example of MAC encapsulation struck me as an interesting place to apply some form of SDN. Instead of applying solutions like OpenFlow to the transport network, VM vendors could implement (ideally) open SDN APIs in the hypervisor networking, separating physical networking from virtual networking.<br /><br />Though I&#39;m neither an SDN or a virtualisation guy, so this is likely nothing new.",
      "id": "1467888061095769584",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Mikkel Shinn",
      "profile": null,
      "pub": "2012-01-04T20:50:47.357+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "04 January 2012 21:41",
      "html": "... this is exactly how I understand Open vSwitch/Nicira controller to work. OVS already supports OpenFlow API.",
      "id": "6913145374235468753",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Ivan Pepelnjak",
      "profile": null,
      "pub": "2012-01-04T21:41:36.660+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    },
    {
      "date": "04 January 2012 21:53",
      "html": "Heh, I&#39;ll be exercising my caveat now.",
      "id": "1802950747854766816",
      "image": "https://resources.blogblog.com/img/blank.gif",
      "name": "Mikkel Shinn",
      "profile": null,
      "pub": "2012-01-04T21:53:56.335+01:00",
      "ref": "3744124764335416954",
      "type": "comment"
    }
  ],
  "count": 11,
  "id": "3744124764335416954",
  "type": "post",
  "url": "2011/12/decouple-virtual-networking-from.html"
}