comments:
- date: 14 October 2011 15:01
  html: "You say no VMotion was used, but the article states: <br /><br />&quot;Thursday\
    \ night I completely failed the core datacenter operations over to the recovery\
    \ servers using a combination of Veeam Replication and VMware migrations that\
    \ in the end, really didn\u2019t need to happen.&quot; <br /><br />Of course you\
    \ can&#39;t tell at the time that it didn&#39;t need to happen, but VMotion was\
    \ part of the DR plan and COULD have been necessary if conditions were slightly\
    \ worse.<br /><br />Also, I have a question.  I don&#39;t know exactly how VMotion\
    \ operates.  Is it possible that the 10 ms RTT restriction might be relaxed in,\
    \ say, three or four years?"
  id: '4387069385181961867'
  image: https://resources.blogblog.com/img/blank.gif
  name: Tom Zeller
  profile: null
  pub: '2011-10-14T15:01:56.101+02:00'
  ref: '3175897294389420778'
  type: comment
- date: 14 October 2011 20:20
  html: '&#39;VMware migration&#39; doesn&#39;t necessarily mean VMotion, and I expect
    in the context of the article it means a cold migration of a powered-off VM to
    a new host and new datastore.'
  id: '2516701521069885903'
  image: https://resources.blogblog.com/img/blank.gif
  name: Chuck
  profile: null
  pub: '2011-10-14T20:20:50.046+02:00'
  ref: '3175897294389420778'
  type: comment
- date: 15 October 2011 10:12
  html: He also says he has 100Mb circuit between sites. vMotion requires at least
    600 Mbps (1 Gbps is recommended). It might work over lower-speed links, but not
    likely (page change rate is too high).<br /><br />As for RTT, it&#39;s actually
    the bandwidth-delay-product problem. You have to copy memory pages to the other
    vSphere host faster than the VM changes them and that&#39;s hard to do if you
    have low-bandwidth or high-latency link. Can it be done? Sure. Will they do it?
    I hope not ;)
  id: '2416318175119527949'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-10-15T10:12:53.320+02:00'
  ref: '3175897294389420778'
  type: comment
- date: 19 October 2011 02:25
  html: vMotion is so cool to see, that it really has set some rough expectation management
    issues.  <br />  <br />While I do have customers, deploying 10G networks &lt;100KM
    who plan to do live vMotion the truth is that for most customers pause/stop-&gt;sync-&gt;resume
    in new location is MORE than enough to meet the business need. And by accepting
    a 30, 60, even 300s window the complexity level goes WAY down and the distances
    supported go WAY up. And no unicorns are harmed ;-)
  id: '3877338809933877146'
  image: https://resources.blogblog.com/img/blank.gif
  name: Jon Hudson
  profile: null
  pub: '2011-10-19T02:25:51.715+02:00'
  ref: '3175897294389420778'
  type: comment
- date: 29 October 2011 12:13
  html: I think they&#39;re playing with it. I have a nice (Cisco internal) slide
    talking about VMotion over 250ms (!) link. Sure, 10Gbit and so on.
  id: '8241460553508580393'
  image: https://resources.blogblog.com/img/blank.gif
  name: Drunken Pole
  profile: null
  pub: '2011-10-29T12:13:10.726+02:00'
  ref: '3175897294389420778'
  type: comment
- date: 29 October 2011 19:52
  html: 'The root cause of the problem is the bandwidth-delay product: how fast can
    you push memory image across the WAN link while the VM changes its memory. <br
    /><br />The actual results depend on the BW, delay and VM page change rate. You
    could probably get reasonable results with WAN acceleration (optimizing TCP and/or
    compressing vMotion data like F5 is doing) if the VM is not doing anything (in
    which case, why would you want to move it at all ;) ).'
  id: '1902317467205216024'
  image: https://resources.blogblog.com/img/blank.gif
  name: Ivan Pepelnjak
  profile: null
  pub: '2011-10-29T19:52:14.892+02:00'
  ref: '3175897294389420778'
  type: comment
count: 6
id: '3175897294389420778'
type: post
url: 2011/10/follow-sun-workload-mobility-get-lost.html
