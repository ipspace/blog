<div class="comments post" id="comments">
  <h4>15 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="3418419792606616042">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Brad Hedlund</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3418419792606616042" href="#3418419792606616042">11 October 2011 02:42</a>
              </span>
            </div>
            <div class="comment-content">Ivan, just thinking out load here but I think as network guys we&#39;re going to have to get past the &quot;extra hops and unpredictable traffic flows&quot; hang up.  The paths and hops look ugly if you look at it from the perspective of the physical network, but it&#39;s all perfectly normal from the perspective of the virtual network.  The physical network needs to evolve to east-west non-blocking architectures to cope with network virtualization.  If the &quot;extra hops&quot; are really a problem, we need to be clear on why those are problem, not just &#39;because it looks ugly on a drawing&#39;.  If the latency is low and the bandwidth non-blocking, why are extra *physical* hops bad?  Just playing devils advocate (kinda) ;-)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3051654315083710469">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Sallie Chait</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3051654315083710469" href="#3051654315083710469">11 October 2011 05:10</a>
              </span>
            </div>
            <div class="comment-content">Just a thought, how robust is this technology when sites are failing over and chaos rules. Seems like an outage could cause a disconnect for software to recover in a robust fashion.  All for Network Virtualization, need to experiment here.  8-)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="737007402682832276">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Tony Bourke</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c737007402682832276" href="#737007402682832276">11 October 2011 07:40</a>
              </span>
            </div>
            <div class="comment-content">One of the biggest drawbacks to virtual appliance versions of load balancers (or, lovingly, application delivery controllers) is the lack of SSL crypto hardware. Most load balancers today employ some type of SSL ASIC to handle the cryptographically intensive asymmetric encryption (RSA) that occurs at the start of any SSL/TLS connection. <br /><br />Intel recently added AES-NI to its server processor lineup (it&#39;s in the new E7&#39;s and 5600 series Xeon), however they only handle symmetric, not asymmetric. <br /><br />So as Ivan said, it&#39;s going to chew up a lot more CPU cycles than would otherwise be chewed.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1534361369543731927">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1534361369543731927" href="#1534361369543731927">11 October 2011 08:17</a>
              </span>
            </div>
            <div class="comment-content">Well, I wouldn&#39;t use VXLAN (or any other L2 technology) between data centers. It&#39;s a nice mechanism to implement many virtual segments within a single failure domain (availability zone), if you want to go beyond that, you need proper application architecture.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1638227976031599565">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1638227976031599565" href="#1638227976031599565">11 October 2011 08:19</a>
              </span>
            </div>
            <div class="comment-content">The &quot;only&quot; reasons I dislike spaghetti-like virtual flows are:<br /><br />(A) troubleshooting complexities<br />(B) increased network utilization<br /><br />I don&#39;t really care about N/S shifting to E/W. That&#39;s happening anyway and needs to be solved, but wasting bandwidth is a different story.<br /><br />Of course, if you have too much bandwidth in your DC and too many CPU cycles (so you can do routing in VM appliances), you might not care.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1873456099160354402">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Andrew</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1873456099160354402" href="#1873456099160354402">11 October 2011 14:16</a>
              </span>
            </div>
            <div class="comment-content">Interesting comment, especially in light of how much my System Admins would love the same subnets at both our data centers.  Is there a good solution for allowing hosts to migrate between data centers that don&#39;t share layer-2 adjacency via any technology (VLAN, VxLAN, etc)?  Maybe LISP?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="1499181114194979818">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Michal Zawirski</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1499181114194979818" href="#1499181114194979818">12 October 2011 13:48</a>
              </span>
            </div>
            <div class="comment-content">Ivan, while I (and probably 99% of network engineers) dislike spaghetti flows exactly for the reasons you mentioned, I agree with Brad’s point here.  In virtualized / cloud environments we are going to see fewer and fewer “clean” designs (as depicted on the left side of your diagram), with well separated roles aligned with the physical network topology.  The network paths should be deterministic (moving virtual appliances around as load changes =&gt; not necessarily a good idea) and performance (incl. latency) needs to be kept under control, but otherwise I would not care about the number of physical hops.<br /><br />I think we’re more likely to see a shared/virtualized pool of physical appliances (loadbalancers with SSL, firewalls, etc), connected to the “network fabric” somewhat like service linecards in a 6500 chassis (and hopefully supporting VXLAN termination natively at some point to avoid the L2 issues you described).<br /><br />Still, VXLAN termination in hardware may help keep the spaghetti slightly less convoluted.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3498665136978892877">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3498665136978892877" href="#3498665136978892877">16 October 2011 20:40</a>
              </span>
            </div>
            <div class="comment-content">Read this post first http://blog.ioshints.info/2011/09/long-distance-vmotion-for-disaster.html, and discuss the cost issue with the server admins ;)<br /><br />Once implemented properly, LISP will solve the IP address mobility problem, but not all the others.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5100007952034316472">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Rati Jokhadze</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5100007952034316472" href="#5100007952034316472">23 October 2011 21:37</a>
              </span>
            </div>
            <div class="comment-content">Gr8 post , thanks ivan</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6166273361663334849">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/14053980077799286588" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6166273361663334849" href="#6166273361663334849">09 August 2012 15:15</a>
              </span>
            </div>
            <div class="comment-content">According to Cisco insiders, ASA Firewall will implement VXLAN to VLAN Gateway mecanisms. <br />But when, and on which type of ASA ?</div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5210383030421651525">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5210383030421651525" href="#5210383030421651525">09 August 2012 15:42</a>
              </span>
            </div>
            <div class="comment-content">I would suggest you ask the above-mentioned Cisco insiders.</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6231984704896536981">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6231984704896536981" href="#6231984704896536981">26 August 2015 10:38</a>
              </span>
            </div>
            <div class="comment-content">A new open source tool Ubridge which can be downloaded under (inside iptools) sourceforge is available for bridge local Wundows machine to vxlan.  Pls refer to this webpage apps.e5link.com/blog</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3156088908462630538">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/01547434616127430094" rel="nofollow">Unknown</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3156088908462630538" href="#3156088908462630538">08 September 2015 01:41</a>
              </span>
            </div>
            <div class="comment-content">Ivan, is this still relevant to today&#39;s date? I have a datacenter which is currently using VRF to separate routing tables for different environments. They route out to their own environment&#39;s firewalls. I was thinking about using VXLAN to remove the VRF and VLAN configuration - but this would be an issue since they want to use their existing gear. </div>
              <div class="comment-replies">
                <div class="comment-thread inline-thread">
                  <span class="thread-count"><a>Replies</a></span>
                    <ol>
      <div>
        <li class="comment" id="5741496218013592862">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5741496218013592862" href="#5741496218013592862">11 September 2015 17:23</a>
              </span>
            </div>
            <div class="comment-content">Louis, the product information is totally out-of-date. Read more recent VXLAN posts for some details, will publish a comprehensive overview in near future.<br /><br />However, I wouldn&#39;t go down the VXLAN-VLAN-FW path (although you could). Why don&#39;t you virtualize the firewall?</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3988773330856352070">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="https://www.blogger.com/profile/13457151406311272386" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3988773330856352070" href="#3988773330856352070">14 September 2015 19:57</a>
              </span>
            </div>
            <div class="comment-content">Here&#39;s the blog post with more recent information (and plenty of comments):<br /><br />http://blog.ipspace.net/2015/09/vxlan-hardware-gateway-overview.html</div>
          </div>
        </li>
      </div>
  </ol>

                </div>
              </div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
