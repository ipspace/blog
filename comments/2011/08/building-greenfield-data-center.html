<div class="comments post" id="comments">
  <h4>12 comments:</h4>
  <div class="comments-content">
    <div class="comment-thread">
        <ol>
      <div>
        <li class="comment" id="1789144820353984505">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous Coward</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c1789144820353984505" href="#1789144820353984505">01 August 2011 13:50</a>
              </span>
            </div>
            <div class="comment-content">Hi Ivan,<br /><br />I just can&#39;t seem to understand your trepidation around layer 2 interconnects.  Yes, I agree layer 3 has it&#39;s benefits; however with STP mitigation through VPLS, MPLS techniques or trill/fabric path I cannnot accept your arguments that appropriately designed L2 DCI has no place in a well designed data centre.  It is very rare that you walk into an enterprise that does not have a multitude of L2 requirements and saying &quot;I like layer 3&quot; does not form a valid business case to migrate away from these legacy requirements.<br />I&#39;m not sure what bridging issues you refer to after STP.  Yes we need to be cognizant of mac table sizes, but not all switches need to learn macs with well designed layer 2.  Broadcast domains are something to keep in mind, but I would say that modern NICs, application behavior and bandwidths that we would be sepeating hosts based on security isolation purposes before we hit any broadcast issues these days.  Fault isolation, well after the STP issue we basically fall to broadcast storms from dodgy hosts, but most modern switches offer mechanisms to deal with this.<br />Keep in mind that I&#39;m approaching this from an enterprise environment perspective, and that I agree that layer 3 should be the default position.  But layer 2 DCI often forms part of the puzzle and with very good results.<br />So why so down on layer 2?<br />And yes, I have seen L2 meltdowns in my time, but everyone was the result of poor design, which cannot be solved with any interconnect solution.  :-P</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6827906787713287377">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Haakon</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6827906787713287377" href="#6827906787713287377">01 August 2011 14:17</a>
              </span>
            </div>
            <div class="comment-content">Working in the telco space, thankfully with an MPLS network. I&#39;ve seen first hand what extended layer 2 networks can do.<br /><br />1) That L2 broadcast storm just took down your sites rather than just one.<br />2) Someone digs up your L2 interconnect and you now have devices at each site that expect to see other servers in their lan that have just gone away.<br /><br />I could go on...</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="9187045102175420945">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous Coward</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9187045102175420945" href="#9187045102175420945">01 August 2011 17:03</a>
              </span>
            </div>
            <div class="comment-content">Enterprise space mainly... But many SPs I&#39;m aware of successfully implement large layer 2&#39;s<br />1)Broadcast storm control - although I&#39;m not suggesting you completely ignore broadcast domain sizing, we are simply talking about extending them.<br />2) path diversity and careful review of split brain failure modes.<br /><br />Please go on... That&#39;s what I&#39;m interested in.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="710979035371315774">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Tony Bourke</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c710979035371315774" href="#710979035371315774">01 August 2011 20:06</a>
              </span>
            </div>
            <div class="comment-content">In terms of iSCSI/NFS, some are still under the impression that FC is a faster protocol than FC. It&#39;s not. Even without jumbo frames, iSCSI is competitive with FC. <br /><br />The storage array itself makes much more of a difference in what your performance will be (number of spindles, caching, SSD, etc.)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5829481514554359732">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Tony Bourke</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5829481514554359732" href="#5829481514554359732">01 August 2011 20:06</a>
              </span>
            </div>
            <div class="comment-content">In terms of iSCSI/NFS, some are still under the impression that FC is a faster protocol than FC. It&#39;s not. Even without jumbo frames, iSCSI is competitive with FC. <br /><br />The storage array itself makes much more of a difference in what your performance will be (number of spindles, caching, SSD, etc.)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="5618615931588556769">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Tony Bourke</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c5618615931588556769" href="#5618615931588556769">01 August 2011 20:06</a>
              </span>
            </div>
            <div class="comment-content">In terms of iSCSI/NFS, some are still under the impression that FC is a faster protocol than FC. It&#39;s not. Even without jumbo frames, iSCSI is competitive with FC. <br /><br />The storage array itself makes much more of a difference in what your performance will be (number of spindles, caching, SSD, etc.)</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="6151068253841846855">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">NotSoAnonymous</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c6151068253841846855" href="#6151068253841846855">02 August 2011 00:23</a>
              </span>
            </div>
            <div class="comment-content">&quot;Overhead cable trays&quot;, &quot;Raised Flooring&quot;....Are the days for patch panels over!!!??? :&#39;(<br /><br />I though patch panels are easiest and cleanest cabling solution! Maybe expensive but its only 1 time..</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2625955582715189288">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">AC2</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2625955582715189288" href="#2625955582715189288">02 August 2011 11:24</a>
              </span>
            </div>
            <div class="comment-content">I&#39;ll tackle this from the other end.<br /><br />Of course you can do this but the point is you end up with another L3. When you have a system that was designed to do this, is field-proven for 20 years and everyone is familiar with, why do you want to reinvent the wheel again?<br /><br />Do you live in the dark ages and still run non-ip protocols? Or a gem like MS NLB? Tell me, what problem do you solve on L2 that you can&#39;t on L3?<br /><br />I mean, why would anyone sane want to &quot;migrate away from these legacy requirements&quot;, given the option of building DC from scratch? I guess you answered your own question.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="8177605452962550532">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous Coward</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c8177605452962550532" href="#8177605452962550532">02 August 2011 14:02</a>
              </span>
            </div>
            <div class="comment-content">Still no answer there I&#39;m afraid.  Why would I want a layer 2 DCI : for the same reason that layer 2 DCI is so common, and so much work has gone into making them scalabe<br />- Stretched Cluster<br />- VM Mobility.  Yes there are lots of complex ways to make this work, and yes we are talking about geographically close DCs here.  But were talking DCI not DR and enterprise.  Until LISP picks up pace L2 is going to be the go too tool here for some time.<br />- transporting FCoE.  yes people have invested lots if money in FC, and no iSCSI and NFS do not offer feature parity, and simple integration the way FCoE do.  And before we start flaming FCoE as well, keep in mind that all the major vendors are getting into this game for a reason, because it is making realistic headway as an FC replacment unlike all the other predecessors.<br /><br />With regards to the &quot;green fields DC&quot;,  I have been involved in many a new DC build, and not one of them started with throwing out all the applications and operating practices that have been with an organization since it&#39;s inception.  Not every app is web app.<br /><br />I&#39;m not interested in starting a religious debate here, I implement both L2 and L3 designs, which one depends on the specific requirements at hand - I also own a mac and pc, and use a windows phone yet am typing this on my iPad  :)<br /><br />Prior to TRILL, I have been bridging the divides with MPLS/VPLS, however now it&#39;s here, that a complexity I can avoid in my DCI.<br /><br />So the question stands.  Why not layer 2?<br /><br />Oh, and I know of lots of people that have been sending letters for 20 years and are familiar with it, but e-mail is just sooo much easier.   :-P</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="3908575845587192985">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Ivan Pepelnjak</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c3908575845587192985" href="#3908575845587192985">02 August 2011 14:16</a>
              </span>
            </div>
            <div class="comment-content">Stretched cluster is a patently bad idea: http://blog.ioshints.info/2011/06/stretched-clusters-almost-as-good-as.html. You&#39;re betting two data centers on the availability of DCI link.<br /><br />Inter-DC VM mobility is a bogus requirement. If you have scale-out application, you don&#39;t need it, if you have a non-scalable application, you can&#39;t achieve high availability anyway.<br /><br />FCoE cannot be transported across anything else but dark fiber (due to lack of PFC) and even there it&#39;s highly limited. http://blog.ioshints.info/2010/11/fcoe-between-data-centers-forget-it.html<br /><br />In my personal opinion, the L2 DCI requirements almost always come from someone who tries to offload his problems and/or bad decisions to the network.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="9152351448014094253">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Anonymous Coward</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c9152351448014094253" href="#9152351448014094253">02 August 2011 14:44</a>
              </span>
            </div>
            <div class="comment-content">Ok.. So make your DCI available - diverse physical path.  And if the DCI does split make sure one side wins.  That fixes your stretched cluster concerns.  Remember why stretch these clusters - it&#39;s easier.<br /><br />VM mobility is a bogus requirement?????  Are we living in the same world.  I&#39;m not suggesting that vmotion will solve my availability requirements, but if my storage is replicated, particularily synchronously, then getting that vm back online quick smart at my second DC is a very positive attribute (and in my view tools like SRM are a pain in the butt in reality).  And aide from that, people like t, its flexibke, its easy, it gives you options.  Oh, and I&#39;m glad to hear that you have the luxury of re-architecting ever app to match your DCI statergy.<br /><br />And as far as FCoE is concerned, if I was replacing my FC SAN which is replicating natively (not via a storage routing solution) then guess what I have, dark fibre, but now I have a single set of switches to manage.  Yes there is still a ways to go, and no it won&#39;t fix ever problem, but t works.<br /><br />And too the last point, that&#39;s what networks have been doing for years, providing solutions to problems to enable the deployment of tools that enable the enterprise.  We build networks to support apps, not the other way round, and from experience, thats the way hierarchy works in most organizations.<br /><br />Still.  Why no layer 2?  Beyond all the other points that hav been raised, I have a genuine curiosity into the problems that others perceive.<br /><br />My view of l2 v l3 is the same as many other features and functions we have in modern kit (FCoe, iSCSI, gre, DMVPN, ospf, bgp, hsrp, nat, etc etc etc), use the tool that makes sense for the problem at hand.  That includes consideration of technical, organizational and financial constraints.  I think that saying never layer 2 is just as short sighted as saying always l2.</div>
          </div>
        </li>
      </div>
      <div>
        <li class="comment" id="2771374063333837326">
          <div class="comment-block">
            <div class="comment-header">
              <cite class="user"><a href="" rel="nofollow">Paul</a></cite>
              <span class="datetime secondary-text">
                <a rel="nofollow" id="c2771374063333837326" href="#2771374063333837326">31 August 2011 00:50</a>
              </span>
            </div>
            <div class="comment-content">Any thoughts on the pro&#39;s and con&#39;s of using dedicated backup networks and in-band management (e.g. rdp, ssh) networks that connect into each server? I&#39;ve seen it done in several data centres but the major down size is that a static routes then need to be created on the server. It feels like a left over from the days of 100Mbit/s networks where bandwidth was an issue but nevertheless it still seems like a popular design choice. Interested in how backup and in-band management should be designed into a greenfield data centre.</div>
          </div>
        </li>
      </div>
  </ol>

    </div>
  </div>
</div>
