---
title: "Is OSPF or IS-IS Good Enough for My Data Center?"
date: "2018-05-15T11:45:00.000+02:00"
tags: [ Design,Data Center,IP routing ]
---

<p>Our good friend mr. Anonymous has too many buzzwords and opinions in his repertoire, at least based on this comment he left on my <a href="http://blog.ipspace.net/2018/05/using-4-byte-bgp-as-numbers-with-evpn.html">Using 4-byte AS Numbers with EVPN</a> blog post:</p>
<blockquote class="cite">But IGPs don't scale well (as you might have heard) except for RIFT and Openfabric. The others are trying to do ECMP based on BGP.</blockquote>
<p>Should you be worried about OSPF or IS-IS scalability when building your data center fabric? Short answer: most probably not. Before diving into a lengthy explanation let's give our dear friend some homework.<!--more--></p>
<div class="separator"><a href="/2018/05/s1600-Bart+-+My+Network+Is+Not+FANG.gif" imageanchor="1"><img border="0" data-original-height="352" data-original-width="657" src="/2018/05/s1600-Bart+-+My+Network+Is+Not+FANG.gif"/></a></div>
<p><strong>TL&amp;DR summary</strong>: OSPF or IS-IS is most probably good enough for your data center… and if it isn’t, I sincerely hope you have an architecture/design team in place and don’t design your data center fabrics based on free information floating around the ‘net.</p>
<h4>What Are the Real Limits of IGPs?</h4><p>Now that our Anonymous friend is (hopefully) busy, let’s try to put the <em>IGPs don’t scale well </em>claim in perspective:</p>
<ul class="ListParagraph"><li>There are service providers having several thousand routers in a single IS-IS area. IS-IS traditionally scaled a bit better than OSPF because it was exposed to more abuse, but it shouldn’t be hard to push OSPF (should you prefer it) to several hundred devices in a single area. I’ve heard of networks having 300+ routers in an OSPF area in times when CPUs were an order of magnitude slower than they are today;</li>
<li>We tried to scope the problem with <a href="https://www.linkedin.com/in/dr-tony-przygienda-018501">Dr. Tony Przygienda</a> during our <a href="http://blog.ipspace.net/2018/03/data-center-routing-with-rift-on.html">Data Center Routing with RIFT</a> discussion, and while he pointed out that the real challenge OSPF and IS-IS are facing in leaf-and-spine fabric is not topology database size but the amount of redundant flooding, he put a comfortable limit of what OSPF or IS-IS could handle today at ~100 switches.</li>
</ul>
<p>RIFT and OpenFabric were designed to perform better in larger environments where you might hit the scaling limitations of traditional OSPF and IS-IS flooding, but we don’t know whether that’s true yet – as of mid-May 2018, you could get RIFT as experimental code running on Junos, and OpenFabric was still in very early stages the last time I chatted with Russ White</p>
<h4>What Can We Build with 100 Switches?</h4><p>Let’s assume for a moment that we’d like to stick with an IGP and are therefore limited to ~100 switches in a single data center fabric. Is that good enough?</p>
<p>Assuming you’re building your leaf-and-spine fabric with most common switch models, you’d have:</p>
<ul class="ListParagraph"><li>48-port switches (10/25GE) with four uplinks (40/100GE) at the leaf layer;</li>
<li>32-port higher-speed (40GE or 100GE) switches at the spine layer.</li>
</ul>
<p>The largest fabric you can build with these devices without going into breakout cables or superspines is a 32-leaf fabric with a total of 1536 ports.</p>
<p>If you use larger spine switches with 64 ports like Arista’s 7260CX3-64 or Cisco’s 9364C, you could get to 3072 ports with 68 switches (64 leaves, 4 spines).</p>
<h4>Quick Detour into Even Larger Fabrics</h4><p>Finally, if you need an even larger fabric, you could use modular switches at the spine layer, or build a superspine layer (we covered both options in <a href="https://my.ipspace.net/bin/list?id=Clos#PHY_TOPOLOGY">Physical Fabric Design</a> section of <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Architectures">Leaf-and-Spine Fabric Architectures</a> webinar). </p>
<p>A superspine architecture with 176 switches (using 32-port switches at the spine layer) gives you 6144 ports, so it might be cheaper to go with breakout cables in a leaf-and-spine fabric (144 switches). Both of them are at the high end of what you might consider comfortable, but still somewhat within reasonable bounds for a single-area IGP. </p>
<p class="info">The detailed designs are left as an exercise for the reader. You’ll find all the information you need to make them work in <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Architectures">Leaf-and-Spine Fabric Architectures</a> or <a href="http://www.ipspace.net/Designing_and_Building_Data_Center_Fabrics">Designing and Building Data Center Fabrics</a> online course.</p>
<h4>Back to Reality</h4><p>The largest data center fabric we could build without investing anything into understanding how things really work has around 70 switches and around 3000 edge-facing ports, and there’s no reason to feel limited by IGP scalability at this size. </p>
<p>Assuming you want redundant server connectivity that’s 1500 bare-metal servers. Assuming you didn’t buy them in a junkyard sale, you could easily put 30-50 reasonably-sized VMs on each one of them, for a total of around 50.000 (application) servers.</p>
<p>Is that good enough? It definitely is for<a href="http://blog.ipspace.net/2017/11/bgp-as-better-igp-when-and-where.html"> most enterprises</a> as well as for <a href="http://blog.ipspace.net/2014/07/how-big-will-your-cloud-be.html">smaller cloud providers</a>… and if your data center network is larger than that, please don’t listen to whatever is being said (overly generalized) on the Internet – you need a proper design done by someone who understands <em>why</em> he’s doing what he’s doing.</p>
<h4>Why Is Everyone So Focused on BGP Then?</h4><p>Dinesh Dutt outlined a few technical reasons why you might consider replacing OSPF or IS-IS with BGP in his part of the <a href="https://my.ipspace.net/bin/list?id=Clos#L3_SINGLE">Layer-3 Fabrics</a> section of leaf-and-spine fabrics webinar (you need at least <a href="http://www.ipspace.net/Subscription/Free">free ipSpace.net subscription</a> to watch those videos). </p>
<p>Here are a few more cynical ones:</p>
<ul class="ListParagraph"><li>We’re telling you BGP is good for you because Petr (and <a href="https://tools.ietf.org/html/rfc7938">RFC 7938</a>) said so. I’ve seen vendor SEs doing exactly that;</li>
<li>I always wanted to play with BGP and now I have an excuse to do so;</li>
<li>I want my network to be as cool as Microsoft’s (that’s where Petr started using BGP as better IGP).</li>
</ul>
<p>While you might decide to replace OSPF or IS-IS with BGP for any one of these reasons, IGP scalability limitations are most probably not on the very top of the list of potential challenges you might be facing in your data center fabric design.</p>
<h4>Master Data Center Fabric Designs</h4><ul class="ListParagraph"><li>Want to learn the basics of data center fabrics and figure out what individual vendors are doing? Check out the <a href="http://www.ipspace.net/Data_Center_Fabrics">Data Center Fabrics</a> webinar.</li>
<li>Want to learn how to design leaf-and-spine fabrics? Go for <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Architectures">Leaf-and-Spine Fabric Architectures</a> webinar.</li>
<li>Looking for a guided and mentored tour with plenty of peer- and instructor support? You probably need <a href="http://www.ipspace.net/Designing_and_Building_Data_Center_Fabrics">Designing and Building Data Center Fabrics online course</a>.</li>
<li>Want to know more about EVPN technology? Watch the <a href="http://www.ipspace.net/EVPN_Technical_Deep_Dive">EVPN Technical Deep Dive</a> webinar.</li>
</ul>

