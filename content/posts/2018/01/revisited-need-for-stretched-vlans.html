---
title: "Revisited: The Need for Stretched VLANs"
date: "2018-01-30T09:31:00.000+01:00"
tags: [ Design,switching,Data Center ]
---

<p>Regardless of how much I write about (the <a href="http://blog.ipspace.net/2015/02/before-talking-about-vmotion-across.html">ridiculousness of using</a>) stretched VLANs, I keep getting questions along the same lines. This time it&rsquo;s:</p>
<blockquote class="cite">What type of applications require L2 Extension and L3 extension?</blockquote>
<p>I don&rsquo;t think I&rsquo;ve seen anyone use <em>L3 extension </em>(after all, isn&rsquo;t that what Internet is all about), so let&rsquo;s focus on the first one.</p>
<p>Stretched VLANs (or L2 extensions) are used to solve a number of unrelated problems, because once a vendor sold you a hammer everything starts looking like a nail, and once you get used to replacing everything with nails, you want to use them in all possible environments, including public and hybrid clouds.<!--more--></p>
<p>Some of the challenges that I&rsquo;ve seen solved with stretched VLANs include:</p>
<p><strong>Subnet mobility</strong>. You must move a subnet from one site to another during disaster recovery process because the subnet (or IP addresses within the subnet) is <a href="http://blog.ipspace.net/2013/04/this-is-what-makes-networking-so-complex.html">hard-coded in application, configuration files, or firewall/load balancer rules</a>.</p>
<p>This one is easiest to solve: start using automation and make network infrastructure recovery part of your overall recovery process.</p>
<p>Alternatively, configure the same subnet on VLAN interfaces or firewall contexts that are shutdown during the regular operation, and enabled when needed.</p>
<p>For whatever reason, most everyone solves this one by stretching a VLAN between data centers (because VMware consultants told them to do so) and then <a href="http://blog.ipspace.net/2013/01/long-distance-vmotion-stretched-ha.html">experiencing a dual-data-center meltdown</a> before ever having the need to do a disaster recovery.</p>
<p><strong>IP address mobility</strong>. Similar to the one above, but caused by <a href="http://blog.ipspace.net/2013/02/hot-and-cold-vm-mobility.html">cold or hot VM move</a>, resulting in an IP subnet stretched across multiple sites.</p>
<p>You can implement this requirement without stretching a VLAN by using <a href="http://blog.ipspace.net/2015/04/rearchitecting-l3-only-networks.html">host-route-based IP forwarding</a> as implemented in <a href="http://blog.ipspace.net/2011/02/local-area-mobility-lam-true-story.html">Local Area Mobility</a> (requires at least Cisco IOS version 10.0), Cisco DFA, Cisco ACI, Cumulus Linux <a href="http://blog.ipspace.net/2015/08/layer-3-only-data-center-networks-with.html"><em>redistribute ARP</em></a>, or any decent EVPN implementation.</p>
<p>However, as you usually cannot announce host routes to WAN or public Internet, this design effectively creates multi-site summarization boundary. Anyone with production-grade multi-area OSPF experience probably knows how bad this could be; everyone else should figure this one out as a nice homework assignment.</p>
<p><strong>IP multicast</strong>. The application needs IP multicast because whatever (the only valid reasons I found: stock exchange feeds and video streaming) and it&rsquo;s easier to stretch a VLAN than to figure out how to spell PIM (btw, I agree with this conclusion).</p>
<p>Some vendors &ldquo;solve&rdquo; this problem by <a href="http://blog.ipspace.net/2017/11/lets-pretend-we-run-distributed-storage.html">requiring layer-2 connectivity between cluster members</a> (see also: <em>let&rsquo;s offload our support costs </em>below).</p>
<p><strong>Simplistic routing on </strong><strong>multihomed </strong><strong>hosts</strong>. I strongly suspect that most of <em>we need layer-2 for iSCSI </em>sentiment comes from inability to properly configure IP routing on multihomed iSCSI clients. VMware fixed this in recent vSphere versions, not sure what other vendors are doing.</p>
<p>This one is way harder than one would expect. I started writing a lengthy article on the topic and still haven&rsquo;t finished it because new worms keep turning up every time you turn around.</p>
<p><strong>Let&rsquo;s offload our support costs to customer&rsquo;s networking team</strong>. A typical trick used by software vendors is to write requirements that include <em>must have layer-2 connectivity between hosts in a cluster</em> for no reason, and reject support requests from environments that violate this ridiculous CYA approach.</p>
<p>Famous vendors in this category: </p>
<ul class="ListParagraph"><li>VMware requiring layer-2 connectivity between kernel interfaces to run a TCP-based vMotion session between them (they got to their senses in the meantime);</li>
<li>Oracle database clusters.</li>
</ul>
<p>Honorable mentions: </p>
<ul class="ListParagraph"><li>Nutanix distributed storage.</li>
<li>Stretched Cisco Hyperflex clusters</li>
</ul>
<p class='warn'>The response I got from the <a href="http://techfieldday.com/appearance/cisco-hyperflex-presents-at-tech-field-day-extra-at-cisco-live-europe/">Hyperflex presenter</a> @ <a href="http://techfieldday.com/event/cleur18/">Tech Field Day Extra CLEUR 2018</a>: "<em>yes, we're using pure IP transport without IP multicast, and no, we haven't tested and validated our solution for use over routed networks</em>." Now you know.</p>
<p>Anything else? Write a comment!</p>
<p><strong>Let&rsquo;s offload error detection to customer&rsquo;s networking team</strong>. It&rsquo;s easier to <a href="http://blog.ipspace.net/2015/11/ethernet-checksums-are-not-good-enough.html">rely on Ethernet checksums than to do application-level checksums</a>.</p>
<p>Bad news: with MAC-over-IP solutions like VXLAN-with-EVPN that almost every data center switching vendor is pushing you lose end-to-end Ethernet checksum even if the whole thing still looks like a thick yellow cable.</p>
<p><strong>Non-IP protocols</strong>. Hey, it&rsquo;s 2018. Let&rsquo;s move on.</p>
<p><strong>We want thick yellow cable</strong>. Solutions implementing stupidities that are skirting the edges of valid Ethernet behavior and work well only on a <a href="http://blog.ipspace.net/2015/02/lets-get-rid-of-thick-yellow-cable.html">thick yellow cable</a>. I&rsquo;m looking at you <a href="http://blog.ipspace.net/2012/02/microsoft-network-load-balancing-behind.html">Network Load Balancing</a>.</p>
<p><strong>Wrap-up</strong>: It makes me sad that after all these years we still have to deal with ignorance and decade-old stupidities that refuse to die. </p>
<p>Fortunately, the big cloud providers don&rsquo;t want to budge on this one because they&rsquo;re focused on making money from running services instead of supporting old crap based on which VP yells louder, so most of the things I mentioned above might die in the next few decades.</p>
<p>For an even more cynical view, join the <a href="http://www.ipspace.net/Building_Next-Generation_Data_Center">Building Next-Generation Data Centers</a> online course and listen to what <a href="http://nextgendc.ipspace.net/Public:5-High-Availability_Concerns#Guest_Speaker">Michele Chubirka has to say about infrastructure, security and DevOps</a>.</p>  

