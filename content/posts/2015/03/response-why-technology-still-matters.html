---
date: 2015-03-03T08:51:00.000+01:00
tags:
- SDN
- data center
- fabric
- OpenFlow
title: 'Response: Why Technology Still Matters'
url: /2015/03/response-why-technology-still-matters.html
---

<p>My good friend Tom Hollingsworth wrote a great blog post about <a href="http://networkingnerd.net/2015/02/17/hypermyopia-in-the-world-of-networking/">hypermyopia in the networking industry</a>. I agree with most everything he wrote (I have to – I’m always telling people to <a href="https://blog.ipspace.net/2013/01/long-distance-vmotion-stretched-ha.html">focus on business needs</a> and to <a href="http://blog.ipspace.net/2014/09/youve-been-doing-same-thing-for-last-20.html">change their mentality</a> before relying on shiny new gizmos), but I still think it’s crucial to consider the technology used in products we’re looking at.<!--more--></p>
<p>A fantastic technology that has <a href="https://blog.ipspace.net/2015/02/before-talking-about-vmotion-across.html">no chance of working in real life</a>, or that doesn’t solve an actual business need is a waste of developers’ energy and everyone’s time (but most vendors don’t get it). On the other hand, a product that <a href="http://blog.ipspace.net/2011/09/long-distance-irf-fabric-works-best-in.html">looks awesome on PowerPoint slides</a> (or in eye-candy demos) could have a broken architecture or might use wrong technology for the job – and it’s impossible to fix these errors without significant reengineering (if at all).</p>
<p>I honestly don’t care about the technology established vendors with long track records use in their mature products. We know how well (or badly) those products perform, and there’s plenty of documented hands-on experience to form an opinion. For example, I don’t care what technology VMware uses in vCenter or what protocols they run between vCenter and ESX hosts. I also (mostly) trust Cisco’s NX-OS verified maximums.</p>
<p>Startups and emerging products from established vendors are a different story. Startups try to dazzle us with shining past careers of their lead developers (message: trust us, these guys know what they’re doing), and established vendors play on the success of their past (sometimes unrelated) products. </p>
<p>However, I remain skeptical when a major server/printer vendor tells me how great they are going to be in data center networking (and I was right: it took them <a href="https://blog.ipspace.net/2014/05/data-center-protocols-in-hp-switches.html">years</a> to get their story straight), and I always try to understand the technology used by startups to be able to evaluate whether their product has a fighting chance when faced with reality. </p>
<p>Obviously it’s impossible to judge the implementation quality of a product without a large-scale trial, but some architectures are so broken that it doesn’t take much to figure out they won’t work without major changes… and if a <a href="https://ccie31104.wordpress.com/2015/02/18/pluribus-impressions-from-nfd9/">startup evades hard questions</a> it’s usually a red flag. For a counterpoint, watch the <a href="https://vimeo.com/119594286">precise</a> and <a href="https://vimeo.com/119594603">factual</a> answers we got from Carly Stoughton (including “<em>we don’t do that</em>” or “<em>it’s on the roadmap</em>”) in <a href="http://techfieldday.com/appearance/cisco-presents-at-networking-field-day-9/">Cisco ACI presentation</a> during <a href="http://techfieldday.com/event/nfd9/">Networking Field Day 9</a>.</p>
<p><strong>Case in point</strong>: large-scale OpenFlow-based data center fabrics. When I first heard about them (and read OpenFlow 1.0 standards), I said “this can never work”… and I was mostly right. </p>
<p>NEC got their product to work, but only after implementing numerous OpenFlow 1.0 extensions (they got rid of most of them when they introduced <a href="https://blog.ipspace.net/2013/10/flow-table-explosion-with-openflow-10.html">OpenFlow 1.3 support</a>), but the last time I checked they still weren’t running control-plane protocols (LACP, LLDP, STP) with hosts attached to the fabric. </p>
<p>It took Big Switch Networks several <a href="https://blog.ipspace.net/2012/08/openstackquantum-sdn-based-virtual.html">failed starts</a> and pivots before they figured out <a href="http://blog.ipspace.net/2015/02/big-cloud-fabric-scaling-openflow-fabric.html">large-scale OpenFlow fabrics can never work</a> with the current version of OpenFlow (want to know more: watch my <a href="http://www.ipspace.net/OpenFlow_Deep_Dive">OpenFlow Deep Dive</a> webinar). I can’t tell you how well their product works in practice, but at least its current architecture seems to be scalable. On the other hand, their changes to OpenFlow standard prevent them from using regular OpenFlow switches, which makes them equivalent to Juniper QFabric or Cisco ACI from <a href="http://blog.ipspace.net/2015/01/lock-in-is-inevitable-get-used-to-it.html">lock-in perspective</a>.</p>
<p>So, yes, I think the architectures and technologies still matter, but only after we figured out what problem needs to be solved and what the best (business and process) way of solving it is.</p>
<p><strong>Disclosure</strong>: Cisco Systems and some other vendors mentioned in this blog post were indirectly covering some of the costs of my attendance at the Network Field Day events. <a href="https://blog.ipspace.net/p/networking-tech-field-day-disclaimer.html">More…</a></p>

