---
date: 2015-01-13T09:38:00.000+01:00
tags:
- data center
- SAN
- fabric
- load balancing
title: Load Balancing Elephant Storage Flows
url: /2015/01/load-balancing-elephant-storage-flows.html
---

<p><a href="https://www.linkedin.com/in/olivierhault">Olivier Hault</a> sent me an interesting challenge:</p>
<blockquote class="cite">I cannot find any simple network-layer solution that would allow me to use total available bandwidth between a Hypervisor with multiple uplinks and a Network Attached Storage (NAS) box.</blockquote>
<p><strong>TL&amp;DR summary</strong>: you cannot find it because there’s none.<!--more--></p>
<h4>The Problem</h4><p>Network attached storage uses TCP-based protocols (iSCSI, NFS, CIFS/SMB) to communicate with the attached hosts. Trying to solve the problem at the network or TCP layer thus transforms the problem into: how can I load-balance packets from a single TCP session across multiple links?</p>
<div class="separator"><a href="/2015/01/s1600-S2N+-+iSCSI+over+2+links.jpg" imageanchor="1"><img border="0" src="/2015/01/s550-S2N+-+iSCSI+over+2+links.jpg"/></a></div>
<p>The simple answer is: you cannot, unless you’re willing to tolerate packet reordering, which will interfere with receive-side TCP offload (Receive Segment Coalescing – RSC) and thus <a href="http://blog.ipspace.net/2014/03/per-packet-load-balancing-interferes.html">impact TCP performance</a> (see also the <a href="http://blog.ipspace.net/2014/03/per-packet-load-balancing-interferes.html?showComment=1395217782705#c7257247553615947533">comments</a> in that blog post).</p>
<p class="note">Link aggregation group (LAG, aka EtherChannel or Port Channel) is not a solution. Most LAG implementations will not send packets from the same TCP session across multiple links to avoid packet reordering.</p>
<p><a href="http://blog.ipspace.net/2011/04/brocade-vcs-fabric-has-almost-perfect.html">Brocade solved the problem</a> by keeping track of packet arrival times (and delaying packets that would arrive out-of-order), but it only works if all links are connected to the same ASIC – you cannot use the same trick across multiple ASICs, let alone across multiple switches (for hosts connected to two ToR switches for redundancy).</p>
<p><strong>Conclusion:</strong> The problem MUST be solved above the network layer.</p>
<h4>The Solutions</h4><p>There are several solutions one can use to solve this challenge:</p>
<ul class="ListParagraph"><li><a href="http://en.wikipedia.org/wiki/Multipath_TCP"><strong>Multipath TCP</strong></a>: insert a shim inside the TCP layer that allows a single application-facing TCP session to be spread across multiple network-facing TCP sessions. All sessions could use the same IP endpoints, and rely on 5-tuple ECMP load balancing between the network and the server (MP-TCP could open new sessions if it figures out the existing sessions hash to the same link) </li>
</ul>
<div class="separator"><a href="/2015/01/s1600-S2N+-+iSCSI+over+MP-TCP.jpg" imageanchor="1"><img border="0" src="/2015/01/s550-S2N+-+iSCSI+over+MP-TCP.jpg"/></a></div>
<ul><li><a href="http://en.wikipedia.org/wiki/Multipath_I/O"><strong>Multipath I/O</strong></a>: use multiple application-level TCP sessions, each one of them running across one of the uplinks. In most implementations the server-to-network links cannot be in a LAG or ECMP group, and the server uses a dedicated IP address (tied to a specific uplink) for each storage session in the MPIO group. MPIO is available in many iSCSI implementations; SMB3 has a similar feature called <a href="http://blogs.technet.com/b/josebda/archive/2012/05/13/the-basics-of-smb-multichannel-a-feature-of-windows-server-2012-and-smb-3-0.aspx">SMB Multichannel</a> (HT: JP. Papillon).</li>
</ul>
<div class="separator"><a href="/2015/01/s1600-S2N+-+iSCSI+with+MPIO.jpg" imageanchor="1"><img border="0" src="/2015/01/s550-S2N+-+iSCSI+with+MPIO.jpg"/></a></div>
<ul><li><strong>Slice the elephant into smaller chunks</strong>. I got the impression that at least in some OpenStack Cinder implementations each VM uses a separate session with the iSCSI/NFS target, which nicely slices the fat one-session-per-hypervisor-host elephant into dozens of smaller bits. In the ideal case, the number of smaller sessions is large enough to make load balancing work well without tweaking the TCP port number selection or load balancing algorithms.</li>
</ul>
<h4>There’s more to come</h4><p>I’ll describe the storage challenges in a dedicated storage networking webinar sometime later this year.</p>

