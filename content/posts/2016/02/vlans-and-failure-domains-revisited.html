---
date: 2016-02-24T09:09:00.000+01:00
tags:
- bridging
- data center
- workshop
- fabric
- high availability
title: VLANs and Failure Domains Revisited
url: /2016/02/vlans-and-failure-domains-revisited.html
---

<p>My friend <a href="http://uebermeister.com/about.html">Christoph Jaggi</a>, the author of fantastic <a href="http://blog.ipspace.net/2015/06/just-out-metro-and-carrier-ethernet.html">Metro Ethernet and Carrier Ethernet Encryptors</a> documents, sent me this question when we were discussing the <a href="http://www.digs.ch/digs-workshop-data-center-fabrics-overview/">Data Center Fabrics Overview workshop</a> I’ll run in Zurich in a few weeks:</p>
<blockquote class="cite">When you are talking about large-scale VLAN-based fabrics I assume that you are pointing towards highly populated VLANs, such as VLANs containing 1000+ Ethernet addresses. Could you provide a tipping point between reasonably-sized VLANs and large-scale VLANs?</blockquote>
<p>It's not the number of hosts in the VLAN but the span of a bridging domain (VLAN or otherwise).<!--more--></p>
<h4>Before we start</h4><p>Please note that I'm looking at the problem purely from the data center perspective - transport VLANs offered by Metro Ethernet or VPLS service providers are a totally different story for several reasons:</p>
<ul class="ListParagraph"><li>There's a difference between <a href="http://blog.ipspace.net/2012/07/the-difference-between-metro-ethernet.html">providing transport service and being responsible for the whole infrastructure</a> (and all stupidities people do on top of it);</li>
<li>Sensible people don't bet their whole IT infrastructure on a single service provider (those that do eventually get the results they deserve). Failure in the transport network is thus not as critical as a data center failure;</li>
<li>Sensible people isolate their internal networks from transport network failures by using <em>routing functionality </em>between their LAN and WAN networks. Some data center architects happily extend a single (v)LAN network across a WAN network.</li>
</ul>
<h4>What could possibly go wrong?</h4><p>There are two failure scenarios I often see when <a href="http://www.ipspace.net/ExpertExpress">people come to me</a> after <a href="http://blog.ipspace.net/2016/01/the-sad-state-of-enterprise-networking.html">experiencing a data center meltdown</a>:</p>
<ul class="ListParagraph"><li><a href="http://blog.ipspace.net/2011/11/virtual-switches-need-bpdu-guard.html">bridging loop caused by a host (or VM)</a>;</li>
<li>bridging loop on the fabric edge - from something as stupid as technicians plugging TX fiber in RX port or <a href="http://blog.ipspace.net/2012/04/stp-loops-strike-again.html">connecting two ports</a> to see if the fiber is OK (sometimes <a href="http://blog.ipspace.net/2015/06/another-spectacular-layer-2-failure.html">coupled with device misconfiguration</a>) to software bugs in MLAG implementations.</li>
</ul>
<p>The first one is annoying, the second one is catastrophic, as the ToR switches easily do packet flooding at wire speed.</p>
<h4>Back to the failure domains</h4><p>In any case, anyone that's part of the same VLAN gets affected, and if someone (in their infinite wisdom) configured <a href="http://blog.ipspace.net/2011/12/vm-aware-networking-improves-iaas-cloud.html">all VLANs on all server-facing ports</a> because that's easier than actually talking with the server/virtualization team or <a href="http://blog.ipspace.net/2013/03/what-did-you-do-to-get-rid-of-manual.html">deploying a VLAN provisioning solution</a>, every server gets impacted.</p>
<p>Furthermore, every link that the affected VLAN crosses has to carry the unnecessary traffic. Not a big deal if you have a 10Gbps bridging loop at the network edge and 40 Gbps fabric links, but a major disaster if your bridging domain includes 1Gbps or 10Gbps links between data centers.</p>
<h4>It’s not a numbers game</h4><p>Christoph continued his question with two example:</p>
<blockquote class="cite">1 VLAN = 1 failure domain<br/>100 VLANs = 100 separate failure domains</blockquote>
<p>Not necessarily. The 100 VLANs touch all the core links, so there’s at least partial overlap between them (a bridging loop in a VLAN that results in saturation of DCI link will kill all other VLANs traversing that link), and if the networking team configured every VLAN on every server-facing port to make their lives easier, the whole data center fabric becomes a single failure domain.</p>
<blockquote class="cite">1 VLAN - 100 MAC-adresses = I am OK<br/>1 VLAN - 1000 MAC addresses = I am close to the limit<br/>1 VLAN = 2000 MAC adresses = I am looking for trouble<br/>1 VLAN = 3000+ MAC adresses = I will get into trouble</blockquote>
<p>That’s the traditional view of the problem: bridging doesn’t scale because hosts are chatty and more hosts in a VLAN result in more flooding overhead.</p>
<p>In reality, you will get into trouble with many IP nodes (hosts or routers) in the same VLAN (that’s why some large layer-2 fabrics use <a href="https://ams-ix.net/technical/specifications-descriptions/controlling-arp-traffic-on-ams-ix-platform">ARP sponges</a>), but the most common causes of network meltdowns that I see are the bridging loops.</p>

