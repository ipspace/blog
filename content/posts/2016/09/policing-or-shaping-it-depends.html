---
url: /2016/09/policing-or-shaping-it-depends/
title: "Policing or Shaping? It Depends"
date: "2016-09-28T09:04:00.000+02:00"
tags: [ TCP,QoS,WAN ]
---

<p>One of my readers watched my <a href="http://www.ipspace.net/TCP,_HTTP_and_SPDY">TCP, HTTP and SPDY webinar</a> and disagreed with my assertion that shaping sometimes works better than policing.</p>
<p class="info">TL&amp;DR summary: <em>policing </em>= dropping excess packets, <em>shaping = </em>delaying excess packets.</p>
<p>Here’s the picture he sent me (<a href="http://my.ipspace.net/bin/get/SPDY/2%20-%20TCP%20and%20HTTP.mp4">watch the video</a> to get the context and <a href="http://packetlife.net/blog/2008/jul/30/policing-versus-shaping/">read this article</a> to get the background details):<!--more--></p>
<div class="separator"><a href="/2016/09/s1600-Policing_Shaping.png" imageanchor="1"><img border="0" src="/2016/09/s550-Policing_Shaping.png"/></a></div>
<p>He observed that:</p>
<blockquote class="cite">While to voice-over suggests that shaping is preferred, the total transmission rate of these two examples look about the same.</blockquote>
<p>You might also notice how unequal the throughput is for individual TCP sessions. I doubt you’d want to be the happy owner of the green session.</p>
<blockquote class="cite">Shaping is more aesthetically pleasing, but, one could argue that policing would avoid buffer bloat and achieve the same total throughput.</blockquote>
<p>Jeremy provided not just throughput graphs but also goodput (<strong>iperf</strong> traffic) results in his blog post. <strong>iperf </strong>managed to push 840 kbps over a link <em>policed</em> at 1 Mbps and 962 kbps over a link <em>shaped</em> at 1 Mbps. Obviously shaping results in better goodput than policing, and equally obviously you have to configure reasonable limits for the shaping queues. For more details, please read <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45411.pdf"><em>Internet-Wide Analysis of Traffic Policing</em></a>, an excellent Google Research paper.</p>
<p>Also, per-session WFQ within the shaping queue results in better TCP performance than FIFO+tail drop shaping queue.</p>
<p class="note">Please don’t ask me what <em>reasonable </em>is. I’d guess that anything beyond bandwidth-delay product makes no sense, and I’d immediately agree with you that the current way of configuring output queues on most routers desperately needs improvements… or maybe it’s time to start asking for <a href="https://www.bufferbloat.net/projects/codel/wiki/">CoDel</a>.</p>
<blockquote class="cite">I would think, in this example, had Stretch run longer, he would eventually fill up his output queues anyway and you would start to see tcp rate changes.</blockquote>
<p>Stretch connected two Ethernet segments with a router. RTT was below a millisecond, so one would hope that the queues would be pretty stable after 45 seconds of continuous transfer.</p>
<blockquote class="cite">TCP should always be trying to increase its rate to see if it can run faster, but a big buffer slows this down by increasing RTT.</blockquote>
<p>There are at least two signals TCP can use to figure out it should slow down: packet drops (example: TCP Reno) or increasing RTT (example: <a href="https://en.wikipedia.org/wiki/TCP_Vegas">TCP Vegas</a>). Some implementations also listen to ECN bits. Response to increased RTT is usually less drastic than response to packet drop (Disagree? Please write a comment!)</p>
<p>Finally, there are the <a href="https://en.wikipedia.org/wiki/Bufferbloat">bufferbloat</a> disasters. While a lot of progress has been made in understanding what causes them and addressing the problem, it’s hard to solve the bufferbloat within the network if the host TCP stack uses huge TCP window sizes and sends way too large bursts. We’ll face bufferbloat challenges till the host TCP stacks are fixed.</p>
<p class="more">FlowQueueCodel seems to be the network-side answer to bufferbloat, but it seems to require per-flow queues (no surprise there, so does WFQ), which are usually expensive to implement in hardware. OTOH, some software implementations <a href="http://blog.cerowrt.org/post/fq_codel_on_ath10k/">work great</a>.</p>
<p>The moral of this story is: there’s no right answer that works for everyone, and just because some implementations are badly broken (storing seconds worth of traffic in an output queue makes absolutely no sense, for example) it doesn’t mean that the idea of delaying packets instead of dropping them is a bad one.</p>
<h4>More information</h4><ul class="ListParagraph"><li><a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45411.pdf">Internet-Wide Analysis of Traffic Policing</a> is an absolute must-read</li>
<li><a href="https://tools.ietf.org/html/draft-ietf-aqm-codel-04">CoDel IETF draft</a> is another one (and reasonably easy to understand)</li>
<li>You should also explore the <a href="https://www.bufferbloat.net/projects/">Bufferbloat site</a></li>
</ul>

