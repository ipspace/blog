---
date: 2020-02-18T08:05:00.000+01:00
tags:
- VXLAN
- overlay networks
- NSX
title: The Never-Ending "My Overlay Is Better Than Yours" Saga
url: /2020/02/the-never-ending-my-overlay-is-better.html
---

<p>I published a <a href="https://blog.ipspace.net/2020/02/do-we-need-complex-data-center-switches.html">blog post describing how complex the underlay supporting VMware NSX still has to be</a> (because someone keeps pretending a network is <a href="https://blog.ipspace.net/2015/02/lets-get-rid-of-thick-yellow-cable.html">just a thick yellow cable</a>), and the <a href="https://twitter.com/ioshints/status/1227504195701362691">tweet announcing it</a> admittedly looked like a clickbait.</p>
<blockquote><p>[Blog] Do We Need Complex Data Center Switches for VMware NSX Underlay</p>
</blockquote>
<p><a href="https://twitter.com/martin_casado">Martin Casado</a> quickly replied <strong>NO</strong> (probably before reading the whole article), starting a whole barrage of overlay-focused neteng-versus-devs fun.<!--more--></p>
<div class="note" data-markdown="1">While I <a href="https://blog.ipspace.net/2011/10/what-is-nicira-really-up-to.html">loved the concept Nicira worked on</a>, the <a href="https://blog.ipspace.net/2014/11/open-vswitch-performance-revisited.html">execution wasn’t exactly stellar</a>, and once it got merged with traditional VMware approach to networking we got what I <a href="https://blog.ipspace.net/2020/02/do-we-need-complex-data-center-switches.html">described</a>. The end result makes me infinitely sad every time I think about its potentials… but then that water has long reached the ocean. </div>
<p>The best response to Martin’s claim was <a href="https://twitter.com/matjovanovic/status/1227840092108140546">made by Mat Jovanovic</a>:</p>
<blockquote><p>Depends… Are we looking at a PPT, or a “I’ve tried it on a commodity underlay” version of the answer? Something tells me it’s quite different…</p>
</blockquote>
<p>In the meantime, the debate veered into “my overlay is better than your overlay”, <a href="https://twitter.com/martin_casado/status/1227793721019600897">starting with</a> Martin's claim that:</p>
<blockquote><p>Good news for you – there are many fast growing overlay solutions that are adopted by apps and security teams and bypass the networking teams altogether.</p>
</blockquote>
<p>Martin furthermore <a href="https://twitter.com/martin_casado/status/1227794153179795456">pointed to Nebula</a> as one of his favorites. I did a quick look at their <a href="https://github.com/slackhq/nebula">GitHub repo</a>, and it looks like they did things the right way and <a href="https://blog.ipspace.net/2009/08/what-went-wrong-tcpip-lacks-session.html">built a badly-needed session layer</a>.</p>
<p>However, being sick-and-tired of everyone claiming how great it is to build overlays on top of overlays (like we didn’t learn anything in the decades <a href="https://blog.ipspace.net/2011/03/mplsvpn-over-gre-over-ipsec-does-it.html">building GRE and IPsec tunnels</a>), I decided to <a href="https://twitter.com/ioshints/status/1227837256779681797">troll a bit more</a>:</p>
<blockquote><p>We had a fast-growing overlay solution in 1970s. It was called TCP. I’ve heard it might still be used. Why do people insist of heaping layers upon layers instead of writing decent code?</p>
</blockquote>
<p>Martin's response <a href="https://twitter.com/martin_casado/status/1227842655725350912">was almost as-expected</a>:</p>
<blockquote><p>App developers : “I’ve created this amazing overlay solution that solves a bunch of our problems”</p>
<p>Networking : “TCP has been around since the 70’s, write better code”</p>
<p>… this is why you’re not being invited to the party ;)</p>
</blockquote>
<p>Someone must have had some traumatic experiences... Anyhow, as you probably know I’m well-aware of the <a href="https://blog.ipspace.net/2010/07/extremely-off-topic-another-recognition.html">popularity of pointing out the state of Emperor’s wardrobe</a> (or lack thereof), and I’m way too old for FOMO, so I don’t care what parties I get invited to.</p>
<p>However, what makes me truly sad is watching highly intelligent people ignorant of environmental limitations (see also: <a href="https://blog.ipspace.net/2020/01/video-fallacies-of-distributed-computing.html">fallacies of distributed computing</a> and RFC 1925 rule 4) reinventing the wheels, and ending with what we already had (in a different disguise, see also RFC 1925 rule 11) after spending years figuring it out and repeating the mistakes we made in the past.</p>
<p>For example:</p>
<ul><li><em>We won’t use DNS</em> (for whatever made-up reason), because we believe in IP addresses. Years later: We don’t care about stinking IP addresses anymore, we have Consul (hint: ever heard of SRV records?)</li>
<li><em>We tied everything to IP addresses</em>, so you <a href="https://blog.ipspace.net/2015/02/before-talking-about-vmotion-across.html">better move them across the globe</a> and <a href="https://blog.ipspace.net/2019/11/stretched-layer-2-subnets-in-azure.html">into public clouds</a>, and <a href="https://blog.ipspace.net/2019/12/you-dont-need-ip-renumbering-for.html">you can’t change them when doing disaster recovery</a>. Years later: containers are cool, and we use Consul anyway, so it’s perfectly fine to hide a dozen of thingies behind the same IP address.</li>
<li><em>We’ll implement our own overlay</em> because your overlay sucks. Years later: OMG, <a href="https://blog.ipspace.net/2015/04/omg-vxlan-encapsulation-has-no-security.html">VXLAN has no security</a>, as <a href="https://blog.ipspace.net/2018/11/omg-vxlan-is-still-insecure.html">security researches tend to find out every other year or so</a>.</li>
<li><a href="https://blog.ipspace.net/2014/05/does-centralized-control-plane-make.html">Centralized control plane</a> is the way to go. Years later: Ouch, scalability and latency suck. Maybe we should focus on automation... or intent... or policies... or whatever.</li>
</ul>
<p>The networking engineers should know better, but even they can’t resist the lure of reinventing broken wheels, for example overlays with cache-based forwarding like LISP. No surprise, such solutions quickly encounter endpoint liveliness problem (and a <a href="https://tools.ietf.org/html/draft-meyer-loc-id-implications-01">few others</a>).</p>
<p><strong>Notes:</strong></p>
<ul><li>I’m guessing LISP is not yet widespread enough to encounter severe cache trashing behavior that still triggers PTSD in anyone remotely involved in the days when Fast Switching crashed the Internet. That rerun might be fun to watch…</li>
<li>Of course I probably messed up at least some of these examples, so please feel to correct me in the comments.</li>
</ul>
<p>Now here’s a crazy idea: what if we’d start communicating with people who understand how stuff works, learn from them, and implement stuff in an optimal way. IT seems to be one of the few areas where we allow people to build sandcastles and ignore the tides, and then <a href="https://blog.ipspace.net/2013/04/this-is-what-makes-networking-so-complex.html">blame someone else when the water inevitable arrives</a>.</p>
<p>To be continued…</p>

