---
date: 2014-01-20T07:13:00.000+01:00
tags:
- link aggregation
- data center
- virtualization
title: vSphere Does Not Need LAG Bandaids – the Network Does
url: /2014/01/vsphere-does-not-need-lag-bandaids.html
---

<p>Chris Wahl <a href="http://wahlnetwork.com/2014/01/13/vsphere-need-lag-bandaids/"><span style="color: 0025E5;">claimed in one of his recent blog posts</span></a> that <em>vSphere doesn't need LAG band-aids</em>. He's absolutely right - <a href="http://blog.ipspace.net/2010/11/vmware-virtual-switch-no-need-for-stp.html"><span style="color: 0025E5;">vSphere's loop prevention logic alleviates the need for STP-blocked links</span></a>, allowing you to use full server uplink bandwidth without the complexity of link aggregation. Now let’s consider the networking perspective.<!--more--></p>
<p>The conclusions Chris reached are perfectly valid in a classic data center or VDI environment with majority of the VM-generated traffic leaving the data center; the situation is drastically different in data centers with predominantly east-west traffic (be it inter-server or IP-based storage traffic).</p>
<p>Let’s start with a simple scenario: </p>
<ul class="ListParagraph"><li>Data center is small enough to have only two switches (for a total of 4 Tbps or more of throughput – that should be enough for most use cases).</li>
<li>Two vSphere servers are connected to the two data center switches in a fully redundant setup (each server has one uplink to each ToR switch).</li>
<li>Load-Based Teaming (LBT) is used within vSphere instead of IP-based hash (vSphere terminology for a Link Aggregation Group / LAG).</li>
</ul>
<p>The two ToR switches are not aware of the exact VM placement, <a href="http://blog.ipspace.net/2011/01/vswitch-in-multi-chassis-link.html">resulting in traffic flowing across inter-switch link even when it could be exchanged locally</a> (yeah, I was writing about this issue almost exactly three years ago). </p>
<div class="separator"><img src="/2014/01/s320-vSwitch_MLAG_Phy.png"/><br/>Physical connectivity<br/><br/><img src="/2014/01/s320-vSwitch_MLAG_Log.png"/><br/>VM MAC reachability as seen by the switches<br/><br/><img src="/2014/01/s320-vSwitch_MLAG_Traffic.png"/><br/>Traffic flow between VMs on adjacent hypervisor hosts </div>
<h4>Can we fix it?</h4><p>You can fix this problem by making most endpoints equidistant. You could introduce a second layer of switches (resulting in a full-blown leaf-and-spine fabric) or you could connect the servers to a layer of fabric extenders, which would also ensure the traffic between any two endpoints gets equal treatment.</p>
<div class="separator"><a href="/2014/01/s1600-FEX_LAG.png" imageanchor="1"><img border="0" src="/2014/01/s400-FEX_LAG.png"/></a></div>
<p class="note">Traffic between VMs appearing near to each other in a leaf-and-spine fabric gets better treatment than any other traffic (no leaf-to-spine oversubscription); in the FEX scenario all traffic gets identical treatment as <a href="http://www.cisco.com/en/US/docs/switches/datacenter/nexus2000/sw/configuration/guide/rel_6_2/b_Configuring_the_Cisco_Nexus_2000_Series_Fabric_Extender_rel_6_2_chapter_01.html">FEX still doesn’t support local switching</a>.</p>
<p>The leaf-and-spine (or FEX) solution obviously costs way more than a simple two-switch solution, so you just might consider using link aggregation and LACP with vSphere.</p>
<h4>But LBT works so much better than IP hash mechanisms</h4><p>Sure it does (and Chris provided some very good arguments for that claim in his blog post), but there’s nothing in the 802.1ax standard dictating the traffic distribution mechanism on a LAG. VMware could have decided to use LBT with a LAG, but they didn’t (because deep inside the virtual switch they tie the concept of a link aggregation group to the IP hash load balancing mechanism). Don’t blame standards and concepts for suboptimal implementations ;)</p>
<h4>But aren’t static port channels unreliable?</h4><p>Of course they are; I wouldn’t touch them with a 10-foot pole. You should always use LACP to form a LAG, but <a href="http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=2034277">VMware supports LACP only in the distributed switch</a>, which <a href="http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=1010555">requires Enterprise Plus license</a>. Yet again, don’t blame the standards or network design requirements, blame a vendor that wants to charge extra for baseline layer-2 functionality. </p>
<h4>Is there another way out of this morass?</h4><p>Buying extra switches (or fabric extenders) is too expensive. Buying Enterprise Plus license for every vSphere host just to get LACP support is clearly out of question. Is there something else we could do? Of course – you can make sure the inter-switch link gets enough bandwidth.</p>
<p>Typical high-end ToR switches (from almost any vendor) have 48 10GE ports and four 40GE ports. Using the 40GE ports for inter-switch links results in worst-case 3:1 oversubscription (48 10GE ports on the left-hand switch communicate exclusively with the 48 10GE ports on the right-hand switch over an equivalent of 16 10GE ports). Problem solved (until you have to add more servers).</p>
