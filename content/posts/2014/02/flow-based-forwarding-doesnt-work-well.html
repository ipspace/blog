---
url: /2014/02/flow-based-forwarding-doesnt-work-well.html
title: "Flow-based Forwarding Doesn’t Work Well in Virtual Switches"
date: "2014-02-17T07:11:00.000+01:00"
tags: [ OpenFlow,virtualization ]
---

<p>I hope it’s obvious to everyone by now that <a href="https://blog.ipspace.net/2014/01/controller-implementation-choices.html">flow-based forwarding doesn’t work well in existing hardware</a>. Switches designed for large number of flow-like forwarding entries (NEC ProgrammableFlow switches, Enterasys data center switches and a few others) might be an exception, but even they <a href="http://blog.ipspace.net/2012/01/fib-update-challenges-in-openflow.html">can’t cope with the tremendous flow update rate</a> required by <a href="http://networkstatic.net/openflow-proactive-vs-reactive-flows/">reactive flow setup</a> ideas.</p>
<p>One would expect virtual switches to fare better. Unfortunately that doesn’t seem to be the case.<!--more--></p>
<h4>A few definitions first</h4><p>Flow-based forwarding is sometimes defined as forwarding of individual transport-layer sessions (sometimes also called <em>microflows</em>). <a href="https://blog.ipspace.net/2009/06/what-went-wrong-atm.html">Numerous failed technologies</a> are a pretty good proof that this approach doesn’t scale.</p>
<p>Other people define flow-based forwarding as <a href="http://www.jedelman.com/1/post/2012/05/flow-based-protocols-and-routeflow-the-new-ways-of-forwarding.html">anything that is not destination-address-only forwarding</a>. I don’t really understand how this definition differs from MPLS <em>Forwarding Equivalence Class </em>(FEC) and why we need a new confusing term.</p>
<h4>Microflow forwarding in Open vSwitch</h4><p>Initial versions of Open vSwitch were a prime example of <a href="https://blog.ipspace.net/2013/04/open-vswitch-under-hood.html">ideal microflow-based forwarding architecture</a>: in-kernel forwarding module performed microflow forwarding and punted all unknown packets to the user-mode daemon. </p>
<p>The user-mode daemon would then perform packet lookup (using OpenFlow forwarding entries or any other forwarding algorithm) and install a microflow entry for the newly discovered flow in the kernel module. </p>
<p class="info">Third parties (example: <a href="https://blog.ipspace.net/2012/08/midokuras-midonet-layer-2-4-virtual.html">Midokura Midonet</a>) use Open vSwitch kernel module in combination with their own user-mode agent to implement non-OpenFlow forwarding architectures.</p>
<p>If you’re old enough to remember the Catalyst 5000, you’re probably getting unpleasant flashbacks of <a href="http://www.net130.com/book/cisco/typical/Configuring%20NetFlow%20Switching.pdf">Netflow switching</a> … but the problems we experienced with that solution must have been caused by poor hardware and underperforming CPU, right? Well, it turns out virtual switches aren't much better.</p>
<p>Digging deep into the bowels of Open vSwitch reveals an interesting behavior: flow eviction. Once the kernel module hits the maximum number of microflows, it starts throwing out old flows. Makes perfect sense – after all, that’s how every caching system works – until you realize the default limit is 2500 microflows, which is barely good enough for a single web server and definitely orders of magnitude too low for a hypervisor hosting 50 or 100 virtual machines.</p>
<h4>Why, oh why?</h4><p>The very small microflow cache size doesn’t make any obvious sense. After all, <a href="http://en.wikipedia.org/wiki/C10k_problem">web servers easily handle 10.000 sessions</a> and some <a href="https://blog.ipspace.net/2013/10/estimating-number-of-tcp-sessions-per.html">Linux-based load balancers handle an order of magnitude more sessions</a> <em>per server</em>. While you can increase the default OVS flow cache size, one’s bound to wonder what the reason for the dismally low default value is.</p>
<p>I wasn’t able to figure out what the underlying root cause is, but I’m suspecting it has to do with <a href="http://openvswitch.org/pipermail/dev/2011-July/010107.html">per-flow accounting</a> – flow counters have to be transferred from the kernel module to the user-mode daemon periodically. Copying hundreds of thousands of flow counters over a user-to-kernel socket at short intervals might result in “somewhat” noticeable CPU utilization.</p>
<p>Did I get it all wrong? Please correct me in the comments ;)</p>
<h4>How can you fix it?</h4><p>Isn’t it obvious? You drop the whole notion of microflow-based forwarding and do things the traditional way. OVS moved in this direction with <a href="http://openvswitch.org/pipermail/announce/2013-August/000054.html">release 1.11 which implemented megaflows</a> (coarser OpenFlow-like forwarding entries) in kernel module, and moved flow eviction from kernel to user-mode OpenFlow agent (which makes perfect sense as kernel forwarding entries almost exactly match user-mode OpenFlow entries).</p>
<p>Not surprisingly, no other virtual switch uses microflow-based forwarding. VMware vSwitch, Cisco’s Nexus 1000V and IBM’s 5000V make forwarding decisions based on destination MAC addresses, <a href="https://blog.ipspace.net/2013/12/hyper-v-network-virtualization-packet.html">Hyper-V</a> and Contrail based on destination IP addresses, and even VMware NSX for vSphere uses <a href="http://blog.ipspace.net/2013/11/layer-2-and-layer-3-switching-in-vmware.html">distributed vSwitch and in-kernel layer-3 forwarding module</a>.</p>
<h4>More information</h4><p>Check out <a href="http://www.ipspace.net/SDN">SDN resources page @ ipSpace.net</a>.</p>

