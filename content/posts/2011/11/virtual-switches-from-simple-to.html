---
url: /2011/11/virtual-switches-from-simple-to.html
title: "Virtual Switches – from Simple to Scalable"
date: "2011-11-29T07:04:00.000+01:00"
tags: [ cloud,virtualization ]
---

<p>Dan sent me an interesting comment after watching my <a href="http://www.ipspace.net/DC30">Data Center 3.0</a> webinar:</p>
<blockquote class="cite">I have a different view regarding VMware vSwitch. For me its the best thing happened in my network in years. The vSwitch is so simple, and its so hard to break something in it, that I let the server team to do what ever they want (with one small rule, only one vNIC per guest). I never have to configure a server port again :).</blockquote>
<p>As always, the right answer is “it depends” – what kind of vSwitch you need depends primarily on your requirements.</p><!--more-->
<p class="note">I’ll try to cover the whole range of virtual networking solutions (from very simple ones to pretty scalable solutions) in a series of blog posts, but before going there, let’s agree on the variety of requirements that we might encounter.</p>
<p>We use virtual switches in two fundamentally different environments today: virtualized data centers on one end of the spectrum, and private and public clouds at the other end (and you’re probably somewhere between these two extremes).</p>
<h4>Virtualized Data Centers</h4><p>You’d expect to see only a few security zones (and logical segments) in a typical small data center; you might even see different applications sharing the same security zone. </p>
<p>The number of physical servers is also reasonably low (in tens, maybe low hundreds, but definitely not thousands) as is the number of virtual machines. The workload is more or less stable, and the virtual machines are moved around primarily for workload balancing / fault tolerance / maintenance / host upgrade reasons.</p>
<h4>Public and Private Clouds</h4><p>Cloud environment is a completely different beast. Workload is dynamic and unpredictable (after all, the whole idea of cloudifying the server infrastructure revolves around the ability to be able to start, stop, move, grow and shrink the workloads instantaneously), there are numerous tenants, and each tenant wants to have its own virtual networks, ideally totally isolated from other tenants.</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/11/s1600-Isolated.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="215" src="/2011/11/s320-Isolated.jpg" width="320"/></a><br/><span style="font-size: 90%">You know some tenants might be obsessed with isolation (source: <a href="http://www.flickr.com/photos/usnationalarchives/3814167697/">flickr.com</a>)</span></div>
<p>The unpredictable workload places extra strains on the networking infrastructure due to large-scale virtual networks needed to support it.</p>
<p>You could limit the scope of the virtual subnets in a more static virtualized data center; after all, it doesn’t make much sense to have the same virtual subnet spanning more than one HA cluster (or at most a few of them). </p>
<p>In a cloud environment, you have to be able to spin up a VM whenever a user requests it … and you usually start the VM within the physical server that happens to have enough compute (CPU+RAM) resources. That physical server can be sitting anywhere in the data center, and the tenant’s logical network has to be able to extend to it; you simply cannot afford to be limited by the geography of the physical network.</p>
<h4>Hybrid environments</h4><p>These data centers can offer you the most fun (or headache) there is – a combination of traditional hosting (with physical servers owned by the tenants) and IaaS cloud (running on hypervisor-powered infrastructure) presents some very unique requirements – just ask <a href="http://www.network-janitor.net/">Kurt (@networkjanitor) Bales</a> about his <a href="http://www.network-janitor.net/2011/08/wholesale-virtualisation-and-selective-qinq/">DC needs</a>.</p>
<h4>Virtual Machine Mobility</h4><p>One of the major (network-related) headaches we’re experiencing in the virtualized data centers is the requirement for VM mobility. You cannot change the IP address of a running VM as you move it between hypervisor hosts due to <a href="/2009/08/what-went-wrong-tcpip-lacks-session.html">broken TCP stack</a> (or you’d lose all data sessions). The <a href="/2010/09/vmotion-elephant-in-data-center-room.html">common way to implement VM mobility</a> without changes to the guest operating system is thus L2 connectivity between the source and destination hypervisor host.</p>
<div class="separator" style="clear: both; text-align: center"><a href="/2010/09/vmotion-elephant-in-data-center-room.html"><img src="/2011/11/s320-vmotion_3.png"/></a></div>
<p>In a virtualized enterprise data center you’d commonly experience a lot of live VM migration; the workload optimizers (like VMware’s DRS) constantly shift VMs around high availability clusters to optimize the workload on all hypervisor hosts (or even shut down some of the hosts if the overall load drops below a certain limit). These migration events are usually geographically limited – Vmware HA cluster can have at most 32 hosts and while it’s prudent to spread them across two racks or rows (for HA reasons), that’s the maximum range that makes sense.</p>
<p>VM migration events are rare in public clouds (at least those that charge by usage). While the cloud operator might care about the server utilization, it’s simpler to allocate resources statically when the VMs are started, implement resource limits to ensure VMs can’t consume more than what the users paid for, and let the users perform their own workload balancing (or not).</p>
<h4>Anything else?</h4><p>If I’ve missed something important, if you disagree with my views, or would like to add your own, don’t hesitate – the comment box is just below the post.</p>
