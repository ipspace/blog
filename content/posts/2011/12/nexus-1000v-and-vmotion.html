---
url: /2011/12/nexus-1000v-and-vmotion.html
title: "Nexus 1000V and vMotion"
date: "2011-12-07T06:31:00.000+01:00"
tags: [ Workshop,cloud,virtualization ]
---

<p>I thought <a href="https://blog.ipspace.net/2011/06/what-exactly-is-nexus-1000v.html">Nexus 1000V</a> is like Aspirin compared to VMware’s vSwitch, providing tons of additional functionality (including <a href="https://blog.ipspace.net/2011/09/nexus-1000v-lacp-offload-and-dangers-of.html">LACP</a> and <a href="https://blog.ipspace.net/2011/11/virtual-switches-need-bpdu-guard.html">BPDU filter</a>) and familiar NX-OS CLI. It turns out I was right in more ways than I imagined; Nexus 1000V solves a lot of headaches, but can also cause heartburn due to a particular combination of its <a href="https://blog.ipspace.net/2011/08/vlans-used-by-nexus-1000v.html">distributed architecture</a> and reliance on vDS object model in vCenter.<!--more--></p>
<p><strong>Fact#1</strong>: Nexus 1000V has a single control plane (Virtual Supervisor Module – VSM) controlling a large number of distributed data planes (Virtual Ethernet Module – VEM). The maximum number of VEMs controlled by a VSM is 64, which proves yet again that centralized control planes aren’t infinitely scalable (OpenFlow aficionados will obviously disagree).</p>
<p class="note">VMware’s vNetwork Distributed Switch (vDS) can span 350 hosts because it doesn’t have a control plane; vDS is just a management-plane templating tool.</p>
<div class="separator"><a href="http://www.flickr.com/photos/cybershotking/329184504/" title="spider web by cybershotking, on Flickr"><img alt="spider web" src="http://farm1.staticflickr.com/133/329184504_b58ce169f4_m.jpg" width="320"/></a><span>VSM must be just around the corner</span></div>
<p><strong>Fact#2</strong>: Nexus 1000V is represented as a vDS in vCenter; that’s the only way to represent a distributed switch in vCenter.</p>
<p><strong>Fact#3</strong>: You cannot move a running virtual machine between hosts that are not part of the same vDS.</p>
<p>You can perform manual vMotion from any vSphere host to any other vSphere host if the VM you’re moving is connected to standard vSwitches (assuming port group names and configuration parameters match between the two hosts); if the VM is connected to a vDS, then the two hosts must belong to the same vDS. </p>
<p class="note">It seems that this is more a vCenter limitation than a Nexus 1000V limitation. VMware probably doesn't care because its vDS spans 350 hosts … and it wouldn't be too hard to stretch it to a larger number of hosts if someone would really want to have that.</p>
<h4>Conclusions</h4><p>You can easily use Nexus 1000V in combination with DRS/HA clusters. These clusters can have at most 32 hosts (64 VEMs is thus more than enough), and the automatic vMotion and load balancing will work as expected. </p>
<p>Combining two DRS/HA clusters in a Nexus 1000V vDS is probably more than enough for typical enterprise data center (you can still vMotion VMs between the clusters if you have to do large-scale maintenance or software upgrade).</p>
<p>IaaS cloud providers might have a different view. Limiting VM mobility to two racks (yes, you can squeeze 32 UCS blades in a single rack) just doesn’t sound right.</p>
<h4>Webinars? Sure!</h4><p>Virtual networking scalability is one of the main topics of my <a href="http://www.ipspace.net/Cloud_Computing_Networking:_Under_the_Hood">Cloud Computing Networking – Under the Hood</a> webinar (<a href="http://cloudnetworking.eventbrite.com/">register for the live session</a>).</p>

