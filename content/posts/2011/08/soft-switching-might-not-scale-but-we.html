---
date: 2011-08-22T04:00:00.000+02:00
tags:
- switching
- data center
- workshop
- virtualization
title: Soft switching might not scale, but we need it
url: /2011/08/soft-switching-might-not-scale-but-we.html
---

<p>Following a <a href="http://networkheresy.wordpress.com/category/open-vswitch/">series of soft switching articles</a> written by Nicira engineers (hint: they are using a similar approach as Juniper’s QFabric marketing team), Greg Ferro wrote a scathing <a href="http://etherealmind.com/soft-switching-fails-at-scale/">Soft Switching Fails at Scale</a> reply. While I agree with many of his arguments, the sad truth is that with the current state of server infrastructure virtualization we need soft switching regardless of the hardware vendors’ claims about the benefits of <a href="https://blog.ipspace.net/2011/05/edge-virtual-bridging-evb-8021qbg-eases.html">802.1Qbg</a> (EVB/VEPA), <a href="https://blog.ipspace.net/2011/06/vn-tag8021qbh-basics.html">802.1Qbh</a> (port extenders) or <a href="https://blog.ipspace.net/2011/08/vm-fex-how-convoluted-can-you-get.html">VM-FEX</a>.<!--more--></p>
<p>A virtual switch embedded in a typical hypervisor OS serves two purposes: it does (usually abysmal) layer-2 forwarding and (more importantly) hides the details of the physical hardware from the VM. Virtual machines think they work with a typical Ethernet NIC – usually based on a well-known chipset like Intel’s 82545 controller or AMD Lance controller – or you could use special drivers that allow the VM to interact with the hypervisor more effectively (for example, VMware’s VMXNET driver). </p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/08/s1600-VM_Net_Stack.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="320" src="/2011/08/s320-VM_Net_Stack.png" width="263"/></a></div>
<p>In both cases, the details of the physical hardware are hidden from the VM, allowing you to deploy the same VM image on any hypervisor host in your data center (or cloudburst it if you believe in that particular mythical beast), regardless of the host’s physical Ethernet NIC. The hardware abstraction also makes the vMotion process run smoothly – the VM does not need to re-initialize the physical hardware once it’s moved to another physical host. VMware (and probably most other hypervisors) solves the dilemma in a brute force way – it <a href="http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=1029371">doesn’t allow you to vMotion</a> a VM that’s communicating directly with the hardware using <a href="http://kb.vmware.com/kb/1010789">VMDirectPath</a>.</p>
<p class="note">The hardware abstraction functionality is probably way more resource-consuming than the simple L2 forwarding performed by the virtual switches; after all, how hard could it be to do a hash table lookup, token bucket accounting, and switch a few ring pointers?</p>
<p>The virtualized networking hardware also allows the hypervisor host to perform all sorts of memory management tricks. Most modern NICs use <a href="http://wiki.nil.com/Queuing_Principles_in_Cisco_IOS#Implementation_details">packet buffer rings</a> to exchange data between the operating system and the NIC; both parties (NIC and the CPUs) can read or write the ring structures at any time. Allowing a VM to talk directly with the physical hardware effectively locks it into the physical memory, as the hypervisor can no longer control how the VM has set up the NIC hardware and the ring structures; the Ethernet NIC can write into any location belonging to the VM it’s communicating with at any time.</p>
<p>I am positive there are potential technical solutions to all the problems I’ve mentioned, but they are simply not available on any server infrastructure virtualization platform I’m familiar with. The vendors deploying new approaches to virtual networking thus have to rely on a forwarding element embedded in the hypervisor kernel, like the passthrough VEM module Cisco is using in its VM-FEX implementation.</p>
<p>In my opinion, it would make way more sense to develop a technology that tightly integrates hypervisor hosts with the network (EVB/VDP parts of the 802.1Qbg standard) than to try to push a square peg into a round hole using VEPA or VM-FEX, but we all know that’s not going to happen. Hypervisor vendors don’t seem to care and the networking vendors want you to buy more of their gear.</p>
<h4>More information</h4><p>You’ll find in-depth description of Adapter FEX, VM-FEX, Nexus 1000V and EVB/VEPA in my <a href="http://www.ioshints.info/VMnet">VMware Networking Deep Dive</a> (<a href="http://www.ioshints.info/Recordings?code=VMnet">recording</a> or <a href="http://vnetwork.eventbrite.com/">live session</a>) webinar. Data center architectures and basics of virtual networking are also described in <a href="http://www.ioshints.info/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking Engineers</a> (<a href="http://www.ioshints.info/Recordings?code=DC30">recording</a>). Both webinars are available as part of the <a href="http://www.ioshints.info/Subscription_to_ioshints_webinars">yearly subscription</a>.</p>

