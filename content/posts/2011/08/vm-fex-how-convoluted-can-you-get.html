---
date: 2011-08-17T07:08:00.000+02:00
tags:
- switching
- data center
- workshop
- virtualization
title: VM-FEX – not as convoluted as it looks
url: /2011/08/vm-fex-how-convoluted-can-you-get.html
---

<p>Reading Cisco’s marketing materials, <a href="http://www.cisco.com/en/US/solutions/ns340/ns517/ns224/ns836/ns1124/vm-fex.html">VM-FEX</a> (the feature probably known as <a href="http://www.cisco.com/en/US/netsol/ns894/index.html">VN-Link</a> before someone went on a FEX-branding spree) seems like a fantastic idea: VMs running in an ESX host are connected directly to virtual physical NICs offered by the Palo adapter and then through point-to-point virtual links to the upstream switch where you can deploy all sorts of features the virtual switch embedded in the ESX host still cannot do. As you might imagine, the reality behind the scenes is more complex.<!--more--></p>
<p>The first picture shows the mental model of VM-FEX architecture I would get after reading high-level whitepapers. According to this mental model, some <a href="http://www.catb.org/jargon/html/H/handwave.html">handwave</a> magic would automatically provision the virtual NIC and the upstream switch every time a new VM is started or vMotioned into an ESX host. </p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/08/s1600-VM-FEX-Marketing.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="248" src="/2011/08/s320-VM-FEX-Marketing.png" width="320"/></a></div>
<p>The second picture shows the reality: the control- and management-plane flows that have to take place for VM-FEX to work.</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/08/s1600-VM-FEX-Reality.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="211" src="/2011/08/s320-VM-FEX-Reality.png" width="320"/></a></div>
<p>Before you can start deploying VM-FEX, the virtual Ethernet (vEthernet) adapters (on Palo NIC) used by VM-FEX have to be pre-provisioned by the UCS Manager; SR-IOV could be used to make changes in real time, but it’s not supported by vSphere.</p>
<p class="info">You might have to reload your physical server before the changes take effect. However, due to the way UCS manager allocates PCI resources, the previously-created vEthernet/HBA adapters won’t change (your server will continue to work after the reload). </p>
<p>When a new VM NIC has to be activated (due to VM startup or vMotion event), the following events take place:</p>
<ul class="ListParagraph"><li>Whenever a new VM is moved to or started in an ESX host, vCenter changes port state in a vDS port group;</li>
<li>ESX signals the port change to the vDS kernel module (Virtual Ethernet Module – VEM). VEM thus learns the port group name and the port number of the newly-enabled virtual port.</li>
<li>VEM selects a free vEthernet adapter and establishes a link between VM’s virtual NIC and the vEthernet adapter.</li>
<li>VEM propagates the change to the UCS manager.</li>
<li>UCS manager configures a virtual port corresponding to the newly-activated vEthernet adapter in the upstream switch (Nexus 6100);</li>
</ul>
<p class="info">Even though both Nexus 1000V and VM-FEX use VEM kernel module, you don’t need Nexus 1000V to implement VM-FEX. Actually, you have to select one or the other today; you cannot run both in the same host (it would make no sense anyway).</p>
<p>There are a few architectural reasons for the complex architecture used by VM-FEX:</p>
<p><strong>You cannot tie a VM to a physical NIC</strong>. The vEthernet NIC created on the Palo NIC appears as regular physical NICs to the operating system (ESX). You cannot tie a VM directly to a physical NIC; although the VMDirectPath allows a VM to use physical hardware, it also disables vMotion for that VM. Therefore even though VMs use physical NICs, we still need a kernel module (VEM) that acts like a patch panel and shuffles the data between VM virtual NIC drivers and physical NIC.</p>
<p class="note">vSphere 5 can support <a href="http://www.cisco.com/en/US/prod/collateral/modules/ps10277/ps10331/white_paper_c11-618838.html">vMotion with VMDirectPath for vEthernet adapters</a>. </p>
<p><strong>ESX cannot create new </strong><strong>vEthernet NICs</strong>. Although you could create hardware on demand with the SR-IOV technology, neither ESX nor Palo adapter support SR-IOV at the moment. The only way to create a new vEthernet adapter on the Palo adapter is thus from the outside (through UCS manager).</p>
<p><strong>vSphere/ESX host cannot signal to the upstream switch what it needs</strong>. What we would need to implement vEthernet integration properly is <a href="https://blog.ipspace.net/2011/05/edge-virtual-bridging-evb-8021qbg-eases.html">EVB’s VSI Discovery Protocol (VDP)</a>. VDP is not implemented by vSphere, so VM-FEX needs a vDS replacement (VEM) that provides both data-plane pass-through functionality and control-plane communication with the upstream devices.</p>
<p class="note">In the initial implementation of VM-FEX, VEM communicates with the UCS Manager. According to Shrijeet Mukherjee from Cisco, the communication will take place directly between VEM and upstream Nexus 6100 switch in the 2.x UCS software release. </p>
<p>Actually, Cisco had to implement functionality equivalent to both <a href="https://blog.ipspace.net/2011/06/vn-tag8021qbh-basics.html">802.1Qbh/802.1BR standard</a> (VN-Tag – support for virtual link tagging) and parts of <a href="https://blog.ipspace.net/2011/05/edge-virtual-bridging-evb-8021qbg-eases.html">802.1Qbg (VDP)</a> to get VM-FEX up and running.</p>
<p class="update">Update 2011-09-23: Shrijeet Mukherjee (Director of Engineering, Virtual Interface Card @ Cisco) kindly helped me understand the technical details of the VM-FEX architecture. I updated the post based on that information.</p>
<!-- <p>When a new VM NIC has to be provisioned, the following events take place (I might be oversimplifying things a bit; if that’s the case, please leave a comment):</p><ul class="ListParagraph"><li>Whenever a new VM is moved to or started in an ESX host, vCenter changes port state in a vDS port group;</li><li>UCS manager picks up the changed vDS port state (hopefully using some sort of vCenter notifications and not periodic polling);</li><li>UCS manager creates a new virtual NIC in the Palo adapter;</li><li>UCS manager configures a virtual port corresponding to the newly-added virtual NIC in the upstream switch (Nexus 6100);</li><li>UCS manager configures the Virtual Ethernet Module (VEM) within the ESX host to link the VM’s virtual NIC to the virtual physical NIC on the Palo adapter.</li></ul><p>There are two fundamental reasons for the complex implementation of VM-FEX – lack of ESX features and architectural limits of FEX/802.1Qbh architecture:</p><p><strong>You cannot tie a VM to a physical NIC</strong>. The virtual NICs created by the Palo adapter appear as regular physical NICs to the operating system (ESX). You cannot tie a VM directly to a physical NIC; although the VMDirectPath allows a VM to use physical hardware, it also disables vMotion for that VM. Therefore even though VMs use physical NICs, we still need a kernel module (VEM) that shuffles the data between VM virtual NIC drivers and physical NIC.</p><p><strong>ESX cannot create new physical NICs</strong>. Although you could create hardware on demand with the SR-IOV technology, neither ESX nor Palo adapter support SR-IOV at the moment. The only way to create a new NIC on the Palo adapter is thus from the outside (through UCS manager).</p><p><strong>Hypervisor host cannot signal to the upstream switch what it needs</strong>. This is a fundamental architectural limitation of <a href="https://blog.ipspace.net/2011/06/vn-tag8021qbh-basics.html">FEX technology and 802.1Qbh/802.1BR standard</a>. The port extender cannot signal to the upstream switch what it needs and what’s attached to it – it can only tell the upstream switch that it has a new port. The upstream switch has to know how to configure the new port; in VM-FEX, UCS manager has to contact vCenter to figure out which VM is connected to the new port, and then use its own configuration rules to configure the Nexus 6100 switch.</p><p>The last point is the really bad news – even if ESX would support SR-IOV and if we would somehow solve the direct NIC access from virtual machines, the existing FEX/802.1Qbh architecture would prevent ESX from telling the upstream switch what it needs. <a href="https://blog.ipspace.net/2011/05/edge-virtual-bridging-evb-8021qbg-eases.html">EVB (802.1Qbg) is much better in this respect</a> – its VSI Discovery Protocol (VDP) allows the hypervisor host to tell the upstream switch what VMs are connected to its virtual switch.</p><p style='font-size: 80%'>Disclaimer: it’s probably a bad idea to write blog posts while sitting at the airport and being bombarded with CNN news in the background. In case I made a major blunder, please write a comment ... and I apologize for it in advance.</p>--><h4>More information</h4><p>You’ll find in-depth description of Adapter FEX, VM-FEX, Nexus 1000V and EVB/VEPA in my <a href="http://www.ioshints.info/VMnet">VMware Networking Deep Dive</a> (<a href="http://www.ioshints.info/Recordings?code=VMnet">recording</a> or <a href="http://vnetwork.eventbrite.com/">live session</a>) webinar. Data center architectures and virtual networking are also described in <a href="http://www.ioshints.info/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking Engineers</a> (<a href="http://www.ioshints.info/Recordings?code=DC30">recording</a>). Both webinars are available as part of the <a href="http://www.ioshints.info/Subscription_to_ioshints_webinars">yearly subscription</a>.</p>

