---
url: /2011/06/fcoe-over-trill-this-time-from-juniper.html
title: "FCoE over TRILL ... this time from Juniper"
date: "2011-06-15T06:27:00.001+02:00"
tags: [ Data Center,SAN,Workshop,FCoE,TRILL ]
---

<p>A <a href="http://twitter.com/jmichelmetz/status/79292972063145985">tweet from J Michel Metz</a> has alerted me to a “<a href="http://searchnetworking.techtarget.com/news/2240036749/Why-TRILL-wont-work-for-data-center-network-architecture"><em>Why TRILL won't work for data center network architecture</em></a>” article by Anjan Venkatramani, Juniper’s VP of Product Management. Most of the long article could be condensed in two short sentences my readers are very familiar about: <em>Bridging does not scale </em>and <em>TRILL does not solve the traffic trombone issues</em> (hidden implication: QFabric will solve all your problems)... but the author couldn’t resist throwing “<em>FCoE over TRILL</em>” bone into the mix.<!--more--></p>
<p>I thought that bone has been scrapped clean in the last few months, but it seems some people still think it’s worth chewing. Let’s try to make it really simple. There are <a href="https://blog.ipspace.net/2010/08/multihop-fcoe-101.html">two fundamental ways of implementing multi-hop FCoE</a>: </p>
<ul class="ListParagraph"><li><strong>Dense-mode FCoE</strong>, where every LAN switch is also Fiber Channel Forwarder (FCF; equivalent to an IP router). FCoE frames are routed hop-by-hop and FCoE routing does <em>not </em>interact with STP or TRILL (because FCoE is not bridged). It’s also easy to implement SAN A/SAN B separation (two independent non-overlapping paths from each server to each storage device) as you control hop-by-hop flow of the FC traffic.</li>
<li><strong>Sparse-mode FCoE</strong>, where LAN switches bridge FCoE frames like any L2 traffic and optionally provide FIP snooping. Because FCoE is bridged in this design, sparse-mode FCoE needs stable L2 transport which TRILL can provide more reliably than existing STP-based networks. Since you can’t control hop-by-hop FCoE traffic flow (it’s bridged according to whatever TRILL or STP thinks is the best path), it’s way harder to implement the SAN separation.</li>
</ul>
<p>Got it? How hard was it? Do I really need to spell it out every few months?</p>
<p class="note">Note to the equipment buyers: every time a vendor mentions “<em>FCoE over TRILL</em>” ask yourself “<em>are they </em><em>trying to divert my attention</em><em>?</em>” Most probably they don’t have full-blown FCF code in their switches.</p>
<p>Obviously I had to check QFX3500 <a href="http://www.juniper.net/techpubs/en_US/junos11.1/information-products/topic-collections/qfx-series/storage/storage.pdf">JunOS documentation</a> after reading the afore-mentioned article. As I came to expect from Juniper, the documentation is precise and well-written. QFX3500 supports FC and FCoE, has up to 12 “universal” ports (two groups of 6 ports) that can be configured as 2/4/8 GB FC or 10GE ... but does not have a full-blown L3 FC stack (as I expected). The only marketing intrusion I found in the documentation was the <em>FCoE transit switch</em>, a term which does not appear anywhere in the FC-BB-5 standard – obviously someone thought it sounds better than <em>FIP snooping bridge</em>.</p>
<p>Here’s what QFX3500 can do on the FCoE/FC front:</p>
<ul class="ListParagraph"><li>It can be a <a href="https://blog.ipspace.net/2010/09/multihop-fcoe-102-vnport-proxy-and-fip.html">FIP-snooping bridge</a>, forwarding FCoE traffic between its 10GE ports and protecting FCFs and FCoE Enodes with dynamically built ACLs based on FIP requests/responses. Bonus points for the documentation being very clear on the traffic flow: FCoE traffic has to go through a full-blown FCF.</li>
<li>It can be an <a href="https://blog.ipspace.net/2010/09/multihop-fcoe-102-vnport-proxy-and-fip.html">FC N_port proxy</a> (using NPIV) for FCoE Enodes. Yet again, as it doesn’t have the FCF code, the traffic between FCoE nodes has to go to a FC-based FCF and back (also clearly documented).</li>
<li>It supports exactly the same DCB standards as Cisco: <a href="https://blog.ipspace.net/2010/09/introduction-to-8021qbb-priority-flow.html">PFC</a>, <a href="https://blog.ipspace.net/2010/09/introduction-to-8021qaz-enhanced.html">ETS</a> and DCBX, but not <a href="https://blog.ipspace.net/2010/11/data-center-bridging-dcb-congestion.html">QCN</a>. PFC and ETS should be enough unless you build really large bridged networks (hint: don’t).</li>
</ul>
<p>In most cases, the functionality offered by QFX3500 is more than enough to implement access-layer convergence. If you use its NPIV functionality and send FCoE traffic from the servers straight into FC SAN, you can even maintain true SAN separation with servers attached to two QFX3500 switches. Transporting FCoE across the network toward an FCF (or another QFX3500 acting as NPIV proxy) that’s several hops away should still work, but it’s harder to maintain the strict SAN separation.</p>
<p>Am I allowed to conclude with a note to the vendors’ marketing departments? We might like you more if you tell us <em>how your boxes actually work, what they can do and how we can build great solutions with them </em>instead of constantly harassing us with the arguments <em>why the things you haven’t implemented aren’t rosy</em>, more so if you have a good product that can’t possibly benefit from such tactics ... not that I would ever expect them to listen.</p>
<h4>More information</h4><p>You’ll find more information about FCoE, FIP snooping, FC proxy solutions and Data Center Bridging (DCB) standards in my <a href="http://www.ioshints.info/DataCenterIntro">Data Center 3.0 for Networking Engineers</a> webinar (<a href="http://www.ioshints.info/Recordings">buy a recording</a> or a <a href="http://www.ioshints.info/Subscription">yearly subscription</a>).</p>

