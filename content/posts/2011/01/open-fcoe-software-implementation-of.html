---
date: 2011-01-31T06:30:00.002+01:00
tags:
- data center
- workshop
- FCoE
title: Open FCoE – Software implementation of the camel jetpack
url: /2011/01/open-fcoe-software-implementation-of.html
---

<p>Intel announced its <a href="http://newsroom.intel.com/community/intel_newsroom/blog/2011/01/27/intel-simplifies-the-data-center">Open FCoE</a> (software implementation of FCoE stack on top of Intel’s 10GB Ethernet adapters) using the cloudy <a href="http://www.bullshitbingo.net/">bullshit bingo</a> including <em>simplifying the Data Center</em>, <em>Free New Technology</em>, <em>Cloud Vision</em> and <em>Green Computing</em> (ok, they used <em>Environmental impact</em>) and lots of positive supporting quotes. The only thing missing was an enthusiastic Gartner quote (or maybe they were too expensive?).</p>
<!--more--><p>Not surprisingly, the storage blogosphere got all ecstatic (example: <a href="http://virtualgeek.typepad.com/virtual_geek/2011/01/native-open-intel-fcoe-software-stack-game-changer-imo.html">Native, Open Intel FCoE Software stack = game changer</a> by Virtual Geek). The server admins might not share the enthusiasm – <a href="http://www.definethecloud.net/intels-betting-the-storage-io-farm-on-the-cpu">Joe Onisick had no good words for the idea of CPU-based FC implementation</a> back in November. And then there’s the obvious compatibility problem. As <a href="http://siliconangle.com/blog/2011/01/28/hp-and-intel-help-open-the-fcoe-market/">Stuart Miniman pointed out</a>, the moment you start implementing hardware functionality in software, you have to relive the missing driver issues that <a href="/2009/12/iscsi-and-san-two-decades-of-rigid.html">prompted the storage industry to cling to SCSI and invent FC in the first place</a> (this time it’s lack of support by vSphere hypervisor).</p>
<p>My cynical view hasn’t changed: Intel obviously doesn’t believe in FCoE or they would have invested resources in developing FC HBA in hardware (like every other CNA vendor is doing). It’s orders of magnitude cheaper to write highly optimized software than it is to develop new silicon.</p>
<p>With the CRC-calculation-focused instruction set being available in Xeon processors and versatile 10GB Ethernet adapters being well accepted by the market, it obviously makes no sense to Intel to invest its hardware design and fabrication resources in reinventing the wheel that’s slowly dying. </p>
<p>Regardless of whether you call FCoE “camel jetpack” (courtesy of Greg Ferro) or LLC2-over-Ethernet (because <a href="/2010/09/storage-networking-is-like-sna.html">storage networking really is like SNA</a>), the history of similar technological battles and the current trends are clear to anyone who’s willing to look. FCoE is a great access solution prolonging the life of a legacy technology (like LLC2-over-Ethernet was to those IBM-focused shops that were fed up with “reassuringly priced” and overly complex Token Ring), but the IT industry has never tolerated competing solutions to the same problem; the cheaper one that was just-good-enough usually won in the long term (which is why Facebook is not running on mainframes, Lotus Notes lost to Outlook and Banyan Vines lost to Netware which lost to Windows networking).</p>
<h4>More information</h4><p>To get an overview of modern Data Center technologies, Data Center Bridging, FCoE and virtualization, watch my <a href="https://www.ipspace.net/DC30">Data Center 3.0 for Networking Engineers</a> webinar (<a href="https://www.ipspace.net/SingleRecording?code=DC30">buy a recording</a> or <a href="https://www.ipspace.net/Subscription">yearly subscription</a>).</p>

