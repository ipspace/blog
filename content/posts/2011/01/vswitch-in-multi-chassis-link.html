---
date: 2011-01-26T06:45:00.011+01:00
tags:
- link aggregation
- switching
- data center
- workshop
- virtualization
title: vSwitch in Multi-chassis Link Aggregation (MLAG) environment
url: /2011/01/vswitch-in-multi-chassis-link.html
---

<p>Yesterday I described how the <a href="https://blog.ipspace.net/2011/01/vmware-vswitch-does-not-support-lacp.html">lack of LACP support in VMware’s vSwitch and vDS can limit the load balancing options offered by the upstream switches</a>. The situation gets totally out-of-hand when you connect an ESX server with two uplinks to two (or more) switches that are part of a Multi-chassis Link Aggregation (MLAG) cluster.</p>
<p>Let’s expand the small network described in the previous post a bit, adding a second ESX server and another switch. Both ESX servers are connected to both switches (resulting in a fully redundant design) and the switches have been configured as a MLAG cluster (using VSS with Catalyst 6500, vPC with Nexus 7000 or Nexus 5000, or IRF with the HP switches). Link aggregation is not used between the physical switches and ESX servers due to lack of LACP support in ESX.</p>
<!--more--><div class="separator" style="clear: both; text-align: center;"><a href="/2011/01/s1600-vSwitch_MLAG_Phy.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" src="/2011/01/s320-vSwitch_MLAG_Phy.png"/></a></div>
<p>The physical switches are unaware of the physical connectivity the ESX servers have. Assuming that vSwitches use per-VM load balancing and each VM is pinned to one of the uplinks, source MAC address learning in the physical switches produces the following logical topology:</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/01/s1600-vSwitch_MLAG_Log.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" src="/2011/01/s320-vSwitch_MLAG_Log.png"/></a></div>
<p>Each VM appears to be single-homed to one of the switches. The traffic between VM A and VM C is thus forwarded locally by the left-hand switch; the traffic between VM A and VM D has to traverse the inter-switch link because neither switch knows VM D can also be reached by a shorter path.</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/01/s1600-vSwitch_MLAG_Traffic.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" src="/2011/01/s320-vSwitch_MLAG_Traffic.png"/></a></div>
<p>In a Multi-chassis Link Aggregation scenario it’s thus almost mandatory to configure static port channel on the switches to which the vSphere servers are connected, otherwise you risk overloading the inter-switch link as the traffic between adjacent ESX servers is needlessly sent across that link. When doing that, you (probably) have to configure IP-hash-based load balancing in vSwitch (more information about the <a href="http://frankdenneman.nl/2009/11/nfs-and-ip-hash-loadbalancing/">vSwitch-side implications if the NICs in ESX are configured in active/standby configuration</a>).</p>
<p>It's much better (at least from Cisco’s perspective) to use Nexus 1000V – it supports LACP, ESX servers start behaving like access-layer switches and the traffic flow always remains optimal (at least <a href="https://blog.ipspace.net/2010/12/multi-chassis-link-aggregation-mlag-and.html">within the boundaries of hot-potato switching</a>).</p>
<h4>More information</h4><p>Interaction of vSwitch with link aggregation is just one of many LAG-related topics covered in my <a href="https://www.ipspace.net/DC30">Data Center 3.0 for Networking Engineers</a> webinar (<a href="https://www.ipspace.net/SingleRecording?code=DC30">buy a recording</a> or <a href="https://www.ipspace.net/Subscription">yearly subscription</a>).</p>

