---
url: /2011/04/vcloud-director-networking.html
title: "vCloud Director Network Isolation (vCDNI) scalability"
date: "2011-04-12T06:43:00.002+02:00"
tags: [ Data Center,Workshop,virtualization ]
---

<p>When VMware launched its vCloud Director Networking Infrastructure, <a href="http://etherealmind.com/">Greg Ferro</a> (of the <a href="http://packetpushers.net/">Packet Pushers Podcast</a> fame) and myself were <a href="https://blog.ipspace.net/2010/11/vcloud-director-hand-network-over-to.html">very skeptical</a> about its scaling capabilities, more so as it uses MAC-in-MAC encapsulation and bridging was never known for its scaling properties. However, Greg’s VMware contacts claimed that vCDNI scales to thousands of physical servers and Greg wanted to do a podcast about it. </p>
<p>As always, we prepared a few questions in advance, including “<em>How are broadcasts, multicasts and unknown unicasts </em><em>handled </em><em>in vCDNI-based private networks?</em>” and “<em>what happens when a single VM goes crazy?</em>” For one reason or another, the podcast never happened. After analyzing <a href="https://blog.ipspace.net/2011/02/looking-for-vcdni-packet-traces.html">Wireshark traces of vCDNI traffic</a>, I probably know why that’s the case.</p>
<!--more--><p><strong>The MAC-in-MAC encapsulation is proprietary</strong> (but we already knew that). It’s still the same encapsulation Lab Manager used years ago. However, Lab Manager was just that; VMware sells vCDNI as a scalable cloud-enabling platform (the only thing they managed to do was to move the MAC-in-MAC encapsulation from a dedicated VM into a hypervisor kernel module).</p>
<pre class="code">Ethernet II frame<br/>    Destination: Akimbi_01:00:11 (00:13:f5:01:00:11)<br/>    Source: Akimbi_01:00:21 (00:13:f5:01:00:21)<br/>    Type: VMware Lab Manager (0x88de)<br/><strong>VMware Lab Manager, Portgroup: 1</strong><br/><strong>    0000 0... = Unknown       : 0x00</strong><br/><strong>    .... .0.. = More Fragments: Not set</strong><br/><strong>    .... ..00 = Unknown       : 0x00</strong><br/><strong>    Portgroup        : 1</strong><br/><strong>    Address          : Vmware_90:33:6a (00:50:56:90:33:6a)</strong><br/><strong>    Destination      : Vmware_90:33:6a (00:50:56:90:33:6a)</strong><br/><strong>    Source           : Vmware_90:30:ab (00:50:56:90:30:ab)</strong><br/><strong>    Encapsulated Type: IP (0x0800)</strong><br/>Internet Protocol, Src: 192.168.1.100, Dst: 192.168.1.101<br/>    Version: 4<br/>    Header length: 20 bytes<br/>    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00)<br/>    Total Length: 60<br/>    Identification: 0x01c1 (449)<br/>    Flags: 0x00<br/>    Fragment offset: 0<br/>    Time to live: 128<br/>    Protocol: ICMP (1)<br/>    Header checksum: 0xb4e6 [correct]<br/>    Source: 192.168.1.100 (192.168.1.100)<br/>    Destination: 192.168.1.101 (192.168.1.101)</pre><p><strong>Broadcasts on internal networks (“protected” with vCDNI) get translated into global broadcasts</strong>. This behavior totally destroys scalability. In VLAN-based designs, the number of hosts and VMs affected by a broadcast is limited by the VLAN configuration... unless you stretch VLANs all across the data center (but then you ask for trouble).</p>
<p>When you use vCDNI, every single ESX server connected to the same LAN will get affected if a single VM goes bonkers.</p>
<pre class="code">Ethernet II frame<br/><strong>    Destination: Broadcast (ff:ff:ff:ff:ff:ff)</strong><br/>    Source: Akimbi_01:00:21 (00:13:f5:01:00:21)<br/>    Type: VMware Lab Manager (0x88de)<br/>VMware Lab Manager, Portgroup: 1<br/>    0000 0... = Unknown       : 0x00<br/>    .... .0.. = More Fragments: Not set<br/>    .... ..00 = Unknown       : 0x00<br/>    Portgroup        : 1<br/><strong>    Address          : Broadcast (ff:ff:ff:ff:ff:ff)</strong><br/><strong>    Destination      : Broadcast (ff:ff:ff:ff:ff:ff)</strong><br/>    Source           : Vmware_90:30:ab (00:50:56:90:30:ab)<br/>    Encapsulated Type: ARP (0x0806)<br/>    Trailer: 000000000000000000000000000000000000<br/>Address Resolution Protocol (request)<br/>    Hardware type: Ethernet (0x0001)<br/>    Protocol type: IP (0x0800)<br/>    Hardware size: 6<br/>    Protocol size: 4<br/>    Opcode: request (0x0001)<br/>    [Is gratuitous: False]<br/>    Sender MAC address: Vmware_90:30:ab (00:50:56:90:30:ab)<br/>    Sender IP address: 192.168.1.100 (192.168.1.100)<br/>    Target MAC address: 00:00:00_00:00:00 (00:00:00:00:00:00)<br/>    Target IP address: 192.168.1.101 (192.168.1.101)</pre><p><strong>Multicasts on internal networks are translated into global multicasts</strong>. To make matters worse, IGMP snooping can no longer help you, as the IGMP messages sent by VMs get encapsulated into the MAC-in-MAC envelope and are thus never seen by the switches.</p>
<pre class="code">Ethernet II frame<br/><strong>    Destination: IPv4mcast_00:00:01 (01:00:5e:00:00:01)</strong><br/>    Source: Akimbi_01:00:21 (00:13:f5:01:00:21)<br/>    Type: VMware Lab Manager (0x88de)<br/>VMware Lab Manager, Portgroup: 1<br/>    0000 0... = Unknown       : 0x00<br/>    .... .0.. = More Fragments: Not set<br/>    .... ..00 = Unknown       : 0x00<br/>    Portgroup        : 1<br/><strong>    Address          : IPv4mcast_00:00:01 (01:00:5e:00:00:01)</strong><br/><strong>    Destination      : IPv4mcast_00:00:01 (01:00:5e:00:00:01)</strong><br/>    Source           : Vmware_90:30:ab (00:50:56:90:30:ab)<br/>    Encapsulated Type: IP (0x0800)<br/>Internet Protocol, Src: 192.168.1.100, <strong>Dst: 224.0.0.1</strong><br/>    Version: 4<br/>    Header length: 20 bytes<br/>    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00)<br/>    Total Length: 60<br/>    Identification: 0x01ce (462)<br/>    Flags: 0x00<br/>    Fragment offset: 0<br/>    Time to live: 128<br/>    Protocol: ICMP (1)<br/>    Header checksum: 0x96e5 [correct]<br/>    Source: 192.168.1.100 (192.168.1.100)<br/>    Destination: 224.0.0.1 (224.0.0.1)<br/>Internet Control Message Protocol<br/>    Type: 8 (Echo (ping) request)<br/>    Code: 0<br/>    Checksum: 0x325c [correct]<br/>    Identifier: 0x0200<br/>    Sequence number: 6400 (0x1900)<br/>    Sequence number (LE): 25 (0x0019)<br/>    Data (32 bytes)</pre><p><strong>It’s totally insecure</strong>. Anyone connected to the same LAN as the ESX servers using vCDNI can insert packets into “protected” portgroups. Even a virtual machine connected to a portgroup using the same NIC could do it (assuming you got the VLAN setup wrong).</p>
<p>It’s also not hard to collect the VM MAC addresses and IP addresses from all “protected” internal networks, as you see every single ARP request (from every portgroup) as soon as you’re connected to the server LAN.</p>
<p><strong>Conclusion</strong>: while I believe vCloud Director is a great product, more so for GUI-happy wizard-craving beginners (it does get boring after you have to create a few objects and I can only hope someone will write a good CLI for it), the current vCDNI implementation is not more than a proof-of-concept that I would never use in a large-scale network.</p>

