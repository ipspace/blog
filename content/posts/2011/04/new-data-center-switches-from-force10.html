---
date: 2011-04-26T17:01:00.005+02:00
tags:
- switching
- data center
title: New Data Center switches from Force10
url: /2011/04/new-data-center-switches-from-force10.html
---

<p>Force10 has just launched a new series of data center switches. The <em>ZettaScale </em>switches are, as one would expect from Force10, down-to-earth high-performance low-footprint products – a good option for those network engineers that like building high-density high-performance data centers with minimal feature overload.</p>
<p class="note">All the information in this post is based on the briefing I’ve received from Force10 last week, the draft materials they sent me and the subsequent answers to my questions. I haven’t been able to touch the boxes or read the product documentation yet.</p>
<!--more--><p>The fixed-configuration Z9000 switch packs 2.5Tbps non-blocking architecture with 128 GE/32 40GE ports in 2RU footprint. As you might know, <a href="http://etherealmind.com/notes-physical-connectors-40-100-gigabit-ethernet/">40GE uses four multiplexed 10GE channels</a> and Force10 decided to make good use of that fact: the physical switch has 40GE ports; with a breakout cable you get four independent 10GE ports out of each 40GE port.</p>
<p>The modular Z9512 switch (available later this year) has 480 10GE/96 40GE/48 100GE ports and 9.6Tbps of switching capacity in 19RU footprint.</p>
<p>On the L2 side, the chipsets used in both switches are capable of handling TRILL and DCB standards. TRILL functionality will be available in a follow-up software upgrade (no dates yet). The switches also support EVB (<em>Ethernet Virtual Bridging </em>– 802.1Qbg); unfortunately I haven’t seen any host/hypervisor-side EVB implementations yet. </p>
<p>FCoE hasn’t been mentioned anywhere, but if you really want to use it, you can probably <a href="/2010/08/multihop-fcoe-101.html">get FCoE running across the Z-series switches once their software supports full-blown DCB</a> (the SAN people might be a nervous wreck because the switches between FCoE clients and FCF won’t support <a href="/2010/09/multihop-fcoe-102-vnport-proxy-and-fip.html">FIP snooping</a>; the LAN people will probably say “a separate VLAN is good enough”).</p>
<p>Both products also have full-blown L3 functionality. As with most other Force10 products, the preferred routing protocol is OSPF.</p>
<p>As you see, Force10 hasn’t been infected with the fabric craze yet, but with the port/bandwidth density they offer most data centers won’t need more than a few switches in the near future anyway. Imagine buying four Z9000 switches and using half of the ports for inter-switch connectivity. You get a total of 250 10GE ports connected to an almost non-blocking switching matrix. Alternatively, you could decide to go for 1:3 oversubscription and get 384 10GE ports.</p>
<p>Now imagine connecting Cisco’s rack servers to these four switches. With four 10GE ports per <a href="http://www.cisco.com/en/US/prod/collateral/ps10265/ps10493/ps11588/data_sheet_c78-648148.html">C260 M2 rack server</a> (which is probably quite an overkill) and 1:3 oversubscription between the switches, you can connect up to 96 rack servers to the four switches, giving you a maximum of 96 TB of RAM, 960 Xeon cores and almost a petabyte of disk capacity. Probably more than enough for a small data center.</p>
<p>The only real grudge I have (so far) is with the Force10 marketing department: read the <a href="http://en.wikipedia.org/wiki/Zetta-">definition of Zetta</a> and try to figure out how many zeroes there are between (almost) 10 terabit switching capacity of a Z9512 and a zettabit.</p>

