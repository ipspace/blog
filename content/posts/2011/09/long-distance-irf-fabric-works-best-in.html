---
url: /2011/09/long-distance-irf-fabric-works-best-in.html
title: "Long-distance IRF fabric: works best in PowerPoint"
date: "2011-09-12T06:39:00.000+02:00"
tags: [ switching,Data Center,Workshop,WAN ]
---

<p>HP has recently commissioned an <a href="http://www3.networktest.com/hpirf/hpirf1.pdf">IRF network test</a> that came to absolutely astonishing conclusions: vMotion runs almost twice as fast across two links bundled in a port channel than across a single link (with the other one being blocked by STP). The test report contains one other gem, this one a result of incredible creativity of HP marketing:</p>
<blockquote class="cite">For disaster recovery, switches within an IRF domain can be deployed across multiple data centers. According to HP, a single IRF domain can link switches up to 70 kilometers (43.5 miles) apart.</blockquote>
<p>You know my <a href="https://blog.ipspace.net/2011/06/stretched-clusters-almost-as-good-as.html">opinions about stretched cluster</a> ... and the more down-to-earth part of HP Networking (the people writing the documentation) agrees with me.<!--more--></p>
<p class="note">Please note: this post is not a critique of IRF fabric technology or its implementation, just of a particularly "creative" use case.</p>
<p>Let’s assume someone is actually brave enough to deploy a network using the design shown in the following figure with switches in two data centers merged into an IRF fabric (according to my Twitter friends this design is <a href="http://twitter.com/singlekorn/status/81069716852056065">occasionally promoted by some HP-certified instructors</a>):</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/09/s1600-IRF_DC_Stupid.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="118" src="/2011/09/s320-IRF_DC_Stupid.png" width="320"/></a></div>
<p>The <a href="http://bizsupport1.austin.hp.com/bc/docs/support/SupportManual/c02985993/c02985993.pdf">IRF documentation for the A7500 switches</a> (<a href="http://h20000.www2.hp.com/bizsupport/TechSupport/DocumentIndex.jsp?contentType=SupportManual&amp;lang=en&amp;cc=us&amp;docIndexId=64179&amp;taskId=101&amp;prodTypeId=12883&amp;prodSeriesId=4177519">published in August 2011</a>) contains the following facts about IRF partitions (split IRF fabric) and Multi-Active Detection (MAD) collisions (more commonly known as <em>split brain </em>problems):</p>
<blockquote class="cite">The partitioned IRF fabrics operate with the same IP address and cause routing and forwarding problems on the network.</blockquote>
<p>No surprise there, we always knew that split subnets cause interesting side effects, but it’s nice to see it acknowledged.</p>
<p class="note">It's interesting to note, though, that pure L2 solution might actually work ... but the split subnets would eventually raise their ugly heads in adjacent L3 devices.</p>
<blockquote class="cite">During an IRF merge, the switches of the IRF fabric that fails the master election must reboot to re-join the IRF fabric that wins the election.</blockquote>
<p>Hold on – I lose the inter-DC link(s), reestablish them, and then half of the switches reboot. Not a good idea.</p>
<p>Let’s assume the above design is “extended” with another bright idea – to detect split brain scenarios, the two switches run BFD over an alternate path (could be the Internet) to detect split brain events. According to the manual:</p>
<blockquote class="cite">An IRF link failure causes an IRF fabric to divide into two IRF fabrics and multi-active collision occurs. When the system detects the collision, it holds a role election between the two collided IRF fabrics. The IRF fabric whose master’s member ID is smaller prevails and operates normally. The state of the other IRF fabric transitions to the recovery state and temporarily cannot forward data packets.</blockquote>
<p>Isn’t that great – not only have you lost the inter-DC link, you’ve lost one of the core switches as well.</p>
<p><strong>Summary</strong>: As always, just because you can doesn’t mean you should ... and remember to be wary when consultants and marketing people peddle ideas that seem too good to be true.</p>
<h4>What are the alternatives?</h4><p>As I’ve explained in the <a href="http://www.ioshints.info/DCI">Data Center Interconnects</a> webinar (available as <a href="http://www.ioshints.info/Recordings?code=DCI">recording</a> or part of the <a href="http://www.ioshints.info/Subscription_to_ioshints_webinars">yearly subscription</a> or <a href="http://www.ioshints.info/Data_Center_trilogy">Data Center Trilogy</a>), there are at least two sensible alternatives if you really want to implement layer-2 DCI and have multiple parallel layer-1 links (otherwise IRF wouldn’t work either)</p>
<p><strong>Bundle multiple links in a port channel between two switches</strong><strong>.</strong> If you’re not concerned about device redundancy (remember: you can merge no more than two high-end switches in an IRF fabric), use port channel between the two DCI switches.</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/09/s1600-IRF_DC_PC.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="118" src="/2011/09/s320-IRF_DC_PC.png" width="320"/></a></div>
<p><strong>Use IRF (or any other MLAG solution) within the data center</strong> and establish a port channel between two IRF (or VSS or vPC) clusters. This design results in full redundancy without unexpected reloads or other interesting side effects (apart from the facts that Earth curvature didn't go away, Earth still orbits the Sun and not vice versa, and split subnets still don’t work).</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2011/09/s1600-IRF_DC_Full.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="200" src="/2011/09/s320-IRF_DC_Full.png" width="320"/></a></div>
<h4>... and don't forget!</h4><p>Should you wish to discuss the data center fabrics in person, don’t forget that I’ll be @ <a href="http://euronog.eu/meeting2011/schedule">EuroNOG</a> in a few weeks (arriving on Wednesday to participate in the second day of PLNOG) and probably @ <a href="http://techfieldday.com/2011/nfd2/">Net Field Day 2</a> in late October.</p>

