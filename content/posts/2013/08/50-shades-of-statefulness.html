---
date: 2013-08-22T07:48:00.000+02:00
tags:
- data center
- workshop
- scalability
- overlay networks
- high availability
title: 50 Shades of Statefulness
url: /2013/08/50-shades-of-statefulness.html
---

<p>A while ago Greg Ferro wrote a great article describing <a href="http://etherealmind.com/integrating-overlay-networking-and-the-physical-network/">integration of overlay and physical networks</a> in which he wrote that “<em>an overlay network tunnel has no state in the physical network</em>”, triggering an <a href="http://www.plexxi.com/2013/07/about-underlays/#sthash.TSJYrc6H.dpbs">almost-immediate reaction</a> from <a href="http://www.linkedin.com/pub/marten-terpstra/0/10b/b2">Marten Terpstra</a> (of RIPE fame, now @ Plexxi) arguing that the network (at least the first ToR switch) knows the MAC and IP address of hypervisor host and thus has at least some state associated with the tunnel.</p>
<p>Marten is correct from a purely <a href="http://en.wikipedia.org/wiki/How_many_angels_can_dance_on_the_head_of_a_pin%3F">scholastic perspective</a> (using his argument, the network keeps some state about TCP sessions as well), but what really matters is <strong>how much </strong>state is kept, <strong>which device </strong>keeps it, <strong>how it’s created </strong>and <strong>how often it changes</strong>.<!--more--></p>
<h4>How much state does a device keep?</h4><p>The end hosts have to keep state of every single TCP and UDP session, but most transit network devices (apart from abominations like NAT) don’t care about those sessions, making Internet as fast as it is. </p>
<p class="info">Decades ago we had a truly reliable system that kept session state in every single network node; it never lost a packet, but it barely coped with 2 Mbps links (the oldtimers might remember it as X.25 ;).</p>
<p>The state granularity should get ever coarser as you go deeper into the network core – edge switches keep MAC address tables and ARP/ND caches of adjacent end hosts, core routers know about IP subnets, routers in public Internet know about the publicly advertised prefixes (including <a href="http://www.cidr-report.org/cgi-bin/as-report?as=AS6389&amp;view=2.0">every prefix Bell South ever assigned to one of its single-homed customers</a>), while the high-speed MPLS routers know about BGP next hops and other forwarding equivalence classes (FECs)</p>
<h4>Which device keeps the state</h4><p>Well-designed architecture has <a href="https://blog.ipspace.net/2011/05/complexity-belongs-to-network-edge.html">complexity (and state) concentrated at the network edge</a>. The core devices keep minimum state (example: IP subnets), while the edge devices keep session state. In a virtual network case, the <a href="http://blog.ipspace.net/2011/12/decouple-virtual-networking-from.html">hypervisors should know the VM endpoints (MAC addresses, IP addresses, virtual segments)</a> and the physical devices just the hypervisor IP address, <a href="http://blog.ipspace.net/2013/07/smart-fabrics-versus-overlay-virtual.html">not the other way round</a>.</p>
<p>Furthermore, as much state as possible should be stored in low-speed devices using software-based forwarding. It’s pretty simple to store a million flows in software-based Open vSwitch (updating them is a different story) and mission-impossible to store 10.000 5-tuple flows in <a href="http://etherealmind.com/merchant-silicon-vendor-software-rise-lost-opportunity/">Trident 2 chipset used by most ToR switches</a>. </p>
<h4>How is state created</h4><p>Systems with control-plane (<a href="http://networkstatic.net/openflow-proactive-vs-reactive-flows/">proactive</a>) state creation (example: routing table built from routing protocol information) are always more scalable than systems that have to react to data-plane events in real time (example: MAC address learning or NAT table maintenance).</p>
<p>Data-plane-driven state is particularly problematic for devices with hardware forwarding – packets that change state (example: TCP SYN packets creating new NAT translation) might have to be <a href="https://blog.ipspace.net/2013/02/process-fast-and-cef-switching-and.html">punted to the CPU</a>, or you might have to implement state maintenance in hardware, which is expensive.</p>
<p>Finally, there’s the square circle aka “soft state” – cases where the protocol designers needed state in the network, but didn’t want to create a proper protocol to maintain it, so the end devices get burdened with periodic state refresh messages, and the transit devices spend CPU cycles refreshing the state. RSVP is a typical example, and everyone running large-scale MPLS/TE networks simply loves the periodic refresh messages sent by tunnel head-ends – they keep the core routers processing them cozily warm. </p>
<h4>How often does state change</h4><p>Devices with slow-changing state (example: BGP routers) are obviously more stable than devices with fast-changing state (example: Carrier-Grade NAT). The proof is left as an <a href="http://www.catb.org/jargon/html/E/exercise--left-as-an.html">exercise for the reader</a>.</p>
<h4>Summary</h4><p>Whenever you’re evaluating a network architecture or reading a vendor whitepaper describing next-generation unicorn-tears-blessed solution, try to identify how much state individual components keep, how it’s created and how often it changes. Hardware devices storing plenty of state tend to be complex and expensive (keep that in mind when evaluating the next application-aware fabric).</p>
<h4>More information</h4><p>Not surprisingly, <a href="http://tools.ietf.org/html/rfc3439">RFC 3429</a> (Some Internet Architectural Guidelines and Philosophy) gives you similar advice, although in way more eloquent form.</p>

