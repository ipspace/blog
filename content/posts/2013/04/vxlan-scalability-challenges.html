---
url: /2013/04/vxlan-scalability-challenges.html
title: "VXLAN scalability challenges"
date: "2013-04-18T07:07:00.000+02:00"
tags: [ VXLAN,Workshop,Overlay networks,virtualization ]
---

<p><a href="https://blog.ipspace.net/2011/08/finally-mac-over-ip-based-vcloud.html">VXLAN</a>, one of the first MAC-over-IP (overlay) virtual networking solutions is definitely a major improvement over traditional VLAN-based virtual networking technologies … but not without its own <a href="https://blog.ipspace.net/2011/12/vxlan-ip-multicast-openflow-and-control.html">scalability limitations</a>.<!--more--></p>
<h4>Implementation issues</h4><p>VXLAN was first implemented in Nexus 1000V, which presents itself as a Virtual Distributed Switch (vDS) to VMware vCenter. A single Nexus 1000V instance cannot have more than 64 VEMs (vSphere kernel modules), limiting the Nexus 1000V domain to 64 hosts (or approximately two racks of UCS blade servers). </p>
<p>It’s definitely possible to configure the same VXLAN NVI and IP multicast address on different Nexus 1000V switches (either manually or using vShield Manager), but you cannot vMotion a VM out of the vDS (that Nexus 1000V presents to vCenter). </p>
<p>VXLAN on Nexus 1000V is thus a great technology if you want to implement HA/DRS clusters spread across multiple racks or rows (you can do it without configuring end-to-end bridging), but falls way short of the “deploy any VM anywhere in the data center” holy grail.</p>
<p>VXLAN is also available in VMware’s vDS switch ... but can only be managed through vShield Manager. vDS can span 500 hosts (the vMotion domain is ~8 times bigger than if you use Nexus 1000V), and supposedly vShield Manager configures VXLAN segments across multiple vDS (using the same VXLAN VNI and IP multicast address on all of them).</p>
<h4>IP multicast scalability issues</h4><p>VXLAN floods layer-2 frames using IP multicast (Cisco has <a href="http://blogs.cisco.com/datacenter/cisco-vxlan-innovations-overcoming-ip-multicast-challenges/">demonstrated unicast-only VXLAN</a> but there’s nothing I could touch on their web site yet), and you can either manually associate an IP multicast address with a VXLAN segment, or let vShield Manager do it automatically (using IP multicast addresses from a single configurable pool).</p>
<p>The number of IP multicast groups (together with the size of the network) obviously influences the overall VXLAN scalability. Here are a few examples:</p>
<p><strong>One or few </strong><strong>multicast group</strong><strong>s</strong><strong> </strong><strong>for a single </strong><strong>Nexus 1000V</strong><strong> instance</strong>. Acceptable if you don’t need more than 64 hosts. Flooding wouldn’t be too bad (not many people would put more than a few thousand VMs on 64 hosts) and the core network would have a reasonably small number of (S/*,G) entries (even with source-based trees the number of entries would be linearly proportional to the number of vSphere hosts).</p>
<p><strong>Many virtual segments in large network with a few multicast groups</strong>. This would make VXLAN as “scalable” as <a href="https://blog.ipspace.net/2011/04/vcloud-director-networking.html">vCDNI</a>. Numerous virtual segments (and consequently numerous virtual machines) would map into a single IP multicast address (vShield Manager uses a simple wrap-around IP multicast address allocation mechanism), and vSphere hosts would receive flooded packets for irrelevant segments.</p>
<p><strong>Use per-VNI multicast group</strong>. This approach would result in minimal excessive flooding but generate large amounts of (S,G) entries in the network.</p>
<p>The size of the multicast routing table would obviously depend on the number of hosts, number of VXLAN segments, and <a href="http://wiki.nil.com/PIM_Sparse_Mode">PIM configuration</a> – do you use shared trees or switch to source tree as soon as possible … and keep in mind that <a href="http://www.cisco.com/en/US/docs/switches/datacenter/sw/verified_scalability/b_Cisco_Nexus_7000_Series_NX-OS_Verified_Scalability_Guide.html#reference_04BA8513CF3140D2A2A6C5E5B4E7C60C">Nexus 7000 doesn’t support more than 32000 multicast entries</a> and Arista’s 7500 <a href="http://www.aristanetworks.com/media/system/pdf/Datasheets/7500_Datasheet.pdf">cannot have more than 4000 multicast routes on a linecard</a>.</p>
<h4>Rules-of-thumb</h4><p>VXLAN has no flooding reduction/suppression mechanisms, so the rules-of-thumb from <a href="http://tools.ietf.org/html/rfc5556#page-9">RFC 5556</a> still apply: a single broadcast domain should have around 1000 end-hosts. In VXLAN terms, that’s around 1000 VMs per IP multicast address.</p>
<p>However, it might be simpler to take another approach: use shared multicast trees (and hope the amount of flooded traffic is negligible ;), and assign anywhere between 75% and 90% of (lowest) IP multicast table size on your data center switches to VXLAN. Due to vShield Manager’s wraparound multicast address allocation policy, the multicast traffic should be well-distributed across all the whole allocated address range.</p>
<h4>More information</h4><p>I’ll be <a href="http://www.interop.com/lasvegas/conference/networking.php?session_id=51">describing overlay virtual networks @ Interop 2013 Las Vegas</a>, so make sure to drop by if you’re attending Interop.</p>
<p>VXLAN is also mentioned in the <a href="http://www.ipspace.net/Introduction_to_Virtualized_Networking">Introduction to Virtual Networking</a> webinar and described in details in the <a href="http://www.ipspace.net/VXLAN_Technical_Deep_Dive">VXLAN Technical Deep Dive</a> webinar. You’ll find some VXLAN use cases in <a href="http://www.ipspace.net/Cloud_Computing_Networking">Cloud Computing Networking</a> webinar. All three webinars are available with the <a href="http://www.ipspace.net/Subscription_to_ioshints_webinars">yearly subscription</a> ... and if you need design help/review or a second opinion, check the <a href="http://www.ipspace.net/ExpertExpress">ExpertExpress</a> service.</p>

