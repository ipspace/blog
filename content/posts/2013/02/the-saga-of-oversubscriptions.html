---
date: 2013-02-14T07:06:00.000+01:00
tags:
- data center
- workshop
- fabric
- FCoE
title: The Saga of Oversubscriptions
url: /2013/02/the-saga-of-oversubscriptions.html
---

<p>Matt Thompson <a href="/2013/02/nexus-6000-and-40ge-why-do-i-care.html?showComment=1360161276836#c6992846004051912358">provided a really good answer</a> to the “<em>what’s acceptable oversubscription ratio in a ToR switch</em>” when he wrote “<em>I’m expecting a ‘</em><em>how long is a piece of string</em><em>’ answer</em>” (note: do watch the <a href="http://www.youtube.com/watch?v=OIdUxlFGvYk">BBC video answering that one</a>).</p>
<p>There’s the 3:1 rule-of-thumb recipe, with a more realistic answer being “<em>it depends</em>”. Now let’s see if we can go beyond that without a deep dive into <a href="http://en.wikipedia.org/wiki/How_many_angels_can_dance_on_the_head_of_a_pin%3F">scholastic waters</a>.<!--more--></p>
<h4>Know Thy Traffic</h4><p>You can provide the best answer to this question by monitoring your data center traffic. Figure out the amount of network and storage traffic, the ratio between outbound (north-south) and internal (east-west) traffic, the amount of overhead traffic (vMotion, data replication and backups could represent a large percentage of your traffic), and do some back-of-napkin math (speaking of napkins, <a href="http://www.amazon.com/gp/product/1591843065/ref=as_li_tf_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1591843065&amp;linkCode=as2&amp;tag=cisioshinandt-20">this book is REALLY good</a>).</p>
<p class="note">Have to mention that this article describes data center environments. Campus networks are totally different; you might easily get away with 100:1 oversubscription, particularly if the users connections run @ 1Gbps. Also, small data centers where majority of the traffic stays within a ToR switch or blade server enclosure could use significantly higher oversubscription ratios.</p>
<p>If you have a larger data center and plenty of budget, consider proper analysis and simulation tools. <a href="http://www.cariden.com">Cariden</a> has a great tool that will definitely fit the bill (they were recently acquired by Cisco but still seem pretty independent). </p>
<p>It’s harder (but still feasible) to use this approach if you’re deploying a totally new architecture, for example migrating from FC to iSCSI, or moving from 1GE to 10GE while radically changing the virtualization ratio. </p>
<p>On the other hand, if you have no reliable network usage statistics, you’re facing a Mission Impossible situation, so here are a few more things to consider.</p>
<h4>Expect the Worst</h4><p>You might have a perfectly designed network that performs flawlessly … until one of the links fails. Link failure reduces the available bandwidth, but might also significantly impair ECMP load balancing algorithm (<a href="http://packetpushers.net/the-scaling-limitations-of-etherchannel-or-why-11-does-not-equal-2/">which is not perfect anyway</a>), regardless of whether it’s based on L2/L3 forwarding tables or port channels. </p>
<p>The dirty details are obviously hardware-specific (check with your vendor <em>and </em>do your own lab tests), but you might lose way more than a quarter of the uplink bandwidth if you lose one of four uplinks.</p>
<p>BTW, <a href="http://www.cariden.com">Cariden</a> tools allow you to simulate link, node and transmission group (bundle of links) failures, iterating over every possible failure (or multiple failures), and documenting worst-case performance.</p>
<h4>The Hogs</h4><p>In many data centers that use network to transfer data generated by backup jobs running on the servers (physical or virtual), backup traffic represents majority of the overall traffic, sometimes overloading the network to the extent that daily backups take more than 24 hours.</p>
<p>If this sounds familiar, remember that you won’t solve the problem by upgrading the weakest link, you’ll just move the problem around. To solve the problem, you have to:</p>
<ul class="ListParagraph"><li>Estimate the minimum backup bandwidth you need from each server or group of servers;</li>
<li>Figure out the paths the backup traffic will take across the network; </li>
<li>Overprovision those paths to ensure the backup traffic doesn’t interfere with the regular use traffic.</li>
</ul>
<p class="note">Obviously there are loads of tricks you can use to alleviate this problem, from deploying a dedicated backup network, to moving backup storage closer to the servers or using virtual disk or storage-based backup solutions (ex: <a href="http://www.veeam.com/">Veeam</a>).</p>
<p>vMotion might be a similar hog, only less predictable. At least we know when the backups start each night, but we never know when a <a href="http://packetpushers.net/wp-content/uploads/2011/06/Clickity-Click-Click.mp3">clickety-click-happy</a> operator decides it’s a good idea to evacuate a rack of servers (because someone told him vMotion Just Works with zero impact to the VM performance).</p>
<p>Oh, and then there are the <a href="http://en.wikipedia.org/wiki/Patch_Tuesday">Happy Tuesdays</a>.</p>
<p><strong>Rule-of-thumb</strong>: You wouldn’t want the hogs to use more than approximately half of your bandwidth. If there’s nothing else you can do, mark the hogs, and keep them in a separate QoS class.</p>
<h4>Know What Matters</h4><p>You might get away with limiting the amount (or percentage) of bandwidth available to the hogs. For example, if backup traffic clogs your network, use QoS tools to give backup a guaranteed but limited amount of bandwidth.</p>
<h4>Converged Storage Networks</h4><p>iSCSI, NFS or FCoE could be another major source of data center traffic, but it’s pretty easy to predict. The storage traffic is limited by the total bandwidth of network adapters on your disk arrays ... unless you’re using peer-to-peer file systems (<a href="http://en.wikipedia.org/wiki/GFS2">GFS</a> comes to mind), in which case all bets are off (<a href="http://aws.amazon.com/message/65648/">just ask Amazon</a>).</p>
<p>If you expect large amounts of storage traffic, it might make sense to build a dedicated storage network – either starting with dedicated server ports and ToR switches (which will make you loved by storage admins and hated by the CFO), or by having shared ToR switches and a separate storage network spine layer with dedicated uplinks.</p>
<p class="note">Have you noticed I just described access-layer FCoE networks with FC ports on ToR switches?</p>
<h4>Actual network traffic</h4><p>Finally we’re in the totally unpredictable waters. The amount of the (production) network traffic depends heavily on your application architecture and the skills of your programmers (ever seen a transaction doing thousands of individual SQL queries because the programmer never got to the JOIN chapter of the SQL manual?). Add “unlimited mobility” and “business agility” to the mix and you have a winner.</p>
<p>There are only a few hints I can give you:</p>
<p><strong>Standalone applications</strong> (example: single-VM <a href="http://en.wikipedia.org/wiki/LAMP_stack">LAMP</a> stacks or IIS/SQL Server Express combos encountered in many web hosting environments) are always limited by the user-facing bandwidth (links at the edge of the data center).</p>
<p><strong>Database traffic</strong> (for simple single-VM applications that use external DB server) is always limited by the NIC bandwidth of the database server(s).</p>
<p>Ideally you have <strong>equidistant bandwidth</strong> between all end points (hint: <a href="http://www.ipspace.net/Clos_fabrics_explained">leaf-and-spine aka Clos fabrics</a>). Not really hard to do unless you plan to have tens of thousands of server ports or your CFO forces you to work with old garbage because he believes data center equipment has 15 year depreciation period.</p>
<h4>Back to rules-of-thumb</h4><p>All things considered, if you don’t have reliable network statistics, 3:1 oversubscription does look reasonable (particularly if you use IP-based storage that is not attached to the ToR switch). I’ll stick with it.</p>
<h4>Thank you!</h4><p><a href="https://twitter.com/ecbanks">Ethan Banks</a>, <a href="https://twitter.com/chrismarget">Chris Marget</a>, <a href="http://www.jeremyfilliben.com">Jeremy Filliben</a> and <a href="https://twitter.com/etherealmind">Greg Ferro</a> provided invaluable feedback and interesting additional details during my writing struggles. Thank you!</p>
<h4>More information</h4><p>If you need ...</p>
<ul class="ListParagraph"><li>An overview of modern data center technologies ➔ <a href="http://www.ipspace.net/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking engineers</a></li>
<li>Technical analysis of data center fabric products from numerous vendors ➔ <a href="http://www.ipspace.net/Data_Center_Fabrics">Data Center Fabric Architectures</a></li>
<li>Data center fabrics with equidistant endpoints ➔ <a href="http://www.ipspace.net/Clos_fabrics_explained">Clos Fabrics Explained</a></li>
<li>All of the above ➔ <a href="http://www.ipspace.net/Subscription">Yearly subscription</a></li>
<li>Design review or second opinion ➔ <a href="http://www.ipspace.net/ExpertExpress">ExpertExpress</a></li>
</ul>

