---
url: /2013/05/multi-vendor-openflow-myth-or-reality.html
title: "Multi-Vendor OpenFlow – Myth or Reality?"
date: "2013-05-21T06:56:00.000+02:00"
tags: [ SDN,OpenFlow ]
---

<p>NEC demonstrated multi-vendor OpenFlow network @ Interop Las Vegas, linking physical switches from Arista, Brocade, Centec, Dell, Extreme, Intel and NEC, and virtual switches in Linux (OVS) and Hyper-V (<a href="https://blog.ipspace.net/2013/01/nec-launched-virtual-openflow-switch.html">PF1000</a>) environments in a leaf-and-spine fabric controlled by ProgrammableFlow controller (watch the video of <a href="http://vimeo.com/65940149">Samrat Ganguly demonstrating the network</a>).</p>
<p>Does that mean we’ve entered the era of multi-vendor OpenFlow networking? Not so fast.<!--more--></p>
<div class="separator"><a href="/2013/05/s1600-OFDemo.png" imageanchor="1"><img border="0" src="/2013/05/s320-OFDemo.png"/><br/>Interop 2013 OpenFlow demo network (source: NEC Corporation of America)</a></div>
<p>You see, building real-life networks with fast feedback loops and fast failure reroutes is hard. It took NEC years to get a stable well-performing implementation, and they had to implement numerous OpenFlow 1.0 extensions to get all the features they needed. For example, they circumvented the <a href="https://blog.ipspace.net/2012/01/fib-update-challenges-in-openflow.html">flow update rate challenges</a> by implementing a very smart architecture effectively equivalent to the <a href="https://blog.ipspace.net/2013/01/edge-and-core-openflow-and-why-mpls-is.html">Edge+Core OpenFlow</a> ideas.</p>
<p>In a NEC-only ProgrammableFlow network, the edge switches (be they PF5240 GE switches or PF1000 virtual switches in Hyper-V environment) do all the hard work, while the core switches do simple path forwarding. Rerouting around a core link failure is thus just a matter of path rerouting, not flow rerouting, reducing the number of entries that have to be rerouted by several orders of magnitude.</p>
<p>In a mixed-vendor environments, ProgrammableFlow controller obviously cannot use all the smarts of the PF5240 switches; it has to fall back to the least common denominator (vanilla OpenFlow 1.0) and install granular flows in every single switch along the path, significantly increasing the time it takes to install new flows after a core link failure.</p>
<p>Will the multi-vendor OpenFlow get any better? It might – OpenFlow 1.3 has enough functionality to implement the Edge+Core design, but of course there aren’t too many OpenFlow 1.3 products out there ... and even the products that have been announced might not have the features ProgrammableFlow controller needs to scale the OpenFlow fabric.</p>
<p>For the moment, the best advice I can give you is “If you want to have a working OpenFlow data center fabric, stick with NEC-only solution.”</p>
<h4>More information</h4><p>If you’re interested in a real-life OpenFlow fabric implementation, you simply must watch the <a href="http://demo.ipspace.net/get/PFlow#Videos">ProgrammableFlow webinar recording</a> – the <a href="http://demo.ipspace.net/get/2.1%20-%20ProgrammableFlow%20Basics.mp4">ProgrammableFlow Basics</a> section has a pretty good description of how path forwarding works.</p>

