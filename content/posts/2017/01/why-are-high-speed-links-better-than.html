---
date: 2017-01-31T08:11:00.000+01:00
tags:
- data center
- fabric
- load balancing
title: Why Are High-Speed Links Better than Port Channels or ECMP
url: /2017/01/why-are-high-speed-links-better-than.html
---

<p>I’m positive I’ve answered this question a dozen times in various blog posts and webinars, but it keeps coming back:</p>
<blockquote class="cite">You always mention that high speed links are always better than parallel low speed links, for example 2 x 40GE is better than 8 x 10GE. What is the rationale behind this?</blockquote>
<p>Here’s the N+1-th answer (hoping I’m being consistent):<!--more--></p>
<p class="info">I'm talking about port channels in this blog post. The exact same reasoning applies to parallel L3 links with ECMP. I'm explaining the difference (or lack thereof) between port channels and ECMP L3 links in <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Designs">leaf-and-spine designs webinar</a>.</p>
<p>It's really hard to <a href="https://blog.ipspace.net/2014/03/per-packet-load-balancing-interferes.html">push a single 20 Gbps TCP session across a bundle of 10Gbps links</a>. Brocade is the only one that has (had?) <a href="http://blog.ipspace.net/2011/04/brocade-vcs-fabric-has-almost-perfect.html">a good answer</a> and as they patented the idea a long while ago I doubt anyone else will go down that same route. Everyone else is limited to 5-tuple load balancing (each TCP/UDP session is pinned to a single physical link) because <a href="http://blog.ipspace.net/2014/03/per-packet-load-balancing-interferes.html">packet reordering causes too much performance loss</a>.</p>
<p>Also, if you get stuck behind an elephant on a 10 GE link it hurts more (latency-wise) than if you're behind that same behemoth on a 40 GE link. </p>
<p class="info"><a href="https://blog.ipspace.net/2015/01/improving-ecmp-load-balancing-with.html">Flowlet-based load balancing</a> (reshuffling idle flows on less-congested links) helps, as do end-system-based solutions like <a href="http://conferences2.sigcomm.org/co-next/2014/CoNEXT_papers/p149.pdf">FlowBender</a>.</p>
<p>Finally, losing a member in a port channel as opposed to a complete higher-speed link results in non-symmetrical forwarding fabric, which might result in unwanted congestions. I described the problem in details in the <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Designs">Leaf-and-Spine Fabric Designs</a> webinar, and the way Arista EOS addresses this challenge with BGP DMZ-Bandwidth attribute in <a href="http://www.ipspace.net/Data_Center_Fabrics_Update">2016 update</a> of <a href="http://www.ipspace.net/Data_Center_Fabrics">Data Center Fabric Architectures</a>.</p>

