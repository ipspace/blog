---
date: 2017-06-07T08:53:00.000+02:00
tags:
- design
- data center
- fabric
title: 'Leaf-and-Spine Fabrics: Implicit or Explicit Complexity?'
url: /2017/06/leaf-and-spine-fabrics-implicit-or.html
---

<p>During Shawn Zandi’s presentation <a href="http://www.ipspace.net/Open_Networking_for_Large-Scale_Networks">describing large-scale leaf-and-spine fabrics</a> I got into an interesting conversation with an attendee that claimed it might be simpler to replace parts of a large fabric with large chassis switches (largest boxes offered by multiple vendors support up to 576 40GE or even 100GE ports).</p>
<p>As always, you have to decide between <em>implicit </em>and <em>explicit </em>complexity.<!--more--></p>
<p class="note">Please note: I don’t claim it’s always better to build your network with 1RU switches instead of using chassis switches. However, you should know what you’re doing (beyond the level of vendor whitepapers) and understand the true implications of your decisions.</p>
<h4>Explicit complexity</h4><p>This is the fabric Shawn described in his presentation. The inner leaf-and-spine fabrics (blue, orange, green, yellow) contain dozens of switches interconnected with fiber cables.</p>
<div class="separator"><a href="/2017/06/s1600-Large+Leaf-and-Spine+Fabric.jpg" imageanchor="1"><img border="0" data-original-height="425" data-original-width="1058" src="/2017/06/s550-Large+Leaf-and-Spine+Fabric.jpg"/></a></div>
<h4>Implicit complexity</h4><p>Imagine replacing each of the inner leaf-and-spine fabrics with a large chassis switch. Fewer management points, fewer explicit interconnects (transceivers and cables are replaced with intra-chassis connections). Sounds like a great idea.</p>
<h4><em>It depends </em>strikes again</h4><p>Shawn addressed a few of the challenges of using chassis switches during the presentation:</p>
<ul class="ListParagraph"><li>You’ll manage hundreds of switches anyway. If you can’t do that automatically, it will turn into a nightmare no matter what. However, once you manage to automate the data center fabric, it doesn’t matter how many switches you have in the core.</li>
<li>Large chassis switches are <em>precious</em>, and you don’t want them to fail. Ever. Welcome to the morass of ISSU, NSF, graceful restart… Upgrading an individual component of a large fabric is way easier, and all you need is fast failure detection and reasonably fast converging routing protocol. I <a href="https://blog.ipspace.net/2015/06/so-you-need-issu-on-your-tor-switch.html">wrote about this dilemma two years ago</a>, and <a href="http://blog.ipspace.net/2014/04/should-we-use-redundant-supervisors.html">three years ago</a>, but of course nobody ever listens to those arguments ;)</li>
<li>Restarting a 1RU switch with a single ASIC is way faster than restarting a complex distributed system with two supervisors, and two dozen linecards and fabric modules (Nexus 7700 anyone?). There’s a rumor a large chassis switch can bring down a cloud provider when restarted at the wrong time ;)</li>
</ul>
<p class="note">We didn’t even consider pricing – that’s left as an exercise for the reader.</p>
<p>I also asked around and got these points from other engineers with operational experience running very large data center fabrics:</p>
<ul class="ListParagraph"><li>Make failure domain (blast radius) of any failure be as small as possible. Large chassis switches are a large failure domain that can take down a significant amount of data center bandwidth.</li>
<li>Scalability of control plane: the ratio of CPU cycles to port bandwidth is much higher in 1RU switches than in large chassis switches with supervisor modules (remember the limited number of spanning tree instances on Nexus 7000?) Worst case, consider the world of containers where containers can come and go rapidly, and there can be hundreds of them per rack. </li>
<li>Simple fixed-form switches fail in relatively simple ways. Redundant architectures (dual supervisors in fail-over scenario) can fail in arcane ways. Also, <a href="https://blog.ipspace.net/2017/01/never-take-two-chronometers-to-sea.html">two is the worst possible number</a> when it comes to voting… but I guess it would be impossible to persuade the customers to buy three supervisor modules per chassis.</li>
<li>Ask anyone who's tried to debug a non-working chassis. The protocols they run internally are proprietary, frame formats are equally so, making a hard task even harder.</li>
<li>With single fixed-form factor switches, you can easily carry cost-effective spares. When things fail, you can replace them quickly with spares and troubleshoot the failing system off production network. Harder to do with expensive chassis switches.</li>
<li>What if you decide to run custom apps on data center switches (natively on Arista EOS or Cumulus Linux, in containers on Nexus-OS)? App developers and others are more familiar with building distributed apps than with writing something that has to work with each vendor's ISSU model. Compute got rid of ISSU a long time ago.</li>
</ul>
<p>Anything else? Please write a comment.</p>
<p>Obviously, the leaf-and-spine wiring remains a mess. No wonder Facebook decided to build a chassis switch… but did you notice that they <a href="https://blog.ipspace.net/2017/02/facebook-backpack-behind-scenes.html">configure and manage every linecard and fabric module separately</a>? Their switch behaves exactly the same way as Shawn’s leaf-and-spine fabric with more stable (and cheaper) wiring.</p>
<p>Want to know more? The update session of the <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Architectures">Leaf-and-Spine Fabrics webinar</a> on June 13<span style="vertical-align: super; font-size: 80%;">th</span> will focus on basic design aspects, sample high-level designs (we covered routing and switching design details last year), and multi-stage fabrics.</p>

