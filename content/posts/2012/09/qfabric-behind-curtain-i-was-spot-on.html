---
date: 2012-09-04T07:23:00.000+02:00
tags:
- switching
- data center
- workshop
- fabric
title: 'QFabric Behind the Curtain: I was spot-on'
url: /2012/09/qfabric-behind-curtain-i-was-spot-on.html
---

<p>A few days ago <a href="http://www.network-janitor.net/">Kurt Bales</a> and <a href="http://cooperlees.com/blog/?p=525">Cooper Lees</a> gave me access to a test QFabric environment. I always wanted to know what was really going on behind the QFabric curtain and the moment Kurt mentioned he was able to see some of those details, I was totally hooked.</p>
<p><strong>Short summary:</strong> <a href="https://blog.ipspace.net/2011/06/speculation-this-is-how-i-would-build.html">QFabric works exactly as I’d predicted</a> three months before the user-facing documentation became publicly available (the behind-the-scenes view described in this blog post is probably still hard to find).<!--more--></p>
<p class="warn">This post is by no means a critique of QFabric. If anything, I’m delighted there’s still a networking vendor that can create innovative solutions without unicorn tears, relying instead on field-tested technologies ... which might, among other things, make the solution more stable.</p>
<h4>It looks like a giant switch</h4><p>When you log into the QFabric management IP address (VIP), it looks exactly like a giant switch – single configuration, single set of interfaces, show commands etc. All the familiar Junos configuration components are there: system group, interfaces, VLANs and protocols. The only really new component is the <strong>fabric </strong>object with <strong>node-group </strong>definitions (<a href="https://blog.ipspace.net/2011/09/qfabric-part-2-control-plane-overview.html">more on QFabric node groups</a>).</p>
<p>However, every giant switch needs troubleshooting, which usually requires access to individual components; in QFabric case, the <strong>request component login </strong>command that unveils the <a href="http://www.youtube.com/watch?v=YWyCCJ6B2WE">really interesting world behind the curtain</a>.</p>
<pre class="code">ip@test&gt; request component login ?<br/>Possible completions:<br/>  &lt;node-name&gt;          Inventory name for the remote node<br/>  DRE-0                Diagnostic routing engine<br/>  IC-Left/RE0          Interconnect device control board<br/>  IC-Left/RE1          Interconnect device control board<br/>  IC-Right/RE0         Interconnect device control board<br/>  IC-Right/RE1         Interconnect device control board<br/>  FC-0                 Fabric control<br/>  FC-1                 Fabric control<br/>  FM-0                 Fabric manager<br/>  NW-NG-0              Node group<br/>  R2-19-Node0          Node device<br/>  R2-19-Node1          Node device<br/>  R2-7-Node4           Node device<br/>  R2-7-Node5           Node device<br/>  R3-12-Node6          Node device<br/>  R3-12-Node7          Node device<br/>  R3-19-Node2          Node device<br/>  R3-19-Node3          Node device<br/>  RSNG01               Node group<br/>  RSNG02               Node group</pre><p class="info">The names of physical entities (QF/Nodes, QF/Interconnects) could be either their serial numbers (default) or user-configurable names (recommended).</p>
<p>As you can see, you can login to individual physical devices, node groups, and virtual components like fabric controls and fabric manager. These virtual components run on QF/Directors – CentOS boxes running KVM (you can log into the QF/Director Linux shell and see the virtual machines with <strong>ps -elf</strong>).</p>
<p>Each QF/Director is running a number of common services, including database (MySQL), DHCP, FTP, NTP, SSH, GFS, DLM (distributed lock manager), NFS and Syslog servers:</p>
<pre class="code">ip@QFabric&gt; show fabric administration inventory director-group status <br/>Director Group Status Sat Aug 25 09:52:08 PDT 2012<br/><br/> Member Status Role     Mgmt Address    CPU Free Memory VMs Up Time<br/> ------ ------ -------- --------------- --- ----------- --- -------------<br/> dg0    online master   xxxxxxxxxxxx    10% 17642780k   4   3 days, 16:23 hrs<br/> dg1    online backup   xxxxxxxxxxxx    6%  20509268k   3   3 days, 16:13 hrs<br/><br/> Member Device Id/Alias  Status  Role<br/> ------ ---------------- ------- ---------<br/> dg0    xxxxxxxxxxxxxxxx online  master   <br/><br/>  Master Services<br/>  ---------------<br/>  Database Server                online    <br/>  Load Balancer Director         online    <br/>  QFabric Partition Address      online    <br/><br/>  Director Group Managed Services<br/>  -------------------------------<br/>  Shared File System             online    <br/>  Network File System            online    <br/>  Virtual Machine Server         online    <br/>  Load Balancer/DHCP             online    <br/><br/>  Hard Drive Status<br/>  ----------------<br/>  Volume ID:4                    optimal    <br/>  Physical ID:1                  online     <br/>  Physical ID:0                  online     <br/>  SCSI ID:1                      100%       <br/>  SCSI ID:0                      100%       <br/><br/>  Size  Used Avail Used% Mounted on <br/>  ----  ---- ----- ----- ----------<br/>  423G  6.3G 395G  2%   /          <br/>  99M   20M  75M   21%  /boot      <br/>  93G   2.0G 91G   3%   /pbdata    <br/><br/>  Director Group Processes<br/>  ------------------------<br/>  Director Group Manager         online    <br/>  Partition Manager              online    <br/>  Software Mirroring             online    <br/>  Shared File System master      online    <br/>  Secure Shell Process           online    <br/>  Network File System            online    <br/>  DHCP Server master             online     master                           <br/>  FTP Server                     online    <br/>  Syslog                         online    <br/>  Distributed Management         online    <br/>  SNMP Trap Forwarder            online    <br/>  SNMP Process                   online    <br/>  Platform Management            online    <br/>[... rest deleted ...]</pre><h4>Lo and behold – it’s actually running BGP internally</h4><p>After logging into one of the fabric control virtual machines, you can execute the <strong>show bgp fabric summary </strong>command, which clearly indicates the control-plane protocol behind the scenes is multi-protocol BGP running numerous address families. Each fabric control VM runs BGP with all server or network nodes (not individual QF/Nodes) and with all QF/Interconnects.</p>
<pre class="code">qfabric-admin@FC-0&gt; show bgp summary fabric | no-more <br/>Groups: 2 Peers: 6 Down peers: 0<br/>Unconfigured peers: 5<br/>Table          Tot Paths  Act Paths Suppressed    History Damp State    Pending<br/>bgp.l3vpn.0          <br/>                      42         18          0          0          0          0<br/>Peer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...<br/>128.0.128.4             100      10517      10602       0       0  3d 6:43:58 Establ<br/>  bgp.l3vpn.0: 17/17/17/0<br/>  bgp.rtarget.0: 28/31/31/0<br/>  bgp.fabricvpn.0: 28/28/28/0<br/>  bgp.bridgevpn.0: 8/8/8/0<br/>  default.inet.0: 17/17/17/0<br/>  default.fabric.0: 19/19/19/0<br/>128.0.128.8             100      10594      10593       0       0  3d 6:44:06 Establ<br/>  bgp.l3vpn.0: 0/18/18/0<br/>  bgp.rtarget.0: 1/32/32/0<br/>  bgp.fabricvpn.0: 0/103/103/0<br/>  bgp.bridgevpn.0: 0/9/9/0<br/>  default.inet.0: 0/18/18/0<br/>  default.fabric.0: 0/91/91/0<br/>128.0.130.4             100      10466      10552       0       0  3d 6:35:42 Establ<br/>  bgp.rtarget.0: 0/4/4/0<br/>  bgp.fabricvpn.0: 34/34/34/0<br/>  bgp.bridgevpn.0: 0/0/0/0<br/>  default.fabric.0: 34/34/34/0<br/>128.0.130.10            100       9751       9636       0       0  3d 1:04:34 Establ<br/>  bgp.rtarget.0: 0/4/4/0<br/>  bgp.fabricvpn.0: 34/34/34/0<br/>  bgp.bridgevpn.0: 0/0/0/0<br/>  default.fabric.0: 34/34/34/0<br/>128.0.130.24            100      10432      10547       0       0  3d 6:18:09 Establ<br/>  bgp.l3vpn.0: 1/7/7/0<br/>  bgp.rtarget.0: 0/7/7/0<br/>  bgp.fabricvpn.0: 7/7/7/0<br/>  bgp.bridgevpn.0: 1/1/1/0<br/>  default.inet.0: 1/7/7/0<br/>  default.fabric.0: 4/4/4/0<br/>128.0.130.26            100      10410      10545       0       0  3d 6:19:11 Establ<br/>  bgp.l3vpn.0: 0/0/0/0<br/>  bgp.rtarget.0: 0/4/4/0<br/>  bgp.fabricvpn.0: 0/0/0/0<br/>  bgp.bridgevpn.0: 0/0/0/0</pre><p>Any other node (example: QF/Interconnect), has two BGP sessions with both fabric control VMs:</p>
<pre class="code">qfabric-admin@IC-Left&gt; show bgp summary fabric <br/>Groups: 1 Peers: 2 Down peers: 0<br/>Peer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...<br/>128.0.128.6             100       9663       9775       0       0  3d 1:16:27 Establ<br/>  bgp.rtarget.0: 28/32/32/0<br/>  bgp.fabricvpn.0: 61/61/61/0<br/>  bgp.bridgevpn.0: 0/0/0/0<br/>  default.fabric.0: 61/61/61/0<br/>128.0.128.8             100       9667       9773       0       0  3d 1:16:23 Establ<br/>  bgp.rtarget.0: 0/32/32/0<br/>  bgp.fabricvpn.0: 0/61/61/0<br/>  bgp.bridgevpn.0: 0/0/0/0<br/>  default.fabric.0: 0/61/61/0</pre><p class="info">Edge nodes use six MP-BGP address families (including default.inet.0 and default.fabric.0), QF/Interconnects have just four.</p>
<p>The fabric control VMs act as BGP route reflectors (<a href="https://blog.ipspace.net/2011/06/speculation-this-is-how-i-would-build.html">exactly as I predicted</a>). You can easily verify that by inspecting any individual BGP entry on one of the node groups – you’ll see the <em>Originator </em>and <em>Cluster List </em>BGP attributes:</p>
<pre class="code">65534:1:192.168.13.37/32 (2 entries, 1 announced)<br/>        *BGP    Preference: 170/-101<br/>                Route Distinguisher: 65534:1<br/>                Next hop type: Indirect<br/>                Address: 0x964f49c<br/>                Next-hop reference count: 6<br/>                Source: 128.0.128.6<br/>                Next hop type: Router, Next hop index: 131070<br/>                Next hop: 128.0.130.24 via dcfabric.0, selected<br/>                Label operation: PFE Id 7 Port Id 55<br/>                Label TTL action: PFE Id 7 Port Id 55<br/>                Session Id: 0x0<br/>                Next hop: 128.0.130.24 via dcfabric.0<br/>                Label operation: PFE Id 8 Port Id 55<br/>                Label TTL action: PFE Id 8 Port Id 55<br/>                Session Id: 0x0<br/>                Protocol next hop: 128.0.130.24:49160(NE_PORT)<br/>                Layer 3 Fabric Label 5<br/>                Composite next hop: 964f440 1738 INH Session ID: 0x0<br/>                Indirect next hop: 92c8d00 131072 INH Session ID: 0x0<br/>                State: &lt;Active Int Ext&gt;<br/>                Local AS:   100 Peer AS:   100<br/>                Age: 3d 6:54:40 Metric2: 0 <br/>                Validation State: unverified <br/>                Task: BGP_100.128.0.128.6+33035<br/>                Announcement bits (1): 0-Resolve tree 1 <br/>                AS path: I (Originator) Cluster list:  0.0.0.1<br/>                AS path:  Originator ID: 128.0.130.24<br/>                Communities: target:65534:117440513(L3:1)<br/>                Import Accepted<br/>                Timestamp: 0x116<br/>                Route flags: arp<br/>                Route type: Host<br/>                Route protocol : arp<br/>                L2domain : 5<br/>                SNPA count: 1, SNPA length: 8<br/>                SNPA Type: Network Element Port SNPA<br/>                NE Port ID: 49160<br/>                Localpref: 100<br/>                Router ID: 128.0.128.6<br/>                Secondary Tables: default.inet.0<br/>                Composite next hops: 1<br/>                        Protocol next hop: 128.0.130.24:49160(NE_PORT)<br/>                        Layer 3 Fabric Label 5<br/>                        Composite next hop: 964f440 1738 INH Session ID: 0x0<br/>                        Indirect next hop: 92c8d00 131072 INH Session ID: 0x0<br/>                        Indirect path forwarding next hops: 2<br/>                                Next hop type: Router<br/>                                Next hop: 128.0.130.24 via dcfabric.0<br/>                                Session Id: 0x0<br/>                                Next hop: 128.0.130.24 via dcfabric.0<br/>                                Session Id: 0x0</pre><h4>Addressing</h4><p>QFabric control plane uses locally-administered MAC addresses and IP address block 128.0.0.0/16. You can see all the MAC and IP addresses with the <strong>show arp </strong>command executed on any of the internal components. The <strong>bme </strong>interfaces are the control-plane interfaces, the <strong>vlan </strong>interface is a user-facing SVI interface.</p>
<pre class="code">qfabric-admin@NW-NG-0&gt; show arp <br/>MAC Address       Address     Name              Interface           Flags<br/>00:13:dc:ff:72:01 10.73.2.9   10.73.2.9         vlan.501            none<br/>02:00:00:00:40:01 128.0.0.1   128.0.0.1         bme0.2              permanent<br/>02:00:00:00:40:02 128.0.0.2   128.0.0.2         bme0.2              permanent<br/>02:00:00:00:40:05 128.0.0.4   128.0.0.4         bme0.0              permanent<br/>02:00:00:00:40:05 128.0.0.5   128.0.0.5         bme0.1              permanent<br/>02:00:00:00:40:05 128.0.0.5   128.0.0.5         bme0.2              permanent<br/>02:00:00:00:40:05 128.0.0.6   128.0.0.6         bme0.0              permanent<br/>02:00:00:00:40:07 128.0.0.7   128.0.0.7         bme0.1              permanent<br/>02:00:00:00:40:07 128.0.0.7   128.0.0.7         bme0.2              permanent<br/>02:00:00:00:40:08 128.0.0.8   128.0.0.8         bme0.1              permanent<br/>02:00:00:00:40:08 128.0.0.8   128.0.0.8         bme0.2              permanent<br/>02:00:00:00:40:09 128.0.0.9   128.0.0.9         bme0.1              permanent<br/>02:00:00:00:40:09 128.0.0.9   128.0.0.9         bme0.2              permanent<br/>[... rest deleted ...]</pre><h4>Look Ma! There are the labels!</h4><p>In <a href="https://blog.ipspace.net/2011/06/speculation-this-is-how-i-would-build.html">my blog post</a> I predicted QFabric uses MPLS internally. It’s impossible to figure out without a 40Gbps sniffer whether MPLS label stack is the exact encapsulation format QFabric is using, but it sure looks like MPLS from the outside. </p>
<p>The <strong>dcfabric </strong>interface uses <strong>mpls </strong>as one of the protocols:</p>
<pre class="code">qfabric-admin@RSNG01&gt; show interfaces dcfabric.0 <br/>  Logical interface dcfabric.0 (Index 64) (SNMP ifIndex 1214251262) <br/>    Flags: SNMP-Traps Encapsulation: ENET2<br/>    Input packets : 0 <br/>    Output packets: 0<br/>    Protocol inet, MTU: 1558<br/>      Flags: Is-Primary<br/>    Protocol mpls, MTU: 1546, Maximum labels: 3<br/>      Flags: Is-Primary<br/>    Protocol eth-switch, MTU: 0<br/>      Flags: Is-Primary</pre><p>You can also see MPLS-like labels in numerous BGP entries, for example in the <em>bridgevpn </em>address family ...</p>
<pre class="code">65534:1:5.c8:e2:c3:01:78:8f/144               <br/>         *[BGP/170] 1w3d 15:28:00, localpref 100<br/>            AS path: I, validation-state: unverified<br/>            to 128.0.128.4 via dcfabric.0, Push 1730, Push 1, Push 55(top)<br/>          &gt; to 128.0.128.4 via dcfabric.0, Push 1730, Push 2, Push 55(top)<br/>          [BGP/170] 1w3d 15:28:00, localpref 100, from 128.0.128.8<br/>            AS path: I, validation-state: unverified<br/>            to 128.0.128.4 via dcfabric.0, Push 1730, Push 1, Push 55(top)<br/>          &gt; to 128.0.128.4 via dcfabric.0, Push 1730, Push 2, Push 55(top)</pre><p>The same set of three labels appears in a host route pointing to a host connected to another QF/Node:</p>
<pre class="code">65534:1:10.73.2.9/32                <br/>           *[BGP/170] 3d 12:32:09, localpref 100<br/>              AS path: I, validation-state: unverified<br/>            &gt; to 128.0.128.4 via dcfabric.0, Push 5, Push 1, Push 23(top)<br/>            [BGP/170] 3d 12:32:09, localpref 100, from 128.0.128.8<br/>              AS path: I, validation-state: unverified<br/>            &gt; to 128.0.128.4 via dcfabric.0, Push 5, Push 1, Push 23(top)</pre><p>IP prefixes directly connected to the QFabric have just one label – probably a pointer to an IP forwarding table entry.</p>
<pre class="code">65534:1:10.73.2.0/29                <br/>           *[BGP/170] 3d 12:31:59, localpref 101, from 128.0.128.4<br/>              AS path: I, validation-state: unverified<br/>            &gt; to 128.0.128.4:129(NE_PORT), Layer 3 Fabric Label 5<br/>            [BGP/170] 3d 12:31:59, localpref 101, from 128.0.128.8<br/>              AS path: I, validation-state: unverified<br/>            &gt; to 128.0.128.4:129(NE_PORT), Layer 3 Fabric Label 5 </pre><p>On the other hand, the MPLS routing and forwarding tables are empty, indicating that this is very probably not the MPLS we’re used to.</p>
<h4>Summary</h4><p>Behind the scenes, QFabric runs like any well-designed service provider network: a cluster of central servers provides common services (including DHCP, NFS, FTP, NTP and Syslog), BGP is used in the control plane to distribute customer prefixes (IP addresses, host/ARP routes, MAC addresses) and MPLS-like encapsulation that can attach a label stack to a L2 frame or L3 datagram is used in the forwarding plane. </p>
<p>The true magic of QFabric is the CLI VM, which presents the internal IP+MPLS-like network as a single switch without any OpenFlow or SDN magic. Wouldn’t it be nice to have something similar in the service provider networks?</p>
<p class="update">2012-12-17: Comments are temporarily disabled, as a moron selling acne-reducing snake oil found this blog post interesting. Contact me using the 'Contact' link at the top of the page.</p>

