---
url: /2012/02/forwarding-state-abstraction-with.html
title: "Forwarding State Abstraction with Tunneling and Labeling"
date: "2012-02-01T06:56:00.000+01:00"
tags: [ MPLS,switching,Workshop,TRILL,virtualization ]
---

<p>Yesterday I described how the <a href="https://blog.ipspace.net/2012/01/fib-update-challenges-in-openflow.html">limited flow setup rates offered by most commercially-available switches</a> force the developers of <a href="http://www.necam.com/pflow/">production-grade OpenFlow controllers</a> to <a href="http://www.cmlab.csie.ntu.edu.tw/~kenneth/qing2011/paper/6.pdf">drop the microflow ideas</a> and focus on state abstraction (<a href="https://blog.ipspace.net/2011/10/openflow-and-state-explosion.html">people living in a dreamland usually go in a totally opposite direction</a>). Before going into OpenFlow-specific details, let’s review the existing forwarding state abstraction technologies.<!--more--></p>
<h4>A mostly theoretical detour</h4><p>Most forwarding state abstraction solutions that I’m aware of (and I’m positive Petr Lapukhov will give me tons of useful pointers to a completely different universe) use a variant of <em>Forwarding Equivalence Class </em>(FEC) concept from MPLS:</p>
<ul class="ListParagraph"><li>All the traffic that expects the same forwarding behavior gets the same label; </li>
<li>The intermediate nodes no longer have to inspect the individual packet/frame headers; they forward the traffic solely based on the FEC indicated by the label.</li>
</ul>
<p>The grouping/labeling operation thus <a href="https://blog.ipspace.net/2012/01/bgp-free-service-provider-core-in.html">greatly reduces the forwarding state in the core nodes</a> (you can call them P-routers, backbone bridges, or whatever other terminology you prefer) and improves the core network convergence due to significantly reduced number of forwarding entries in the core nodes.</p>
<div class="separator" style="clear: both; text-align: center; margin: 2em auto;"><a href="/2012/02/s1600-MPLS_Forwarding.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="147" src="/2012/02/s400-MPLS_Forwarding.png" width="400"/></a><br/>MPLS forwarding diagram from the <a href="http://www.ipspace.net/Enterprise_MPLS_VPN_Deployment">Enterprise MPLS/VPN Deployment webinar</a></div>
<p class="info">The core network convergence is improved due to <em>reduced state </em>not due to <em>pre-computed alternate paths</em> that <a href="https://blog.ipspace.net/2012/01/prefix-independent-convergence-pic.html">Prefix-Independent Convergence</a> or MPLS Fast Reroute uses.</p>
<h4>From Theory to Practice</h4><p>There are two well-known techniques you can use to transport traffic grouped in a FEC across the network core: <a href="https://blog.ipspace.net/2011/10/mpls-is-not-tunneling.html">tunneling and virtual circuits</a> (or <em>Label Switched Paths </em>if you want to use non-ITU terminology).</p>
<p>When you use <strong><em>tunneling</em></strong>, the FEC is the tunnel endpoint – all traffic going to the same tunnel egress node uses the same tunnel destination address. </p>
<p>All sorts of tunneling mechanisms have been proposed to scale layer-2 broadcast domains and virtualized networks (IP-based layer-3 networks scale way better by design): </p>
<ul class="ListParagraph"><li><a href="http://en.wikipedia.org/wiki/IEEE_802.1ah-2008">Provider Backbone Bridges</a> (PBB – 802.1ah), <a href="http://en.wikipedia.org/wiki/Shortest_Path_Bridging#Shortest_Path_Bridging-MAC_-_SPBM">Shortest Path Bridging-MAC</a> (SPBM – 802.1aq) and <a href="https://blog.ipspace.net/2011/04/vcloud-director-networking.html">vCDNI</a> use MAC-in-MAC tunneling – the destination MAC address used to forward user traffic across the network core is the egress bridge or the destination physical server (for vCDNI).</li>
</ul>
<div class="separator" style="clear: both; text-align: center; margin: 2em auto;"><a href="/2012/02/s1600-SPBM_Forwarding.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="155" src="/2012/02/s400-SPBM_Forwarding.png" width="400"/></a><br/>SPBM forwarding diagram from the <a href="http://www.ipspace.net/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking Engineers</a> webinar</div>
<ul class="ListParagraph"><li><a href="https://blog.ipspace.net/2011/08/finally-mac-over-ip-based-vcloud.html">VXLAN</a>, <a href="https://blog.ipspace.net/2011/10/vxlan-and-nvgre-more-information.html">NVGRE</a> and <a href="https://blog.ipspace.net/2011/10/what-is-nicira-really-up-to.html">GRE (used by Open vSwitch)</a> use MAC-over-IP tunneling, which scales way better than MAC-over-MAC tunneling because the core switches can do another layer of state abstraction (subnet-based forwarding and IP prefix aggregation).</li>
</ul>
<div class="separator" style="clear: both; text-align: center; margin: 2em auto;"><a href="/2012/02/s1600-vXLAN-Typical-Architecture.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="128" src="/2012/02/s400-vXLAN-Typical-Architecture.png" width="400"/></a><br/>Typical VXLAN architecture - from the <a href="http://www.ipspace.net/Introduction_to_Virtualized_Networking">Introduction to Virtual Networking</a> webinar</div>
<ul class="ListParagraph"><li><a href="http://en.wikipedia.org/wiki/TRILL">TRILL</a> is closer to VXLAN/NVGRE than to SPB/vCDNI as it <a href="https://blog.ipspace.net/2010/08/trill-and-8021aq-are-like-apples-and.html">uses full L3 tunneling between TRILL endpoints</a> with L3 forwarding inside RBridges and L2 forwarding between RBridges.</li>
</ul>
<div class="separator" style="clear: both; text-align: center; margin: 2em auto;"><a href="/2012/02/s1600-TRILL_Forwarding.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="168" src="/2012/02/s400-TRILL_Forwarding.png" width="400"/></a><br/>TRILL forwarding diagram from the <a href="http://www.ipspace.net/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking Engineers</a> webinar</div>
<p>With <strong><em>tagging</em></strong><strong> </strong>or <strong><em>labeling</em></strong> a short tag is attached in front of the data (ATM VPI/VCI, MPLS label stack on point-to-point links) or somewhere in the header (VLAN tags) instead of encapsulating the user’s data into a full L2/L3 header. The core network devices perform packet/frame forwarding based exclusively on the tags. That’s how <a href="http://en.wikipedia.org/wiki/Shortest_Path_Bridging#Shortest_Path_Bridging-VID_-_SPBV">SPBV</a>, MPLS and ATM work.</p>
<div class="separator" style="clear: both; text-align: center; margin: 2em auto;"><a href="/2012/02/s1600-MPLS_Frame_Format.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="17" src="/2012/02/s400-MPLS_Frame_Format.png" width="400"/></a><br/>MPLS-over-Ethernet frame format from the <a href="http://www.ipspace.net/Enterprise_MPLS_VPN_Deployment">Enterprise MPLS/VPN Deployment webinar</a></div>
<p class="info">MPLS-over-Ethernet commonly used in today’s high-speed networks is an abomination as it uses both L2 tunneling between adjacent LSRs and labeling ... but that’s what you get when you have to reuse existing hardware to support new technologies.</p>
<h4>Next steps</h4><p>Tomorrow we’ll focus on how you could use these techniques in the very limited world of OpenFlow 1.0.</p>

