---
date: 2012-12-06T07:29:00.000+01:00
tags:
- data center
- workshop
- fabric
title: Who the **** needs 16 uplinks? Welcome to 10GE world!
url: /2012/12/who-needs-16-uplinks-welcome-to-10ge.html
---

<p>Will made an <a href="https://blog.ipspace.net/2012/11/stackable-data-center-switches-do-math.html?showComment=1354193971527#c8943305862881555440">interesting comment</a> to my <a href="https://blog.ipspace.net/2012/11/stackable-data-center-switches-do-math.html"><em>Stackable Data Center Switches</em></a> article: “Who the heck has 16 uplinks?” Most of us do in the brave new 10GE world.<!--more--></p>
<h4>A bit of a background</h4><p>Most data centers have a hierarchical design, with top-of-rack (or leaf) switches connected via a set of uplinks to aggregation (or core or spine) switches, and the performance of your data center network depends heavily on the <em>oversubscription ratio </em>– the ratio between server-facing bandwidth and uplink bandwidth (assuming most server traffic traverses the uplinks).</p>
<p class="note">Alternatives include <a href="https://blog.ipspace.net/2012/04/full-mesh-is-worst-possible-fabric.html">full mesh design</a>, <a href="https://blog.ipspace.net/2012/04/monkey-design-still-doesnt-work-well.html">monkey-see-monkey-do design</a>, and <a href="http://www.plexxi.com/">novel approaches I can’t discuss yet</a>.</p>
<h4>Going from GE to 10GE</h4><p>Most ToR switches we were using to connect Gigabit Ethernet server NICs to the network had 10GE uplinks, and the oversubscription ratios were reasonably low, ranging from 1:1.2 (various Nexus 2000 fabric extenders) to 1:2.4 (Juniper EX4200, HP 5830-48). </p>
<p>Some 10GE ToR switches have only 10GE ports (Brocade VDX 67xx, Cisco Nexus 5500, Juniper EX4500, HP 5920), and the <a href="http://etherealmind.com/merchant-silicon-vendor-software-rise-lost-opportunity/">Trident-based ones</a> have a mixture of 10GE and 40GE ports (and you can use 40GE ports as 4x10GE ports with a breakout cable). </p>
<p>To maintain a reasonable oversubscription ratio, you have to use a quarter of the switch ports as uplinks (resulting in a 1:3 oversubscription) – sixteen 10GE ports in a 64-port 10GE switch or four 40GE ports in an equivalent 10/40GE switch. Regardless of the switch model you use, the number of fiber strands you need remains the same; <a href="http://etherealmind.com/notes-physical-connectors-40-100-gigabit-ethernet/">40GE link needs four fibers pairs</a>.</p>
<p><strong>Conclusion:</strong> if you want to have 1:3 oversubscription ratio, you need 16 fiber pairs to connect a 64-port 10GE ToR switch (or 48x10GE+4x40GE switch or a 16-port 40GE switch) to the network core.</p>
<h4>Higher oversubscription ratios?</h4><p>Do you really have to keep the oversubscription ratio low? Is 1:3 a good number? How about 1:7? As always, the answer is “it depends.” You have to know your traffic profile, workload characteristics, and plan for the foreseeable future. </p>
<p>Don’t forget that you can easily fit ~130 <a href="http://aws.amazon.com/ec2/instance-types/">M1 Medium EC2 instances</a> in a single physical server with 512GB of RAM. Assuming the server has two 10GE uplinks and you use 1:3 oversubscription ratio, that’s 51 Mbps per instance (ignoring storage and vMotion traffic). Is that good enough? You tell me.</p>
<h4>More information</h4><p>You’ll find numerous fabric designs guidelines in the <a href="http://www.ipspace.net/Clos_fabrics_explained">Clos Fabrics Explained</a> webinar. Port densities and fabric behavior of almost all data center switches available from nine major vendors are described in the <a href="http://www.ipspace.net/Data_Center_Fabrics">Data Center Fabrics</a> webinar. </p>
<p>Both webinars are available as part of the <a href="http://www.ipspace.net/Subscription">yearly subscription</a> and you can <a href="http://www.ipspace.net/Consulting">always ask me for a second opinion or a design review</a>.</p>

