---
date: 2012-06-21T08:03:00.000+02:00
tags:
- data center
- workshop
- TRILL
title: Does TRILL make sense at all?
url: /2012/06/does-trill-make-sense-at-all.html
---

<p>It’s clear that <a href="/2011/08/imagine-ruckus-when-hypervisor-vendors.html">major hypervisor vendors consider MAC-over-IP to be the endgame for virtual networking</a>; they’re still squabbling about the best technology and proper positioning of bits in various headers, but the big picture is crystal-clear. Once they get there (solving “a few” not-so-trivial problems on the way), and persuade everyone to use <a href="/2011/04/virtual-network-appliances-benefits-and.html">virtual appliances</a>, the network will have to provide seamless IP transport, nothing more. </p>
<p>At that moment, large-scale bridging will finally become a history (until the <a href="/2009/12/lies-damned-lies-and-product-marketing.html">big layer pendulum swings again</a>) and one has to wonder whether there’s any data center future for TRILL, SPB, FabricPath and other vendor-specific derivatives.<!--more--></p>
<h4>How large is Large-Scale anyway?</h4><p>Most customers want to have large-scale bridging solutions to support VM mobility. The current state of the hypervisor market (vSphere 5, June 2012) is as follows:</p>
<ul class="ListParagraph"><li>32 hypervisor hosts in a high-availability cluster (where the VMs move automatically based on load changes);</li>
<li>350 hosts in a virtual distributed switch (vDS); you cannot move a running VM between two virtual distributed switches, and having hundreds of vSphere hosts with the classic vSwitch is a management nightmare.</li>
</ul>
<p>The maximum reasonable size of a large-scale bridging solution is thus around 700 10GE ports (I don’t think there are too many applications that are able to saturate more than two 10GE server uplinks).</p>
<p>Assuming a Clos fabric with two spine nodes and 3:1 oversubscription at leaf nodes, you need ~120 non-blocking line-rate 10GE ports on the spine switch (or <a href="/2012/06/qfabric-lite.html">four spine switches with 60 10GE ports or 16 40GE ports</a>) ... the only requirement is that STP should not block any links, which can be easily solved with multi-chassis link aggregation. There are at least five major data center switch vendors with products matching these requirements.</p>
<div class="separator"><a href="/2012/06/s1600-Clos_wo_MLAG.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="400" src="/2012/06/s400-Clos_wo_MLAG.jpg" width="380"/></a></div>
<p class="info">There are scenarios where you really need larger bridging domains (example: link customer’s physical servers with their VMs for thousands of customers); if your data center network is one of them, you should talk to <a href="/2012/02/nicira-uncloaked.html">Nicira</a> or wait for Cisco's VXLAN-to-VLAN gateway. On the other hand, implementing large-scale bridging to support <a href="/2011/06/stretched-clusters-almost-as-good-as.html">stretched HA clusters</a> doesn’t <a href="/2012/05/layer-2-network-is-single-failure.html">make much sense</a>.</p>
<h4>Is there any need for TRILL?</h4><p>As you’ve seen in the previous section, you can build layer-2 fabrics that satisfy reasonable real-life requirements with leaf-and-spine architecture using multi-chassis link aggregation (MLAG) between leaf and spine switches. Is TRILL thus useless?</p>
<p>Actually, it’s not. Every single MLAG solution is a brittle kludge that can potentially result in a split-brain scenario, and every MLAG-related software bug could have catastrophic impact (VSS or VPC bugs anyone?). It’s much simpler (and more reliable) to turn the layer-2 network into a <a href="/2010/07/why-is-trill-not-routing-at-layer-2.html">somewhat-routed network transporting MAC frames</a> and rely on time-proven features of routing protocols (IS-IS in this particular case) to work around failures. From the technology perspective, TRILL does make sense until we get rid of VLANs and move to MAC-over-IP.</p>
<h4>TRILL is so awesome some vendors want to charge extra for the privilege to use it</h4><p>Compared to MLAG, TRILL reduces the network complexity and makes the layer-2 fabric more robust ... but I’m not sure we should be paying extra for it (I’m looking at you, Cisco). After all, if a particular vendor’s solution results in a more stable network, I just might buy more gear from the same vendor, and reduce their support costs. </p>
<p>Charging a separate license cost for TRILL (actually FabricPath) just might persuade me to stick with MLAG+STP (after all, it does work most of the time), potentially making me unhappy (when it stops working). I might also start considering alternate vendors, because every single vendor out there supports MLAG+STP designs.</p>
<h4>Server-facing MLAG – the elephant in the room</h4><p>If your data center runs exclusively on VMware, you don’t need server-facing link aggregation; after all, the <a href="/2011/01/vmware-vswitch-does-not-support-lacp.html">static LAG configuration on the switch and IP-based hash on vSphere is a kludge</a> that can easily break. </p>
<p>If you use any other hypervisor or bare-metal servers, you might want to use LACP and proper MLAG to <a href="/2011/01/vswitch-in-multi-chassis-link.html">optimize the inter-VM</a> or <a href="/2010/12/multi-chassis-link-aggregation-mlag-and.html">inter-server traffic flow</a>.</p>
<div class="separator"><img src="/2012/06/s320-vSwitch_MLAG_Phy.png"/></div>
<p>Unfortunately, Brocade is the only vendor that <a href="/2012/04/beware-of-fabric-wide-link-aggregation.html">integrated TRILL-like VCS Fabric with MLAG</a>. You can use MLAG with Cisco’s FabricPath, but they’re totally separated – MLAG requires VPC. We haven’t really gained much if we’ve got rid of MLAG on inter-switch links by replacing STP with TRILL, but remained stuck with VPC to support MLAG on server-to-switch links.</p>
<h4>Conclusions</h4><p>Assuming you’re not yet ready to go down the VXLAN/NVGRE path, TRILL is definitely a technology worth considering, just to get rid of STP and MLAG combo. However, be careful:</p>
<ul class="ListParagraph"><li>Unless you’re running VMware vSphere or other similarly network-impaired operating system that’s never heard of LACP, you’ll probably need multi-chassis LAG for redundancy reasons ... and thus Brocade VCS Fabric is the only option if you don’t want to remain stuck with VPC.</li>
<li>Even when running TRILL in the backbone, I would still run the full range of STP features on the server-facing ports to prevent potential bridging loops created by clueless physical server or VM administrators (some people actually think it <a href="/2011/11/virtual-switches-need-bpdu-guard.html">makes sense to bridge between two virtual NICs in a regular VM</a>). Cisco is the only vendor offering TRILL-like fabric and STP in the same product at the same time (while VDX switches from Brocade work just fine with STP, they <a href="/2011/05/ignoring-stp-be-careful-be-very-careful.html">start pretending it doesn’t exist the moment you configure VCS fabric</a>).</li>
</ul>
<p>Hmm, seems like there are no good choices after all. Maybe you should still wait a little bit before jumping head-on into the murky TRILL waters.</p>
<h4>More information</h4><p>You know I have to mention a few webinars at the end of every blog post. Here’s the list of the most relevant ones (you get all of them with the <a href="http://www.ipspace.net/Subscription">yearly subscription</a>)</p>
<ul class="ListParagraph"><li><a href="http://www.ipspace.net/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking Engineers</a></li>
<li><a href="http://www.ipspace.net/Data_Center_Fabrics">Data Center Fabrics</a></li>
<li><a href="http://www.ipspace.net/Introduction_to_Virtualized_Networking">Introduction to Virtualized Networking</a></li>
<li><a href="http://www.ipspace.net/VMware_Networking_Deep_Dive">VMware Networking Deep Dive</a></li>
</ul>

