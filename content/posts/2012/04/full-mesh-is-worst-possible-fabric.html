---
url: /2012/04/full-mesh-is-worst-possible-fabric.html
title: "Full mesh is the worst possible fabric architecture"
date: "2012-04-16T07:35:00.000+02:00"
tags: [ Design,Data Center,Fabric ]
---

<p>One of the answers you get from some of the vendors selling you <em>data center fabrics </em>is “you can use any topology you wish” and then they start to rattle off an impressive list of buzzword-bingo-winning terms like <em>full mesh, </em><a href="http://en.wikipedia.org/wiki/Hypercube"><em>hypercube</em></a><em> </em>and <em>Clos fabric</em>. While <em>full mesh </em>sounds like a great idea (after all, what could possibly go wrong if every switch can talk directly to any other switch), it’s actually the worst possible architecture (apart from the fully randomized <a href="https://blog.ipspace.net/2012/04/monkey-design-still-doesnt-work-well.html">Monkey Design</a>).</p>
<p class="more">Before reading the rest of this post, you might want to visit <a href="http://packetpushers.net/the-sad-state-of-data-center-networking/"><em>Derick Winkworth’s The Sad State of Data Center Networking</em></a> to get in the proper mood.<!--more--></p>
<p>Imagine you just bought a bunch (let’s say eight, to keep the math simple) of typical data center switches. Most of them <a href="http://etherealmind.com/merchant-silicon-vendor-software-rise-lost-opportunity/">use the same chipset today</a>, so you probably got 48 x 10GE ports and 4 x 40GE ports that you can use as 16 x 10GE ports. Using the pretty standard 1:3 oversubscription, you dedicate 48 ports for server connections and 16 ports for intra-fabric connectivity … and you wire them in a full mesh (next diagram), giving you 20 Gbps of bandwidth between any two switches.</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2012/04/s1600-FM_Mesh.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="371" src="/2012/04/s400-FM_Mesh.jpg" width="400"/></a></div>
<p>Guess what … unless you have a perfectly symmetrical traffic pattern, you’ve just wasted most of the intra-fabric bandwidth. For example, if you’re doing a lot of <a href="https://blog.ipspace.net/2010/09/vmotion-elephant-in-data-center-room.html">vMotion</a> between servers attached to switches A and B, the maximum throughput you can get is 20 Gbps (even though you have 140 Gbps of uplink bandwidth on every single switch).</p>
<p class="note">Remember that all data center fabric technologies use <em>Equal Cost Multipath</em> because vendors think that MPLS Traffic Engineering and virtual circuits fry the brains of data center engineers.</p>
<p>Now let’s burn a bit more of our budget: buy two more switches, and wire all ten of them in a Clos fabric. You have just enough ports on the two “core” switches to connect 80 Gbps of uplink bandwidth from each “edge” switch to each of them (sometimes directly, sometimes splitting 40 GE port on the edge switch into 4 x 10GE and using port channel across eight 10 GE uplinks). </p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2012/04/s1600-FM_Clos.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="299" src="/2012/04/s400-FM_Clos.jpg" width="400"/></a></div>
<p class="note">Having core switches with plenty of 40GE ports makes your life even simpler.</p>
<p>Have we gained anything (apart from some goodwill from our friendly vendor/system integrator)?</p>
<p>Actually we did – using the Clos fabric you get a maximum of 160 Gbps between any pair of edge switches. Obviously the maximum amount of traffic any edge switch can send into the fabric or receive from it is still limited to 160 Gbps (unless you use <a href="https://blog.ipspace.net/2011/03/open-networking-foundation-fabric.html">unicorn-based OpenFlow</a> you can’t really violate the laws of physics), but we’re no longer limiting our ability to use all available bandwidth.</p>
<p>Now imagine you use slightly bigger core switches and attach uplinks directly to them. Shake the whole thing a bit, let the edge switches “fall to the floor” and you have the traditional 2-tier data center network design (or a 3-tier design if you use the Nexus 5000/FEX combo). Maybe we weren’t so <a name="_GoBack"></a>stupid after all …</p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2012/04/s1600-FM_2Tier.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="147" src="/2012/04/s400-FM_2Tier.jpg" width="400"/></a></div>
<h4>More information</h4><p>Since publishing this blog post I created a whole series of webinars on <a href="http://www.ipspace.net/Data_Center_Fabrics">data center fabrics</a>, <a href="http://www.ipspace.net/Leaf-and-Spine_Fabric_Architectures">leaf-and-spine fabric architectures</a>, and <a href="http://www.ipspace.net/EVPN_Technical_Deep_Dive">EVPN</a>. If you prefer to have online support and someone reviewing your design assignments, check out the <a href="http://www.ipspace.net/Designing_and_Building_Data_Center_Fabrics">Designing and Building Data Center Fabrics</a> online course.</p>

