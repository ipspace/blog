---
url: /2012/05/transparent-bridging-aka-l2-switching.html
title: "Transparent Bridging (aka L2 Switching) Scalability Issues"
date: "2012-05-11T07:14:00.000+02:00"
tags: [ bridging,switching ]
---

<p>Stephen Hauser sent me an interesting question after the <a href="http://www.guidetonetworkfabrics.com/">Data Center fabric webinar I did with Abner Germanow from Juniper</a>:</p>
<blockquote class="cite">A common theme in your talks is that L2 does not scale. Do you mean that Transparent (Learning) Bridging does not scale due to its flooding? Or is there something else that does not scale?</blockquote>
<p>As is oft the case, I’m not precise enough in my statements, so let’s fix that first:<!--more--></p>
<p>There are numerous layer-2 protocols, but when I talk about <em>layer-2 (L2) scalability</em> in data center context, I always talk about <em>Ethernet bridging</em> (also known under its <a href="https://blog.ipspace.net/2011/02/how-did-we-ever-get-into-this-switching.html">marketing name switching</a>), more precisely, <em>transparent bridging</em> that uses flooding of broadcast, unknown unicast, and multicast frames (I love the BUM acronym) to compensate for lack of host-to-switch- and routing (MAC reachability distribution) protocols.</p>
<p>Large transparently bridged Ethernet networks face three layers of scalability challenges:</p>
<p><strong>Dismal control plane protocol</strong> (Spanning Tree Protocol in its myriad incarnations), combined with <a href="https://blog.ipspace.net/2012/04/stp-loops-strike-again.html">broken implementations of STP kludges</a>. Forward-before-you-think behavior of Cisco’s PortFast and lack of CPU protection on some of the switches immediately come to mind.</p>
<p>TRILL (or a proprietary TRILL-like implementation like FabricPath) would solve most of the STP-related issues once implemented properly (<a href="https://blog.ipspace.net/2011/05/ignoring-stp-be-careful-be-very-careful.html">ignoring STP</a> does not count as properly scalable implementation in my personal opinion). However, we still have limited operational experience and some vendors implementing TRILL might still face a steep learning curve before all the <a href="https://blog.ipspace.net/2009/02/connecting-switch-to-itself-does-it.html">loop detection/prevention</a> and STP integration features work as expected.</p>
<p><strong>Flooding of BUM frames</strong> is an inherent part of transparent bridging and cannot be disabled if you want to retain its existing properties that are <a href="https://blog.ipspace.net/2012/02/microsoft-network-load-balancing-behind.html">relied upon by broken software implementations</a>.</p>
<p>Every broadcast frame flooded throughout a L2 domain must be processed by every host participating in that domain (where <em>L2 domain </em>means a <em>transparently bridged Ethernet VLAN or equivalent</em>). Ethernet NICs do perform some sort of multicast filtering, but it’s usually hash-based and not ideal (for more information, read <a href="http://www.fragmentationneeded.net/2010/10/mapping-multicast-groups-to-mac.html">multicast-related blog posts</a> written by <a href="http://www.fragmentationneeded.net/">Chris Marget</a>).</p>
<p>Finally, while Ethernet NICs usually ignore flooded unicast frames (those frames still eat the bandwidth on every single link in the L2 domain, including host-to-switch links), servers running hypervisor software are not that fortunate. The hypervisor requirements (number of unicast MAC addresses within a single physical host) typically exceed the NIC capabilities, <a href="https://blog.ipspace.net/2011/07/hypervisors-use-promiscuous-nic-mode.html">forcing hypervisors to put physical NICs in promiscuous mode</a>. Every hypervisor host thus has to receive, process, and oft ignore every flooded frame. Some of those frames have to be propagated to one or more VMs running in that hypervisor and further processed by them (assuming the frame belongs to the proper VLAN).</p>
<p>In a typical <a href="https://blog.ipspace.net/2011/12/vmware-vswitch-baseline-of-simplicity.html">every-VLAN-on-every-access-port</a> design, every hypervisor host has to processes every BUM frame generated anywhere in the L2 domain (regardless of whether its VMs belong to the VLAN generating the flood or not).</p>
<p>You might be able to make bridging scale better if you’d implement fully IP-aware L2 solution. Such a solution would have to include ARP proxy (or central ARP servers), IGMP snooping and a total ban on other BUM traffic. <a href="http://www.youtube.com/watch?v=N-25NoCOnP4">TRILL as initially envisioned by Radia Perlman</a> was moving in that direction and got thoroughly crippled and force-fit into the <em>ECMP bridging </em>rathole by the IETF working group.</p>
<p><strong>Lack of addressing hierarchy</strong> is the final stumbling block. Modern data center switches (<a href="http://etherealmind.com/merchant-silicon-vendor-software-rise-lost-opportunity/">most of them using the same hardware</a>) support up to 100K MAC addresses, so other problems will probably kill you way before you reach this milestone.</p>
<p>Finally, every L2 domain (VLAN) is a single failure domain (primarily due to BUM flooding). There are numerous knobs you can try to tweak (storm control, for example), but you cannot change two basic facts:</p>
<ul class="ListParagraph"><li>A software glitch in a switch that causes a forwarding (and thus flooding) loop involving core links will inevitably cause a network-wide meltdown (due to lack of TTL field in L2 headers);</li>
<li>A software glitch (or virus/malware/you-name-it), or uncontrolled flooding started by any host or VM attached to a VLAN will impact all other hosts (or VMs) attached to the same VLAN, as well as all core links. A bug resulting in broadcasts will also impact the CPU of all layer-3 (IP) switches with IP addresses configured in that VLAN.</li>
</ul>
<p class="warn">You can use storm control to reduce the impact of an individual VM, but <a href="https://supportforums.cisco.com/message/3422955">even the market leader might have a problem or two with this feature</a>.</p>
<h4>More information</h4><p>If you got this far, you really should read two more blog posts: <a href="https://blog.ipspace.net/2010/07/bridging-and-routing-is-there.html">Bridging and Routing – Is There a Difference?</a> and <a href="https://blog.ipspace.net/2010/07/bridging-and-routing-part-ii.html">Bridging and Routing – Part II</a>. You might also be interested in the <a href="https://blog.ipspace.net/2011/02/how-did-we-ever-get-into-this-switching.html">history of the <em>switching </em>buzzword</a>.</p>
<p>As always, I have a few <a href="http://www.ipspace.net/Roadmap/Data_center_webinars">data center webinars</a> that can help you: <a href="http://www.ipspace.net/Cloud_Computing_Networking:_Under_the_Hood">Cloud Computing Networking</a> (particularly the <a href="http://www.ipspace.net/Cloud_Computing_Networking:_Will_It_Scale%3F"><em>Will It Scale</em></a><em> </em>part of it), <a href="http://www.ipspace.net/Data_Center_Fabrics">Data Center Fabric Architectures</a> and <a href="http://www.ipspace.net/Data_Center_3.0_for_Networking_Engineers">Data Center 3.0 for Networking Engineer</a>. All three webinars are <a href="http://www.ipspace.net/Subscription">part of the yearly subscription</a>.</p>

