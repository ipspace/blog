---
date: 2012-03-30T15:40:00.000+02:00
tags:
- data center
- workshop
- virtualization
title: 'Cisco & VMware: Merging the Virtual and Physical NICs'
url: /2012/03/cisco-vmware-merging-virtual-and.html
---

<p><a href="https://blog.ipspace.net/2011/12/vmware-vswitch-baseline-of-simplicity.html">Virtual (soft) switches</a> present in almost every hypervisor significantly reduce the performance of high-bandwidth virtual machines (<a href="http://www.cisco.com/en/US/solutions/collateral/ns340/ns517/ns224/ns944/white_paper_c11-593280.html">measurements done by Cisco</a> a while ago indicate you could get up to 38% more throughput if you tie VMs directly to hardware NICs), but as I argued in my “<a href="https://blog.ipspace.net/2011/08/soft-switching-might-not-scale-but-we.html"><em>Soft Switching Might Not Scale, But We Need It</em></a>” post, we need hypervisor switches to isolate the virtual machines from the vagaries of the physical NICs. </p>
<p>Engineering gurus from Cisco and VMware have yet again proven me wrong – you can combine VMDirectPath and vMotion if you <a href="https://blog.ipspace.net/2011/08/vm-fex-how-convoluted-can-you-get.html">use VM-FEX</a>.<!--more--></p>
<div class="separator" style="clear: both; text-align: center;"><a href="/2012/03/s1600-VM-FEX-Reality.png"><img border="0" src="/2012/03/s400-VM-FEX-Reality.png"/></a><br/>VM-FEX architecture</div>
<p>This is (approximately) how that marvel of engineering works (and you’ll find <a href="http://www.cisco.com/web/learning/le21/le34/downloads/689/vmworld/preso/VMDirectPath_with_vMotion_on_Cisco_UCS_VM-FEX.pdf">more details in this presentation</a>):</p>
<ul class="ListParagraph"><li>You have to configure <a href="https://blog.ipspace.net/2011/08/vm-fex-how-convoluted-can-you-get.html">VM-FEX</a> (which means that you can only use this trick if you have an UCS system with Palo chipset in the server blades).</li>
<li>Palo chipset emulates the registers and data structures used by the VMXNET3 paravirtualized device driver (and most VMs use VMXNET3 today due to its <a href="http://www.vmware.com/pdf/vsp_4_vmxnet3_perf.pdf">performance benefits</a>). You can thus link a VM with VMXNET3 device driver directly to the physical hardware presented to the server by the Palo chipset (using VMDirectPath, for example).</li>
</ul>
<p class="note">Cisco was using VMDirectPath in the <a href="http://www.cisco.com/en/US/solutions/collateral/ns340/ns517/ns224/ns944/white_paper_c11-593280.html">VM-FEX performance measurements</a>; in most VM-FEX deployments you’d use the passthrough VEM to enable vMotion of the VMs using VM-FEX.</p>
<ul class="ListParagraph"><li>vSphere 5 introduced support for <a href="http://pubs.vmware.com/vsphere-50/index.jsp?topic=/com.vmware.vsphere.networking.doc_50/GUID-F9E3ECA9-B00A-4C29-BDE3-D5418E922043.html">vMotion with VMDirectPath</a> for VM-FEX NICs. This enhancement is crucial as it allows a VM using VM-FEX NIC without a VEM to be vMotioned to another host.</li>
</ul>
<p>The trick VMware’s engineers used is very simple (conceptually, but I’m positive there are numerous highly convoluted implementation details): once you get a request to vMotion a VM, you freeze the VM, copy physical registers of the VM-FEX VIC to the data structures used by the hypervisor kernel implementation of VMXNET3 device, disconnect the VM from the physical hardware, and allow it to continue working through the virtual VMXNET3 device and VEM. Once the VM is moved to another ESX host, the contents of the VMXNET3 virtual device registers get copied to the physical NIC, and the VM yet again regains full access to the physical hardware.</p>
<h4>Was it all just an alphabet soup?</h4><p>Check out my <a href="http://www.ipspace.net/Roadmap/Virtualization_webinars">virtualization webinars</a> – they will help you get a decent foothold in the brave new world of server and network virtualization.</p>

