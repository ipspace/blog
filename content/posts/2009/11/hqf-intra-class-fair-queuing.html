---
url: /2009/11/hqf-intra-class-fair-queuing/
title: "HQF: intra-class fair queuing"
date: "2009-11-16T06:43:00.001+01:00"
tags: [ QoS ]
---

<p>Continuing from my <a href="/2009/11/first-hqf-impressions-excellent-job/">first excursion into the brave new world of HQF</a>, I wanted to check how well the intra-class fair queuing works. I’ve started with the <a href="/2009/11/first-hqf-impressions-excellent-job/">same testbed and router configurations as before</a> and configured the following policy-map on the WAN interface:</p>
<pre class="code">policy-map WAN<br/> class P5001<br/>    bandwidth percent 20<br/>    fair-queue<br/> class P5003<br/>    bandwidth percent 30<br/>class class-default<br/>    fair-queue</pre><p>The test used this background load:</p>
<table class="codeTable"><tr><th>Class</th><th>Background load</th></tr><tr><td valign="top">P5001</td><td valign="top">10 parallel TCP sessions</td></tr><tr><td valign="top">P5003</td><td valign="top">1500 kbps UDP flood</td></tr><tr><td valign="top">class-default</td><td>1500 kbps UDP flood</td></tr></table><!--more--><p>As expected, the bandwidth distribution between the three traffic classes was almost optimal:</p>
<pre class="code">a1#<strong>show policy-map interface serial 0/1/0 | include map|bps</strong><br/>    Class-map: P5001 (match-all)<br/>      30 second offered rate 394000 bps, drop rate 0 bps<br/>      bandwidth 20% (400 kbps)<br/>    Class-map: P5003 (match-all)<br/>      30 second offered rate 2073000 bps, drop rate 1479000 bps<br/>      bandwidth 30% (600 kbps)<br/>    Class-map: class-default (match-any)<br/>      30 second offered rate 1780000 bps, drop rate 790000 bps</pre><p>Next I’ve started a single (<em>iperf</em>) TCP session in the P5003 class. Using traditional CB-WFQ the session wouldn’t even start due to heavy congestion caused by the UDP floods. The same problem occurred with HQF since the P5003 class was using FIFO queuing.</p>
<p>However, once I’ve configured <strong>fair-queue </strong>within the P5003 class, the TCP session got half the allocated bandwidth:</p>
<pre class="code">$ <strong>iperf -c 10.0.20.10 -t 3600 -p 5003 -i 60</strong><br/>------------------------------------------------------------<br/>Client connecting to 10.0.20.10, TCP port 5003<br/>TCP window size: 8.00 KByte (default)<br/>------------------------------------------------------------<br/>[1916] local 10.0.0.10 port 1309 connected with 10.0.20.10 port 5003<br/>[ ID] Interval       Transfer     Bandwidth<br/>[1916]  0.0-60.0 sec  2.05 MBytes   287 Kbits/sec<br/>[1916] 60.0-120.0 sec  2.05 MBytes   286 Kbits/sec</pre><p>As expected, if you start numerous parallel TCP sessions, each one will get as much bandwidth as the UDP flooding stream. I started ten parallel TCP sessions with <em>ipref </em>and got an aggregate <a href="http://en.wikipedia.org/wiki/Goodput">goodput</a> of 524 kbps (leaving UDP flood with approximately 60 kbps):</p>
<pre class="code">$ <strong>iperf -c 10.0.20.10 -t 3600 -p 5003 -i 60 -P 10</strong><br/>------------------------------------------------------------<br/>Client connecting to 10.0.20.10, TCP port 5003<br/>TCP window size: 8.00 KByte (default)<br/>------------------------------------------------------------<br/>[1916] local 10.0.0.10 port 1310 connected with 10.0.20.10 port 5003<br/>[1900] local 10.0.0.10 port 1311 connected with 10.0.20.10 port 5003<br/>[1884] local 10.0.0.10 port 1312 connected with 10.0.20.10 port 5003<br/>[1868] local 10.0.0.10 port 1313 connected with 10.0.20.10 port 5003<br/>[1852] local 10.0.0.10 port 1314 connected with 10.0.20.10 port 5003<br/>[1836] local 10.0.0.10 port 1315 connected with 10.0.20.10 port 5003<br/>[1820] local 10.0.0.10 port 1316 connected with 10.0.20.10 port 5003<br/>[1804] local 10.0.0.10 port 1317 connected with 10.0.20.10 port 5003<br/>[1788] local 10.0.0.10 port 1318 connected with 10.0.20.10 port 5003<br/>[1772] local 10.0.0.10 port 1319 connected with 10.0.20.10 port 5003<br/>[ ID] Interval       Transfer     Bandwidth<br/>[1868]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1820]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1772]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1836]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1916]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1900]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1852]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1804]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1788]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/>[1884]  0.0-60.0 sec   384 KBytes  52.4 Kbits/sec<br/><span class="high">[SUM]  0.0-60.0 sec  3.75 MBytes   524 Kbits/sec</span></pre><p class="note">This is one of the very valid reasons Service Providers hate peer-to-peer file sharing services like BitTorrent.</p>

