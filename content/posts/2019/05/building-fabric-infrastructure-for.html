---
date: 2019-05-14T08:42:00.000+02:00
tags:
- VXLAN
- data center
- fabric
- virtualization
title: Building Fabric Infrastructure for an OpenStack Private Cloud
url: /2019/05/building-fabric-infrastructure-for.html
---

<p>An attendee in my <a href="https://www.ipspace.net/Building_Next-Generation_Data_Center">Building Next-Generation Data Center</a> online course was asked to deploy numerous relatively small OpenStack cloud instances and wanted select the optimum virtual networking technology. Not surprisingly, every $vendor had just the right answer, including Arista:</p>
<blockquote class="cite">We’re considering moving from hypervisor-based overlays to ToR-based overlays using Arista’s CVX for approximately 2000 VLANs. </blockquote>
<p>As I explained in <a href="https://www.ipspace.net/Overlay_Virtual_Networking">Overlay Virtual Networking</a>, <a href="https://www.ipspace.net/Networking_in_Private_and_Public_Clouds">Networking in Private and Public Clouds</a> and <a href="https://www.ipspace.net/Designing_Private_Cloud_Infrastructure">Designing Private Cloud Infrastructure</a> (plus several <a href="https://www.ipspace.net/Presentations">presentations</a>) you have three options to implement virtual networking in private clouds:<!--more--></p>
<p><strong>Hypervisor-based overlays</strong>. This solution is the most scalable one, but requires decent software on the hypervisors (and speaking of OpenStack, <a href="https://blog.ipspace.net/2014/11/open-vswitch-performance-revisited.html">OVS isn’t known as the fastest virtual switch on the market</a> - please write a comment if you have recent performance data). It also decouples virtual networking from physical infrastructure, reducing the number of interdependent moving parts, and making the physical infrastructure totally stable.</p>
<p><strong>ToR-based overlays</strong> usually using VXLAN and EVPN these days. This approach works well for environments with relatively few statically-configured VLANs, but might not scale well when faced with thousands of dynamic VLANs.</p>
<p>As the cloud orchestration system might deploy workload belonging to any virtual network on any hypervisor the typical design includes configuring all VLANs on all ToR-to-Server links, effectively turning the whole fabric into a <a href="https://blog.ipspace.net/2012/05/layer-2-network-is-single-failure.html">single broadcast (and thus failure) domain</a>.</p>
<p><strong>ToR-based overlays coupled with cloud orchestration system</strong>. Networking vendors try to <a href="https://blog.ipspace.net/2013/06/network-virtualization-and-spaghetti.html">“solve” the scalability challenges I just described</a> by tightly coupling ToR switches with orchestration systems: every time a VM is deployed the orchestration system tells the relevant ToR switches they need a new VLAN on particular server-facing ports.</p>
<p>Sometimes the ToR switches connect directly to the orchestration system (see the description of VM Tracker/Tracer in <a href="https://www.ipspace.net/Data_Center_Fabrics">Data Center Fabrics</a> webinar), sometimes the networking vendor inserts another controller in the middle. The end result is always the same: a conundrum of too-many tightly coupled moving parts. All you need is a single weak link (like a failing REST API service) and the whole house of cards collapses.</p>
<p><strong>Moral of the story</strong>: As always, there is no right answer - you have to figure out what matters most: scalability, forwarding performance, or ease-of-deployment… but whatever you do, try to keep the number of moving parts and interdependent components to the bare minimum. Your network (and your business) will eventually be grateful.</p>

